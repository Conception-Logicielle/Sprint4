<articles>
	<article>
		<preamble>ACL2004-HEADLINE.txt</preamble>
		<titre>Hybrid Headlines: Combining Topics and Sentence Compression</titre>
		<auteur>David Zajic, Bonnie Dorr, Stacy President</auteur>
		<abstract>This paper presents Topiary, a headline- generation system that creates very short, informative summaries for news stories by combining sentence compres- sion and unsupervised topic discovery. We will show that the combination of linguistically motivated sentence com- pression with statistically selected topic terms performs better than either alone, according to some automatic summary evaluation measures. In addition we de- scribe experimental results establishing an appropriate extrinsic task on which to measure the effect of summarization on human performance. We demonstrate the usefulness of headlines in compar- ison to full texts in the context of this extrinsic task.</abstract>
		<introduction>In this paper we present Topiary, a headline- generation system that creates very short, infor- mative summaries for news stories by combining sentence compression and unsupervised topic dis- covery. Hedge Trimmer performs sentence com- pression by removing constituents from a parse tree of the lead sentence according to a set of linguistically-motivated heuristics until a length threshold is reached. Unsupervised Topic Discov- ery is a statistical method for deriving a set of topic models from a document corpus, assigning mean- ingful names to the topic models, and associating sets of topics with speciﬁc documents. The top- ics and sentence compressions are combined in a manner that preserves the advantages of each ap- proach: the ﬂuency and event-oriented informa- tion from the lead sentence with the broader cov- erage of the topic models. The next section presents previous work in the area of automatic summarization. Following this we describe Hedge Trimmer and Unsupervised Topic Discovery in more detail, and describe the algorithm for combining sentence compression with topics. Next we show that Topiary scores higher than either Hedge Trimmer or Unsuper- vised Topic Discovery alone according to certain automatic evaluation tools for summarization. Fi- nally we propose event tracking as an extrinsic task using automatic summarization for measur- ing how human performance is affected by auto- matic summarization, and for correlating human peformance with automatic evaluation tools. We describe an experiment that supports event track- ing as an appropriate task for this purpose, and show results that suggest that a well-written hu- man headline is nearly as useful for event tracking as the full text.</introduction>
		<corps>2 Previous Work

Hedge Trimmer is a sentence compression algo-
rithm based on linguistically-motivated heuristics.
Previous work on sentence compression (Knight
and Marcu, 2000) uses a noisy-channel model to
ﬁnd the most probable short string that gener-
ated the observed full sentence. Other work (Eu-
ler, 2002) combines a word-list with syntactic in-


formation to decide which words and phrases to
cancel. Our approach differs from Knight’s in
that we do not use a statistical model, so we do
not require any prior training on a large corpus
of story/headline pairs. Topiary shares with Eu-
ler the combination of topic lists and sentence
compression. However Euler uses the topic lists
to guide sentence selection and compression to-
wards a query-speciﬁc summary, whereas Topiary
uses topics to augment the concept coverage of a
generic summary.

Summaries can also consist of lists of words or
short phrases indicating that the topic or concept
they denote is important in the document. Extrac-
tive topic summaries consist of keywords or key
phrases that occur in the document. (Bergler et al.,
2003) achieves this by choosing noun phrases that
represent the most important text entities, as repre-
sented by noun phrase coreference chains. (Zhou
and Hovy, 2003) imposes ﬂuency onto a topic list
by ﬁnding phrase clusters early in the text that con-
tain important topic words found throughout the
text. In text categorization documents are assigned
to pre-deﬁned categories. This is equivalent to as-
signing topics to a document from a static topic
list, so the words in the summary need not actually
appear in the document. (Lewis, 1992) describes
a probabilistic feature-based method for assigning
Reuters topics to news stories. OnTopic (Schwartz
et al., 1997) uses a HMM to assign topics from a
topic-annotated corpus to a new document.

3 Algorithm Description

Topiary produces headlines by combining the out-
put of Hedge Trimmer, a sentence compression
algorithm, with Unsupervised Topic Detection
(UTD). In this section we will give brief descrip-
tions of Hedge Trimmer, recent modiﬁcations to
Hedge Trimmer, and UTD. We will then describe
how Hedge Trimmer and UTD are combined.

3.1 Hedge Trimmer

Hedge Trimmer (Dorr et al., 2003b) generates
a headline for a news story by compressing the
lead (or main) topic sentence according to a lin-
guistically motivated algorithm. For news stories,
the ﬁrst sentence of the document is taken to be
the lead sentence. The compression consists of

parsing the sentence using the BBN SIFT parser
(Miller et al., 1998) and removing low-content
syntactic constituents. Some constituents, such as
certain determiners (the, a) and time expressions
are always removed, because they rarely occur in
human-generated headlines and are low-content
in comparison to other constituents. Other con-
stituents are removed one-by-one until a length
threshold has been reached. These include, among
others, relative clauses, verb-phrase conjunction,
preposed adjuncts and prepositional phrases that
do not contain named entities. 1 The threshold can
be speciﬁed either in number of words or number
of characters. If the threshold is speciﬁed in num-
ber of characters, Hedge Trimmer will not include
partial words.

3.2 Recent Hedge Trimmer Work

Recently we have investigated a rendering of the
summary as “Headlinese” (M˚ardh, 1980) in which
certain constituents are dropped with no loss of
meaning. The result of this investigation has been
used to enhance Hedge Trimmer, most notably
the removal of certain instances of have and be.
For example, the previous headline generator pro-
duced summaries such as Sentence (2), whereas
the have/be removal produces (3).

(1)

Input: The senior Olympic ofﬁcial who lev-
eled stunning allegations of corruption within
the IOC said Sunday he had been “muzzled”
by president Juan Antonio Samaranch and
might be thrown out of the organization.

(2) Without participle have/be removal: Senior

Olympic ofﬁcial said he had been muzzled

(3) With participle have/be removal:

Senior
Olympic ofﬁcial said he muzzled by presi-
dent Juan Antonio Samaranch

Have and be are removed if they are part of a past
or present participle construction.
In this exam-
ple, the removal of had been allows a high-content
constituent by president Juan Antonio Samaranch
to ﬁt into the headline.

The removal of forms of to be allows Hedge
Trimmer to produce headlines that concentrate

1More details of the Hedge Trimmer algorithm can be

found in (Dorr et al., 2003b) and (Dorr et al., 2003a).


more information in the allowed space. The re-
moval of forms of to be results in sentences that
are not grammatical in general English, but are
typical of Headlinese English. For example, sen-
tences (5), (6) and all other examples in this paper
were trimmed to ﬁt in 75 characters.

emergency shortening methods which are only
to be used when the alternative is truncating the
headline after the threshold, possibly cutting the
middle of a word. These include removal of ad-
verbs and adverbial phrases, adjectives and adjec-
tive phrases, and nouns that modify other nouns.

(4)

Input: Leading maxi yachts Brindabella, Say-
onara and Marchioness were locked in a
three-way duel down the New South Wales
state coast Saturday as the Sydney to Hobart
ﬂeet faced deteriorating weather.

(5) Without to be removal: Sayonara and Mar-

chioness were locked in three

(6) With to be removal: Leading maxi yachts
and Marchioness

Brindabella Sayonara
locked in three

When have and be occur with a modal verb, the
modal verb is also removed. Sentence (9) shows
an example of this.
It could be argued that by
removing modals such as should and would the
meaning is vitally changed. The intended use of
the headline must be considered. If the headlines
are to be used for determining query relevance, re-
moval of modals may not hinder the user while
making room for additional high-content words
may help.

(7)

Input: Organizers of December’s Asian
Games have dismissed press reports that a
sports complex would not be completed on
time, saying preparations are well in hand, a
local newspaper said Friday.

(8) Without Modal-Have/Be Removal: Organiz-
ers have dismissed press reports saying

(9) With Modal-Have/Be Removal: Organizers
dismissed press reports sports complex not
completed saying

In addition when it or there appears as a subject
with a form of be or have, as in extraposition (It
was clear that the thief was hungry) or existential
clauses (There have been a spate of dog maulings),
the subject and the verb are removed.

Finally,

for situations in which the length
threshold is a hard constraint, we added some

3.3 Unsupervised Topic Discovery

Unsupervised Topic Discovery (UTD) is used
when we do not have a corpus annotated with top-
ics. It takes as input a large unannotated corpus
in any language and automatically creates a set of
topic models with meaningful names. The algo-
rithm has several stages. First, it analyzes the cor-
pus to ﬁnd strings of words that occur frequently.
(It does this using a Minimum Description Length
criterion.) These are frequently phrases that are
meaningful names of topics.

Second, it ﬁnds the high-content words in each
document (using a modiﬁed tf.idf measure). These
It
are possible topic names for each document.
keeps only those names that occur in at least four
different documents. These are taken to be an ini-
tial set of topic names.

In the third stage UTD trains topic models cor-
responding to these topic names. The modiﬁed
EM procedure of OnTopicTMis used to determine
which words in the documents often signify these
topic names. This produces topic models.

Fourth, these topic models are used to ﬁnd the
most likely topics for each document. This often
adds new topics to documents, even though the
topic name did not appear in the document.

We found, in various experiments, that the top-
ics derived by this procedure were usually mean-
ingful and that the topic assignment was about as
good as when the topics were derived from a cor-
pus that was annotated by people. We have also
used this procedure on different languages and
shown the same behavior.

Sentence (10) is a topic list generated for a story
about the investigation into the bombing of the
U.S. Embassy in Nairobi on August 7, 1998.

(10) BIN LADEN EMBASSY BOMBING PO-
LICE OFFICIALS PRISON HOUSE FIRE
KABILA


3.4 Combination of Hedge Trimmer and

Topics: Topiary

The Hedge Trimmer algorithm is constrained to
take its headline from a single sentence. It is of-
ten the case that there is no single sentence that
contains all the important information in a story.
The information can be spread over two or three
sentences, with pronouns or ellipsis used to link
them. In addition, our algorithms do not always
select the ideal sentence and trim it perfectly.

Topics alone also have drawbacks. UTD rarely
generates any topic names that are verbs. Thus
topic lists are good at indicating the general sub-
ject are but rarely give any direct indication of
what events took place.

Topiary is a modiﬁcation of the enhanced
Hedge Trimmer algorithm to take a list of top-
ics with relevance scores as additional input. The
compression threshold is lowered so that there
will be room for the highest scoring topic term
that isn’t already in the headline. This amount of
threshold lowering is dynamic, because the trim-
ming of the sentence can remove a previously in-
eligible high-scoring topic term from the headline.
After trimming is complete, additional topic terms
that do not occur in the headline are added to use
up any remaining space.

This often results in one or more main topics
about the story and a short sentence that says what
happened concerning them. The combination is
often more concise than a fully ﬂuent sentence and
compensates for the fact that the topic and the de-
scription of what happened to it do not appear in
the same sentence in the original story.

Sentences (11) and (12) are the output of Hedge
Trimmer and Topiary for the same story for which
the topics in Sentence (10) were generated.

(11) FBI agents this week began questioning rel-

atives of the victims

(12) BIN LADEN EMBASSY BOMBING FBI
agents this week began questioning relatives

Topiary was submitted to the Document Under-
standing Conference Workshop. Figure 1 shows
how Topiary peformed in comparison with other
DUC2004 participants on task 1, using ROUGE.
Task 1 was to produce a summary for a single news

document no more than than 75 characters. The
different ROUGE variants are sorted by overall
performance of the systems. The key observations
are that there was a wide range of performance
among the submitted systems, and that Topiary
scored ﬁrst or second among the automatic sys-
tems on each ROUGE measure.

4 Evaluation

We used two automatic evaluation systems, BLEU
(Papineni et al., 2002) and ROUGE (Lin and
Hovy, 2003), to evaluate nine variants of our head-
line generation systems. Both measures make n-
gram comparisons of the candidate systems to a
set of reference summaries.
In our evaluations
four reference summaries for each document were
used. The nine variants were run on 489 stories
from the DUC2004 single-document summariza-
tion headline generation task. The threshold was
75 characters, and longer headlines were truncated
to 75 characters. We also evaluated a baseline
that consisted of the ﬁrst 75 characters of the doc-
ument. The systems and the average lengths of
the headlines they produced are shown in Table
1. Trimmer headlines tend to be shorter than the
threshold because Trimmer removes constituents
until the length is below the threshold. Sometimes
it must remove a large constituent in order to get
below the threshold. Topiary is able to make full
use of the space by ﬁlling in topic words.

4.1 ROUGE

ROUGE is a recall-based measure for summa-
rizations. This automatic metric counts the num-
ber of n-grams in the reference summaries that
occur in the candidate and divides by the num-
ber of n-grams in the reference summaries. The
size of the n-grams used by ROUGE is conﬁg-
urable. ROUGE-n uses 1-grams through n-grams.
ROUGE-L is based on longest common subse-
quences, and ROUGE-W-1.2 is based on weighted
longest common subsequences with a weighting
of 1.2 on consecutive matches of length greater
than 1.

The ROUGE scores for the nine systems and the
baseline are shown in Table 2. Under ROUGE-
1 the Topiary variants scored signiﬁcantly higher
than the Trimmer variants, and both scored signif-


0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

ROUGE-1

ROUGE-L

ROUGE-W-1.2

ROUGE-2

ROUGE-3

ROUGE-4

Automatic Summaries

Reference Summaries

Topiary

Figure 1: ROUGE Scores for DUC2004 Automatic Summaries, Reference Summaries and Topiary

System
Trim

Trim.E

Trim.HB

Trim.HB.E

Top

Top.E

Top.HB

Top.HB.E

UTD

Description
Trimmer
no have/be removal
no emergency shortening
Trimmer
no have/be removal
emergency shortening
Trimmer
have/be removal
no emergency shortening
Trimmer
have/be removal
emergency shortening
Topiary
no have/be removal
no emergency shortening
Topiary
no have/be removal
emergency shortening
Topiary
have/be removal
no emergency shortening
Topiary
have/be removal
emergency shortening
UTD Topics

Words Chars
8.7

57.3

8.7

57.1

8.6

57.7

8.6

57.4

10.8

73.3

10.8

73.2

10.7

73.2

10.7

73.2

9.5

71.1

Table 1: Systems and Headline Lengths

icantly higher than the UTD topic lists with 95%
conﬁdence. Since ﬂuency is not measured at all
by unigrams, we must conclude that the Trimmer
headlines, by selecting the lead sentence, included
more or better topic words than UTD. The high-
est scoring UTD topics tend to be very meaning-
ful while the ﬁfth and lower scoring topics tend
to be very noisy. Thus the higher scores of Topi-
ary can be attributed to including only the best of
the UTD topics while preserving the lead sentence
topics. The same groupings occur with ROUGE-L
and ROUGE-W, indicating that the longest com-
mon subsequences are dominated by sequences of
length one.

Under the higher order ROUGE evaluations
the systems group by the presence or absence of
have/be removal, with higher scores going to sys-
tems in which have/be removal was performed.
This indicates that the removal of these light con-
tent verbs makes the summaries more like the lan-
guage of headlines. The value of emergency short-
ening over truncation is not clear.


Top.HB.E
Top.HB
Top.E
Top
baseline
Trim.HB.E
Trim.HB
Trim.E
Trim
UTD

ROUGE-1 ROUGE-2 ROUGE-3
0.06449
0.24914
0.06595
0.24873
0.06169
0.24812
0.06309
0.24621
0.06370
0.22136
0.06571
0.20415
0.06565
0.20380
0.06226
0.20105
0.06283
0.20061
0.01585
0.15913

0.02122
0.02267
0.01874
0.01995
0.02118
0.02527
0.02508
0.02221
0.02266
0.00087

ROUGE-4 ROUGE-L ROUGE-W-1.2
0.00712
0.00826
0.00562
0.00639
0.00707
0.00950
0.00945
0.00774
0.00792
0.00000

0.11891
0.11970
0.11837
0.11861
0.16955
0.11127
0.11118
0.11003
0.10996
0.07797

0.19951
0.20061
0.19856
0.19856
0.11738
0.18506
0.18472
0.18287
0.18248
0.13041

Table 2: ROUGE Scores sorted by ROUGE-1

4.2 BLEU

BLEU is a system for automatic evaluation of ma-
chine translation that uses a modiﬁed n-gram pre-
cision measure to compare machine translations to
reference human translations. This automatic met-
ric counts the number of n-grams in the candidate
that occur in any of the reference summaries and
divides by the number of n-grams in the candidate.
The size of the n-grams used by BLEU is conﬁg-
urable. BLEU-n uses 1-grams through n-grams. In
our evaluation of headline generation systems, we
treat summarization as a type of translation from
a verbose language to a concise one, and compare
automatically generated headlines to human gen-
erated headlines.

The BLEU scores for the nine systems and
the baseline are shown in Table 3. For BLEU-1
the Topiary variants score signiﬁcantly better than
the Trimmer variants with 95% conﬁdence. Un-
der BLEU-2 the Topiary scores are higher than
the Trimmer scores, but not signiﬁcantly. Under
BLEU-4 the Trimmer variants score slightly but
not signiﬁcantly higher than the Topiary variants,
and at BLEU-3 there is no clear pattern. Trim-
mer and Topiary variants score signiﬁcantly higher
than UTD for all settings of BLEU with 95% con-
ﬁdence.

5 Extrinsic Task

For an automatic summarization evaluation tool to
be of use to developers it must be shown to cor-
relate well with human performance on a speciﬁc
extrinsic task. In selecting the extrinsic task it is
important that the task be unambiguous enough
that subjects can perform it with a high level of
If the task is so difﬁcult that sub-
agreement.

Top.HB.E
Top.HB
Top.E
Top
Trim.HB.E
Trim.HB
baseline
Trim.E
Trim
UTD

BLEU-1 BLEU-2 BLEU-3 BLEU-4
0.4368
0.4362
0.4310
0.4288
0.3712
0.3705
0.3695
0.3636
0.3635
0.2859

0.0849
0.0885
0.0739
0.0832
0.0939
0.0943
0.0853
0.0897
0.0922
0.0000

0.1443
0.1476
0.1381
0.1417
0.1495
0.1493
0.1372
0.1442
0.1461
0.0263

0.2443
0.2463
0.2389
0.2415
0.2333
0.2331
0.2214
0.2285
0.2297
0.0954

Table 3: BLEU Scores sorted by BLEU-1

jects cannot perform with a high level of agree-
ment – even when they are shown the entire docu-
ment – it will not be possible to detect signiﬁcant
differences among different summarization meth-
ods because the amount of variation due to noise
will overshadow the variation due to summariza-
tion method.

In an earlier experiment we attempted to use
document selection in the context of informa-
tion retrieval as an extrinsic task. Subjects were
asked to decide if a document was highly rele-
vant, somewhat relevant or not relevant to a given
query. However we found that subjects who had
been shown the entire document were only able
to agree with each other 75% of the time and
agreed with the allegedly correct answers only
70% of the time. We were unable to draw any</corps>
		<conclusion>conclusions about the relative performance of the
summarization systems, and thus were not able
to make any correlations between human perfor-
mance and scores on automatic summarization
evaluation tools. For more details see (Zajic et al.,
2004).

We propose a more constrained type of docu-


ment relevance judgment as an appropriate extrin-
sic task for evaluating human performance using
automatic summarizations. The task, event track-
ing, has been reported in NIST TDT evaluations
to provide the basis for more reliable results. Sub-
jects are asked to decide if a document contains
information related to a particular event in a spe-
ciﬁc domain. The subject is told about a speciﬁc
event, such as the bombing of the Murrah Federal
Building in Oklahoma City. A detailed descrip-
tion is given about what information is considered
relevent to an event in the given domain. For in-
stance, in the criminal case domain, information
about the crime, the investigation, the arrest, the
trial and the sentence are relevant.

We performed a small event tracking experi-
ment to compare human performance using full
news story text against performance using human-
generated headlines of the same stories. Seven
events and twenty documents per event were cho-
sen from the 1999 Topic Detection and Tracking
(TDT3) corpus. Four subjects were asked to judge
the full news story texts or story headlines as rel-
evant or not relevant to each speciﬁed event. The
documents in the TDT3 corpus were already an-
notated as relevant or not relevant to each event
by NIST annotators. The NIST annotations were
taken to be the correct answers by which to judge
the overall performance of the subjects. The sub-
jects were shown a practice event, three events
with full story text and three events with story
headlines.

We calculated average agreement between sub-
jects as the number of documents on which two
subjects made the same judgment divided by the
number of documents on which the two subjects
had both made judgments. The average agreement
between subjects was 86% for full story texts and
80% for headlines. The average agreement with
the NIST annotations was slightly higher when us-
ing the full story text than the headline, with text
producing 86% overall agreement with NIST and
headlines producing 84% agreement with NIST.
Use of headlines resulted in a signiﬁcant increase
in speed. Subjects spent an average of 30 seconds
per document when shown the entire text, but only
7.7 seconds per document when shown the head-
line. Table 4 shows the precision, recall and (cid:0)(cid:2)(cid:1)

Full Text
Headline

Precision
0.831
0.842

Recall
0.900
0.842

(cid:3)(cid:5)(cid:4)(cid:7)(cid:6)

0.864
0.842

Table 4: Results of Event Tracking Experiment

with (cid:9)(cid:11)(cid:10)(cid:13)(cid:12)(cid:5)(cid:14)(cid:16)(cid:15) .

The small difference in NIST agreement be-
tween full texts and headlines seems to suggest
that the best human-written headlines can supply
sufﬁcient information for performing event track-
ing. However it is possible that subjects found the
task of reading entire texts dull, and allowed their
performance to diminish as they grew tired.

Full texts yielded a higher recall than head-
lines, which is not surprising. However headlines
yielded a slightly higher precision than full texts
which means that subjects were able to reject non-
relevant documents as well with headlines as they
could by reading the entire document. We ob-
served that subjects sometimes marked documents
as relevant if the full text contained even a brief
mention of the event or any detail that could be
construed as satisfying the domain description. If
avoiding false positives (or increasing precision) is
an important goal, these results suggest that use of
headlines provides an advantage over reading the
entire text.

Further event tracking experiments will include
a variety of methods for automatic summariza-
tion. This will give us the ability to compare hu-
man performance using the summarization meth-
ods against one another and against human perfor-
mance using full text. We do not expect that any
summarization method will allow humans to per-
form event tracking better than reading the entire
document, however we hope that we can improve
human performance time while introducing only
a small, acceptable loss in performance. We also
plan to calibrate automatic summarization evalu-
ation tools, such as BLEU and ROUGE, to ac-
tual human performance on event tracking for each
METHOD.

6 Conclusions and Future Work

We have shown the effectiveness of combining
sentence compression and topic lists to construct
informative summaries. We have compared three

(cid:8)

the American Association for Artiﬁcial Intelligence
AAAI2000, Austin, Texas.

David Lewis. 1992. An evaluation of phrasal and clus-
tered representations on a text categorization task.
In Proceedings of the 15th annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 37–50, Copen-
hagen, Denmark.

Chin-Yew Lin and Eduard Hovy.

2003. Auto-
matic Evaluation of Summaries Using N-gram Co-
Occurrences Statistics. In Proceedings of the Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics, Edmonton,
Alberta.

Ingrid M˚ardh. 1980. Headlinese: On the Grammar of

English Front Page Headlines. Malmo.

S. Miller, M. Crystal, H. Fox, L. Ramshaw,
R. Schwartz, R. Stone, and R. Weischedel. 1998.
Algorithms that Learn to Extract Information; BBN:
Description of the SIFT System as Used for MUC-7.
In Proceedings of the MUC-7.

K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of Association of
Computational Linguistics, Philadelphia, PA.

R. Schwartz, T. Imai, F. Jubala, L. Nguyen, and
J. Makhoul. 1997. A maximum likelihood model
for topic classiﬁcation of broadcast news.
In
Eurospeech-97, Rhodes, Greece.

David Zajic, Bonnie Dorr, Richard Schwartz, and Stacy
President. 2004. Headline evaluation experiment
results, umiacs-tr-2004-18. Technical report, Uni-
versity of Maryland Institute for Advanced Comput-
ing Studies, College Park, Maryland.

Liang Zhou and Eduard Hovy. 2003. Headline sum-
marization at isi. In Proceedings of the 2003 Doc-
ument Understanding Conference, Draft Papers,
pages 174–178, Edmonton, Candada.

approaches to automatic headline generation (Top-
iary, Hedge Trimmer and Unsupervised Topic Dis-
covery) using two automatic summarization evalu-
ation tools (BLEU and ROUGE). We have stressed
the importance of correlating automatic evalua-
tions with human performance of an extrinsic task,
and have proposed event tracking as an appropri-
ate task for this purpose.

We plan to perform a study in which Topiary,
Hedge Trimmer, Unsupervised Topic Discovery
and other summarization methods will be evalu-
ated in the context of event tracking. We also plan
to extend the tools described in this paper to the
domains of transcribed broadcast news and cross-
language headline generation.

Acknowledgements

The University of Maryland authors are sup-
ported,
in part, by BBNT Contract 020124-
7157, DARPA/ITO Contract N66001-97-C-8540,
and NSF CISE Research Infrastructure Award
EIA0130422.</conclusion>
		<discussion>Aucune discussion trouvée.</discussion>
		<biblio>Sabine Bergler, Ren´e Witte, Michelle Khalife,
Zhuoyan Li, and Frank Rudzicz.
2003. Using
knowledge-poor coreference resolution for text sum-
marization. In Proceedings of the 2003 Document
Understanding Conference, Draft Papers, pages 85–
92, Edmonton, Candada.

Bonnie Dorr, David Zajic, and Richard Schwartz.
Cross-language headline generation for
2003a.
hindi. ACM Transactions on Asian Language Infor-
mation Processing (TALIP), 2:2.

Bonnie Dorr, David Zajic, and Richard Schwartz.
2003b. Hedge trimmer: A parse-and-trim approach
to headline generation. In Proceedings of the HLT-
NAACL 2003 Text Summarization Workshop, Ed-
monton, Alberta, Canada, pages 1–8.

T. Euler. 2002. Tailoring text using topic words: Se-
lection and compression.
In Proceedings of 13th
International Workshop on Database and Expert
Systems Applications (DEXA 2002), 2-6 Septem-
ber 2002, Aix-en-Provence, France, pages 215–222.
IEEE Computer Society.

Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization – step one: Sentence com-
In The 17th National Conference of
pression.</biblio>
	</article>
	<article>
		<preamble>Boudin-Torres-2006.txt</preamble>
		<titre>Manchester, August 2008</titre>
		<auteur>AScalableMMRApproachtoSentenceScoringforMulti-DocumentUpdateSummarizationFlorianBoudin\andMarcEl-B`eze\\LaboratoireInformatiqued’Avignon339chemindesMeinajaries,BP1228,84911AvignonCedex9,France.florian.boudin@univ-avignon.frmarc.elbeze@univ-avignon.frJuan-ManuelTorres-Moreno\,[[´EcolePolytechniquedeMontr´ealCP6079Succ.CentreVilleH3C3A7Montr´eal(Qu´ebec),Canada.juan-manuel.torres@univ-avignon.frAbstractWepresentSMMR,ascalablesentencescoringmethodforquery-orientedup-datesummarization.Sentencesarescoredthankstoacriterioncombiningqueryrele-vanceanddissimilaritywithalreadyreaddocuments(history).Astheamountofdatainhistoryincreases,non-redundancyisprioritizedoverquery-relevance.WeshowthatSMMRachievespromisingre-sultsontheDUC2007updatecorpus.1IntroductionExtensiveexperimentsonquery-orientedmulti-documentsummarizationhavebeencarriedoutoverthepastfewyears.Mostofthestrategiestoproducesummariesarebasedonanextrac-tionmethod,whichidentiﬁessalienttextualseg-ments,mostoftensentences,indocuments.Sen-tencescontainingthemostsalientconceptsarese-lected,orderedandassembledaccordingtotheirrelevancetoproducesummaries(alsocalledex-tracts)(ManiandMaybury,1999).RecentlyemergedfromtheDocumentUnder-standingConference(DUC)20071,updatesum-marizationattemptstoenhancesummarizationwhenmoreinformationaboutknowledgeacquiredbytheuserisavailable.Itasksthefollowingques-tion:hastheuseralreadyreaddocumentsonthetopic?Inthecaseofapositiveanswer,producinganextractfocusingononlynewfactsisofinter-est.Inthisway,animportantissueisintroduced:c(cid:13)2008.LicensedundertheCreativeCommonsAttribution-Noncommercial-ShareAlike3.0Unportedli-cense(http://creativecommons.org/licenses/by-nc-sa/3.0/).Somerightsreserved.1DocumentUnderstandingConferencesareconductedsince2000bytheNationalInstituteofStandardsandTech-nology(NIST),http://www-nlpir.nist.govredundancywithpreviouslyreaddocuments(his-tory)hastoberemovedfromtheextract.Anaturalwaytogoaboutupdatesummarizationwouldbeextractingtemporaltags(dates,elapsedtimes,temporalexpressions...)(ManiandWilson,2000)ortoautomaticallyconstructthetimelinefromdocuments(SwanandAllan,2000).Thesetemporalmarkscouldbeusedtofocusextractsonthemostrecentlywrittenfacts.However,mostre-centlywrittenfactsarenotnecessarilynewfacts.MachineReading(MR)wasusedby(Hickletal.,2007)toconstructknowledgerepresentationsfromclustersofdocuments.Sentencescontain-ing“new”information(i.e.thatcouldnotbein-ferredbyanypreviouslyconsidereddocument)areselectedtogeneratesummary.However,thishighlyefﬁcientapproach(bestsysteminDUC2007update)requireslargelinguisticresources.(Witteetal.,2007)proposearule-basedsystembasedonfuzzycoreferenceclustergraphs.Again,thisapproachrequirestomanuallywritethesen-tencerankingscheme.Severalstrategiesremain-ingonpost-processingredundancyremovaltech-niqueshavebeensuggested.Extractsconstructedfromhistorywereusedby(BoudinandTorres-Moreno,2007)tominimizehistory’sredundancy.(Linetal.,2007)haveproposedamodiﬁedMax-imalMarginalRelevance(MMR)(CarbonellandGoldstein,1998)re-rankerduringsentenceselec-tion,constructingthesummarybyincrementallyre-rankingsentences.Inthispaper,weproposeascalablesentencescoringmethodforupdatesummarizationderivedfromMMR.Motivatedbytheneedforrelevantnovelty,candidatesentencesareselectedaccord-ingtoacombinedcriterionofqueryrelevanceanddissimilaritywithpreviouslyreadsentences.Therestofthepaperisorganizedasfollows.Section2 24 introducesourproposedsentencescoringmethodandSection3presentsexperimentsandevaluatesourapproach.2MethodTheunderlyingideaofourmethodisthatasthenumberofsentencesinthehistoryincreases,thelikelihoodtohaveredundantinformationwithincandidatesentencesalsoincreases.WeproposeascalablesentencescoringmethodderivedfromMMRthat,asthesizeofthehistoryincreases,givesmoreimportancetonon-redundancythattoqueryrelevance.WedeﬁneHtorepresentthepre-viouslyreaddocuments(history),Qtorepresentthequeryandsthecandidatesentence.Thefol-lowingsubsectionsformallydeﬁnethesimilaritymeasuresandthescalableMMRscoringmethod.2.1Aquery-orientedmulti-documentsummarizerWehaveﬁrststartedbyimplementingasimplesummarizerforwhichthetaskistoproducequery-focusedsummariesfromclustersofdocuments.Eachdocumentispre-processed:documentsaresegmentedintosentences,sentencesareﬁltered(wordswhichdonotcarrymeaningareremovedsuchasfunctionalwordsorcommonwords)andnormalizedusingalemmasdatabase(i.e.inﬂectedforms“go”,“goes”,“went”,“gone”...arereplacedby“go”).AnN-dimensionalterm-spaceΓ,whereNisthenumberofdifferenttermsfoundinthecluster,isconstructed.SentencesarerepresentedinΓbyvectorsinwhicheachcomponentisthetermfrequencywithinthesentence.Sentencescor-ingcanbeseenasapassageretrievaltaskinInfor-mationRetrieval(IR).Eachsentencesisscoredbycomputingacombinationoftwosimilaritymea-suresbetweenthesentenceandthequery.Theﬁrstmeasureisthewellknowncosineangle(Saltonetal.,1975)betweenthesentenceandthequeryvec-torialrepresentationsinΓ(denotedrespectively~sand~Q).ThesecondsimilaritymeasureisbasedontheJaro-Winklerdistance(Winkler,1999).TheoriginalJaro-Winklermeasure,denotedJW,usesthenumberofmatchingcharactersandtransposi-tionstocomputeasimilarityscorebetweentwoterms,givingmorefavourableratingstotermsthatmatchfromthebeginning.WehaveextendedthismeasuretocalculatethesimilaritybetweenthesentencesandthequeryQ:JWe(s,Q)=1|Q|·Xq∈Qmaxm∈S0JW(q,m)(1)whereS0isthetermsetofsinwhichthetermsmthatalreadyhavemaximizedJW(q,m)arere-moved.TheuseofJWesmoothsnormalizationandmisspellingerrors.Eachsentencesisscoredusingthelinearcombination:Sim1(s,Q)=α·cosine(~s,~Q)+(1−α)·JWe(s,Q)(2)whereα=0.7,optimallytunedonthepastDUCsdata(2005and2006).Thesystemproducesalistofrankedsentencesfromwhichthesummaryisconstructedbyarrangingthehighscoredsentencesuntilthedesiredsizeisreached.2.2AscalableMMRapproachMMRre-rankingalgorithmhasbeensuccessfullyusedinquery-orientedsummarization(Yeetal.,2005).Itstrivestoreduceredundancywhilemain-tainingqueryrelevanceinselectedsentences.Thesummaryisconstructedincrementallyfromalistofrankedsentences,ateachiterationthesentencewhichmaximizesMMRischosen:MMR=argmaxs∈S[λ·Sim1(s,Q)−(1−λ)·maxsj∈ESim2(s,sj)](3)whereSisthesetofcandidatessentencesandEisthesetofselectedsentences.λrepresentsaninterpolationcoefﬁcientbetweensentence’srele-vanceandnon-redundancy.Sim2(s,sj)isanor-malizedLongestCommonSubstring(LCS)mea-surebetweensentencessandsj.Detectingsen-tencerehearsals,LCSiswelladaptedforredun-dancyremoval.WeproposeaninterpretationofMMRtotackletheupdatesummarizationissue.SinceSim1andSim2arerangedin[0,1],theycanbeseenasprob-abilitieseventhoughtheyarenot.Justasrewriting(3)as(NRstandsforNoveltyRelevance):NR=argmaxs∈S[λ·Sim1(s,Q)+(1−λ)·(1−maxsh∈HSim2(s,sh))](4)Wecanunderstandthat(4)equatestoanORcom-bination.Butaswearelookingforamoreintu-itiveANDandsincethesimilaritiesareindepen-dent,wehavetousetheproductcombination.The 25 scoringmethoddeﬁnedin(2)ismodiﬁedintoadoublemaximizationcriterioninwhichthebestrankedsentencewillbethemostrelevanttothequeryANDthemostdifferenttothesentencesinH.SMMR(s)=Sim1(s,Q)·(cid:18)1−maxsh∈HSim2(s,sh)(cid:19)f(H)(5)Decreasingλin(3)withthelengthofthesum-marywassuggestedby(Murrayetal.,2005)andsuccessfullyusedintheDUC2005by(Hacheyetal.,2005),therebyemphasizingtherelevanceattheoutsetbutincreasinglyprioritizingredun-dancyremovalastheprocesscontinues.Sim-ilarly,weproposetofollowthisassumptioninSMMRusingafunctiondenotedfthatastheamountofdatainhistoryincreases,prioritizenon-redundancy(f(H)→0).3ExperimentsThemethoddescribedintheprevioussectionhasbeenimplementedandevaluatedbyusingtheDUC2007updatecorpus2.Thefollowingsubsec-tionspresentdetailsofthedifferentexperimentswehaveconducted.3.1TheDUC2007updatecorpusWeusedforourexperimentstheDUC2007up-datecompetitiondataset.Thecorpusiscomposedof10topics,with25documentspertopic.Theup-datetaskgoalwastoproduceshort(∼100words)multi-documentupdatesummariesofnewswirear-ticlesundertheassumptionthattheuserhasal-readyreadasetofearlierarticles.Thepurposeofeachupdatesummarywillbetoinformthereaderofnewinformationaboutaparticulartopic.GivenaDUCtopicandits3documentclusters:A(10documents),B(8documents)andC(7doc-uments),thetaskistocreatefromthedocumentsthreebrief,ﬂuentsummariesthatcontributetosat-isfyingtheinformationneedexpressedinthetopicstatement.1.AsummaryofdocumentsinclusterA.2.AnupdatesummaryofdocumentsinB,un-dertheassumptionthatthereaderhasalreadyreaddocumentsinA.2MoreinformationabouttheDUC2007corpusisavail-ableathttp://duc.nist.gov/.3.AnupdatesummaryofdocumentsinC,un-dertheassumptionthatthereaderhasalreadyreaddocumentsinAandB.Withinatopic,thedocumentclustersmustbepro-cessedinchronologicalorder.Oursystemgener-atesasummaryforeachclusterbyarrangingthehighrankedsentencesuntilthelimitof100wordsisreached.3.2EvaluationMostexistingautomatedevaluationmethodsworkbycomparingthegeneratedsummariestooneormorereferencesummaries(ideally,producedbyhumans).Toevaluatethequalityofourgeneratedsummaries,wechoosetousetheROUGE3(Lin,2004)evaluationtoolkit,thathasbeenfoundtobehighlycorrelatedwithhumanjudgments.ROUGE-Nisan-gramrecallmeasurecalculatedbetweenacandidatesummaryandasetofreferencesum-maries.InourexperimentsROUGE-1,ROUGE-2andROUGE-SU4willbecomputed.3.3ResultsTable1reportstheresultsobtainedontheDUC2007updatedatasetfordifferentsentencescor-ingmethods.cosine+JWestandsforthescor-ingmethoddeﬁnedin(2)andNRimprovesitwithsentencere-rankingdeﬁnedinequation(4).SMMRisthecombinedadaptationwehavepro-posedin(5).Thefunctionf(H)usedinSMMRisthesimplerationalfunction1H,whereHincreaseswiththenumberofpreviousclusters(f(H)=1forclusterA,12forclusterBand13forclusterC).Thisfunctionallowstosimplytesttheassumptionthatnon-redundancyhavetobefavouredasthesizeofhistorygrows.Baselineresultsareobtainedonsummariesgeneratedbytakingtheleadingsen-tencesofthemostrecentdocumentsofthecluster,upto100words(ofﬁcialbaselineofDUC).ThetablealsoliststhethreetopperformingsystemsatDUC2007andthelowestscoredhumanreference.Aswecanseefromtheseresults,SMMRout-performstheothersentencescoringmethods.BywaysofcomparisonoursystemwouldhavebeenrankedsecondattheDUC2007updatecompeti-tion.Moreover,nopost-processingwasappliedtotheselectedsentencesleavinganimportantmarginofprogress.Anotherinterestingresultisthehighperformanceofthenon-updatespeciﬁcmethod(cosine+JWe)thatcouldbeduetothesmallsize3ROUGEisavailableathttp://haydn.isi.edu/ROUGE/. 26 ofthecorpus(littleredundancybetweenclusters).ROUGE-1ROUGE-2ROUGE-SU4Baseline0.262320.045430.082473rdsystem0.357150.096220.132452ndsystem0.369650.098510.13509cosine+JWe0.359050.101610.13701NR0.362070.100420.13781SMMR0.363230.102230.138861stsystem0.370320.111890.14306Worsthuman0.404970.105110.14779Table1:ROUGEaveragerecallscorescomputedontheDUC2007updatecorpus.4DiscussionandFutureWorkInthispaperwehavedescribedSMMR,ascal-ablesentencescoringmethodbasedonMMRthatachievesverypromisingresults.Animportantas-pectofoursentencescoringmethodisthatitdoesnotrequiresre-rankingnorlinguisticknowledge,whichmakesitasimpleandfastapproachtotheissueofupdatesummarization.ItwaspointedoutattheDUC2007workshopthatQuestionAnswer-ingandquery-orientedsummarizationhavebeenconvergingonacommontask.Thevalueaddedbysummarizationliesinthelinguisticquality.Ap-proachesmixingIRtechniquesarewellsuitedforquery-orientedsummarizationbuttheyrequirein-tensiveworkformakingthesummaryﬂuentandcoherent.Amongtheothers,thisisapointthatwethinkisworthyoffurtherinvestigation.AcknowledgmentsThisworkwassupportedbytheAgenceNationaledelaRecherche,France,projectRPM2.ReferencesBoudin,F.andJ.M.Torres-Moreno.2007.ACo-sineMaximization-MinimizationapproachforUser-OrientedMulti-DocumentUpdateSummarization.InRecentAdvancesinNaturalLanguageProcessing(RANLP),pages81–87.Carbonell,J.andJ.Goldstein.1998.TheuseofMMR,diversity-basedrerankingforreorderingdocumentsandproducingsummaries.In21stannualinterna-tionalACMSIGIRconferenceonResearchandde-velopmentininformationretrieval,pages335–336.ACMPressNewYork,NY,USA.Hachey,B.,G.Murray,andD.Reitter.2005.TheEmbraSystematDUC2005:Query-orientedMulti-documentSummarizationwithaVeryLargeLatentSemanticSpace.InDocumentUnderstandingCon-ference(DUC).Hickl,A.,K.Roberts,andF.Lacatusu.2007.LCC’sGISTexteratDUC2007:MachineReadingforUp-dateSummarization.InDocumentUnderstandingConference(DUC).Lin,Z.,T.S.Chua,M.Y.Kan,W.S.Lee,L.Qiu,andS.Ye.2007.NUSatDUC2007:UsingEvolu-tionaryModelsofText.InDocumentUnderstandingConference(DUC).Lin,C.Y.2004.Rouge:APackageforAutomaticEvaluationofSummaries.InWorkshoponTextSum-marizationBranchesOut,pages25–26.Mani,I.andM.T.Maybury.1999.AdvancesinAuto-maticTextSummarization.MITPress.Mani,I.andG.Wilson.2000.Robusttemporalpro-cessingofnews.In38thAnnualMeetingonAsso-ciationforComputationalLinguistics,pages69–76.AssociationforComputationalLinguisticsMorris-town,NJ,USA.Murray,G.,S.Renals,andJ.Carletta.2005.ExtractiveSummarizationofMeetingRecordings.InNinthEu-ropeanConferenceonSpeechCommunicationandTechnology.ISCA.Salton,G.,A.Wong,andC.S.Yang.1975.Avectorspacemodelforautomaticindexing.Communica-tionsoftheACM,18(11):613–620.Swan,R.andJ.Allan.2000.Automaticgenerationofoverviewtimelines.In23rdannualinternationalACMSIGIRconferenceonResearchanddevelop-mentininformationretrieval,pages49–56.Winkler,W.E.1999.Thestateofrecordlinkageandcurrentresearchproblems.InSurveyMethodsSec-tion,pages73–79.Witte,R.,R.Krestel,andS.Bergler.2007.Generat-ingUpdateSummariesforDUC2007.InDocumentUnderstandingConference(DUC).Ye,S.,L.Qiu,T.S.Chua,andM.Y.Kan.2005.NUSatDUC2005:Understandingdocumentsviacon-ceptlinks.InDocumentUnderstandingConference(DUC).</auteur>
		<abstract>AScalableMMRApproachtoSentenceScoringforMulti-DocumentUpdateSummarizationFlorianBoudin\andMarcEl-B`eze\\LaboratoireInformatiqued’Avignon339chemindesMeinajaries,BP1228,84911AvignonCedex9,France.florian.boudin@univ-avignon.frmarc.elbeze@univ-avignon.frJuan-ManuelTorres-Moreno\,[[´EcolePolytechniquedeMontr´ealCP6079Succ.CentreVilleH3C3A7Montr´eal(Qu´ebec),Canada.juan-manuel.torres@univ-avignon.frAbstractWepresentSMMR,ascalablesentencescoringmethodforquery-orientedup-datesummarization.Sentencesarescoredthankstoacriterioncombiningqueryrele-vanceanddissimilaritywithalreadyreaddocuments(history).Astheamountofdatainhistoryincreases,non-redundancyisprioritizedoverquery-relevance.WeshowthatSMMRachievespromisingre-sultsontheDUC2007updatecorpus.1IntroductionExtensiveexperimentsonquery-orientedmulti-documentsummarizationhavebeencarriedoutoverthepastfewyears.Mostofthestrategiestoproducesummariesarebasedonanextrac-tionmethod,whichidentiﬁessalienttextualseg-ments,mostoftensentences,indocuments.Sen-tencescontainingthemostsalientconceptsarese-lected,orderedandassembledaccordingtotheirrelevancetoproducesummaries(alsocalledex-tracts)(ManiandMaybury,1999).RecentlyemergedfromtheDocumentUnder-standingConference(DUC)20071,updatesum-marizationattemptstoenhancesummarizationwhenmoreinformationaboutknowledgeacquiredbytheuserisavailable.Itasksthefollowingques-tion:hastheuseralreadyreaddocumentsonthetopic?Inthecaseofapositiveanswer,producinganextractfocusingononlynewfactsisofinter-est.Inthisway,animportantissueisintroduced:c(cid:13)2008.LicensedundertheCreativeCommonsAttribution-Noncommercial-ShareAlike3.0Unportedli-cense(http://creativecommons.org/licenses/by-nc-sa/3.0/).Somerightsreserved.1DocumentUnderstandingConferencesareconductedsince2000bytheNationalInstituteofStandardsandTech-nology(NIST),http://www-nlpir.nist.govredundancywithpreviouslyreaddocuments(his-tory)hastoberemovedfromtheextract.Anaturalwaytogoaboutupdatesummarizationwouldbeextractingtemporaltags(dates,elapsedtimes,temporalexpressions...)(ManiandWilson,2000)ortoautomaticallyconstructthetimelinefromdocuments(SwanandAllan,2000).Thesetemporalmarkscouldbeusedtofocusextractsonthemostrecentlywrittenfacts.However,mostre-centlywrittenfactsarenotnecessarilynewfacts.MachineReading(MR)wasusedby(Hickletal.,2007)toconstructknowledgerepresentationsfromclustersofdocuments.Sentencescontain-ing“new”information(i.e.thatcouldnotbein-ferredbyanypreviouslyconsidereddocument)areselectedtogeneratesummary.However,thishighlyefﬁcientapproach(bestsysteminDUC2007update)requireslargelinguisticresources.(Witteetal.,2007)proposearule-basedsystembasedonfuzzycoreferenceclustergraphs.Again,thisapproachrequirestomanuallywritethesen-tencerankingscheme.Severalstrategiesremain-ingonpost-processingredundancyremovaltech-niqueshavebeensuggested.Extractsconstructedfromhistorywereusedby(BoudinandTorres-Moreno,2007)tominimizehistory’sredundancy.(Linetal.,2007)haveproposedamodiﬁedMax-imalMarginalRelevance(MMR)(CarbonellandGoldstein,1998)re-rankerduringsentenceselec-tion,constructingthesummarybyincrementallyre-rankingsentences.Inthispaper,weproposeascalablesentencescoringmethodforupdatesummarizationderivedfromMMR.Motivatedbytheneedforrelevantnovelty,candidatesentencesareselectedaccord-ingtoacombinedcriterionofqueryrelevanceanddissimilaritywithpreviouslyreadsentences.Therestofthepaperisorganizedasfollows.Section2</abstract>
		<introduction>24 introducesourproposedsentencescoringmethodandSection3presentsexperimentsandevaluatesourapproach.2MethodTheunderlyingideaofourmethodisthatasthenumberofsentencesinthehistoryincreases,thelikelihoodtohaveredundantinformationwithincandidatesentencesalsoincreases.WeproposeascalablesentencescoringmethodderivedfromMMRthat,asthesizeofthehistoryincreases,givesmoreimportancetonon-redundancythattoqueryrelevance.WedeﬁneHtorepresentthepre-viouslyreaddocuments(history),Qtorepresentthequeryandsthecandidatesentence.Thefol-lowingsubsectionsformallydeﬁnethesimilaritymeasuresandthescalableMMRscoringmethod.2.1Aquery-orientedmulti-documentsummarizerWehaveﬁrststartedbyimplementingasimplesummarizerforwhichthetaskistoproducequery-focusedsummariesfromclustersofdocuments.Eachdocumentispre-processed:documentsaresegmentedintosentences,sentencesareﬁltered(wordswhichdonotcarrymeaningareremovedsuchasfunctionalwordsorcommonwords)andnormalizedusingalemmasdatabase(i.e.inﬂectedforms“go”,“goes”,“went”,“gone”...arereplacedby“go”).AnN-dimensionalterm-spaceΓ,whereNisthenumberofdifferenttermsfoundinthecluster,isconstructed.SentencesarerepresentedinΓbyvectorsinwhicheachcomponentisthetermfrequencywithinthesentence.Sentencescor-ingcanbeseenasapassageretrievaltaskinInfor-mationRetrieval(IR).Eachsentencesisscoredbycomputingacombinationoftwosimilaritymea-suresbetweenthesentenceandthequery.Theﬁrstmeasureisthewellknowncosineangle(Saltonetal.,1975)betweenthesentenceandthequeryvec-torialrepresentationsinΓ(denotedrespectively~sand~Q).ThesecondsimilaritymeasureisbasedontheJaro-Winklerdistance(Winkler,1999).TheoriginalJaro-Winklermeasure,denotedJW,usesthenumberofmatchingcharactersandtransposi-tionstocomputeasimilarityscorebetweentwoterms,givingmorefavourableratingstotermsthatmatchfromthebeginning.WehaveextendedthismeasuretocalculatethesimilaritybetweenthesentencesandthequeryQ:JWe(s,Q)=1|Q|·Xq∈Qmaxm∈S0JW(q,m)(1)whereS0isthetermsetofsinwhichthetermsmthatalreadyhavemaximizedJW(q,m)arere-moved.TheuseofJWesmoothsnormalizationandmisspellingerrors.Eachsentencesisscoredusingthelinearcombination:Sim1(s,Q)=α·cosine(~s,~Q)+(1−α)·JWe(s,Q)(2)whereα=0.7,optimallytunedonthepastDUCsdata(2005and2006).Thesystemproducesalistofrankedsentencesfromwhichthesummaryisconstructedbyarrangingthehighscoredsentencesuntilthedesiredsizeisreached.2.2AscalableMMRapproachMMRre-rankingalgorithmhasbeensuccessfullyusedinquery-orientedsummarization(Yeetal.,2005).Itstrivestoreduceredundancywhilemain-tainingqueryrelevanceinselectedsentences.Thesummaryisconstructedincrementallyfromalistofrankedsentences,ateachiterationthesentencewhichmaximizesMMRischosen:MMR=argmaxs∈S[λ·Sim1(s,Q)−(1−λ)·maxsj∈ESim2(s,sj)](3)whereSisthesetofcandidatessentencesandEisthesetofselectedsentences.λrepresentsaninterpolationcoefﬁcientbetweensentence’srele-vanceandnon-redundancy.Sim2(s,sj)isanor-malizedLongestCommonSubstring(LCS)mea-surebetweensentencessandsj.Detectingsen-tencerehearsals,LCSiswelladaptedforredun-dancyremoval.WeproposeaninterpretationofMMRtotackletheupdatesummarizationissue.SinceSim1andSim2arerangedin[0,1],theycanbeseenasprob-abilitieseventhoughtheyarenot.Justasrewriting(3)as(NRstandsforNoveltyRelevance):NR=argmaxs∈S[λ·Sim1(s,Q)+(1−λ)·(1−maxsh∈HSim2(s,sh))](4)Wecanunderstandthat(4)equatestoanORcom-bination.Butaswearelookingforamoreintu-itiveANDandsincethesimilaritiesareindepen-dent,wehavetousetheproductcombination.The 25 scoringmethoddeﬁnedin(2)ismodiﬁedintoadoublemaximizationcriterioninwhichthebestrankedsentencewillbethemostrelevanttothequeryANDthemostdifferenttothesentencesinH.SMMR(s)=Sim1(s,Q)·(cid:18)1−maxsh∈HSim2(s,sh)(cid:19)f(H)(5)Decreasingλin(3)withthelengthofthesum-marywassuggestedby(Murrayetal.,2005)andsuccessfullyusedintheDUC2005by(Hacheyetal.,2005),therebyemphasizingtherelevanceattheoutsetbutincreasinglyprioritizingredun-dancyremovalastheprocesscontinues.Sim-ilarly,weproposetofollowthisassumptioninSMMRusingafunctiondenotedfthatastheamountofdatainhistoryincreases,prioritizenon-redundancy(f(H)→0).3ExperimentsThemethoddescribedintheprevioussectionhasbeenimplementedandevaluatedbyusingtheDUC2007updatecorpus2.Thefollowingsubsec-tionspresentdetailsofthedifferentexperimentswehaveconducted.3.1TheDUC2007updatecorpusWeusedforourexperimentstheDUC2007up-datecompetitiondataset.Thecorpusiscomposedof10topics,with25documentspertopic.Theup-datetaskgoalwastoproduceshort(∼100words)multi-documentupdatesummariesofnewswirear-ticlesundertheassumptionthattheuserhasal-readyreadasetofearlierarticles.Thepurposeofeachupdatesummarywillbetoinformthereaderofnewinformationaboutaparticulartopic.GivenaDUCtopicandits3documentclusters:A(10documents),B(8documents)andC(7doc-uments),thetaskistocreatefromthedocumentsthreebrief,ﬂuentsummariesthatcontributetosat-isfyingtheinformationneedexpressedinthetopicstatement.1.AsummaryofdocumentsinclusterA.2.AnupdatesummaryofdocumentsinB,un-dertheassumptionthatthereaderhasalreadyreaddocumentsinA.2MoreinformationabouttheDUC2007corpusisavail-ableathttp://duc.nist.gov/.3.AnupdatesummaryofdocumentsinC,un-dertheassumptionthatthereaderhasalreadyreaddocumentsinAandB.Withinatopic,thedocumentclustersmustbepro-cessedinchronologicalorder.Oursystemgener-atesasummaryforeachclusterbyarrangingthehighrankedsentencesuntilthelimitof100wordsisreached.3.2EvaluationMostexistingautomatedevaluationmethodsworkbycomparingthegeneratedsummariestooneormorereferencesummaries(ideally,producedbyhumans).Toevaluatethequalityofourgeneratedsummaries,wechoosetousetheROUGE3(Lin,2004)evaluationtoolkit,thathasbeenfoundtobehighlycorrelatedwithhumanjudgments.ROUGE-Nisan-gramrecallmeasurecalculatedbetweenacandidatesummaryandasetofreferencesum-maries.InourexperimentsROUGE-1,ROUGE-2andROUGE-SU4willbecomputed.3.3ResultsTable1reportstheresultsobtainedontheDUC2007updatedatasetfordifferentsentencescor-ingmethods.cosine+JWestandsforthescor-ingmethoddeﬁnedin(2)andNRimprovesitwithsentencere-rankingdeﬁnedinequation(4).SMMRisthecombinedadaptationwehavepro-posedin(5).Thefunctionf(H)usedinSMMRisthesimplerationalfunction1H,whereHincreaseswiththenumberofpreviousclusters(f(H)=1forclusterA,12forclusterBand13forclusterC).Thisfunctionallowstosimplytesttheassumptionthatnon-redundancyhavetobefavouredasthesizeofhistorygrows.Baselineresultsareobtainedonsummariesgeneratedbytakingtheleadingsen-tencesofthemostrecentdocumentsofthecluster,upto100words(ofﬁcialbaselineofDUC).ThetablealsoliststhethreetopperformingsystemsatDUC2007andthelowestscoredhumanreference.Aswecanseefromtheseresults,SMMRout-performstheothersentencescoringmethods.BywaysofcomparisonoursystemwouldhavebeenrankedsecondattheDUC2007updatecompeti-tion.Moreover,nopost-processingwasappliedtotheselectedsentencesleavinganimportantmarginofprogress.Anotherinterestingresultisthehighperformanceofthenon-updatespeciﬁcmethod(cosine+JWe)thatcouldbeduetothesmallsize3ROUGEisavailableathttp://haydn.isi.edu/ROUGE/. 26 ofthecorpus(littleredundancybetweenclusters).ROUGE-1ROUGE-2ROUGE-SU4Baseline0.262320.045430.082473rdsystem0.357150.096220.132452ndsystem0.369650.098510.13509cosine+JWe0.359050.101610.13701NR0.362070.100420.13781SMMR0.363230.102230.138861stsystem0.370320.111890.14306Worsthuman0.404970.105110.14779Table1:ROUGEaveragerecallscorescomputedontheDUC2007updatecorpus.4DiscussionandFutureWorkInthispaperwehavedescribedSMMR,ascal-ablesentencescoringmethodbasedonMMRthatachievesverypromisingresults.Animportantas-pectofoursentencescoringmethodisthatitdoesnotrequiresre-rankingnorlinguisticknowledge,whichmakesitasimpleandfastapproachtotheissueofupdatesummarization.ItwaspointedoutattheDUC2007workshopthatQuestionAnswer-ingandquery-orientedsummarizationhavebeenconvergingonacommontask.Thevalueaddedbysummarizationliesinthelinguisticquality.Ap-proachesmixingIRtechniquesarewellsuitedforquery-orientedsummarizationbuttheyrequirein-tensiveworkformakingthesummaryﬂuentandcoherent.Amongtheothers,thisisapointthatwethinkisworthyoffurtherinvestigation.AcknowledgmentsThisworkwassupportedbytheAgenceNationaledelaRecherche,France,projectRPM2.ReferencesBoudin,F.andJ.M.Torres-Moreno.2007.ACo-sineMaximization-MinimizationapproachforUser-OrientedMulti-DocumentUpdateSummarization.InRecentAdvancesinNaturalLanguageProcessing(RANLP),pages81–87.Carbonell,J.andJ.Goldstein.1998.TheuseofMMR,diversity-basedrerankingforreorderingdocumentsandproducingsummaries.In21stannualinterna-tionalACMSIGIRconferenceonResearchandde-velopmentininformationretrieval,pages335–336.ACMPressNewYork,NY,USA.Hachey,B.,G.Murray,andD.Reitter.2005.TheEmbraSystematDUC2005:Query-orientedMulti-documentSummarizationwithaVeryLargeLatentSemanticSpace.InDocumentUnderstandingCon-ference(DUC).Hickl,A.,K.Roberts,andF.Lacatusu.2007.LCC’sGISTexteratDUC2007:MachineReadingforUp-dateSummarization.InDocumentUnderstandingConference(DUC).Lin,Z.,T.S.Chua,M.Y.Kan,W.S.Lee,L.Qiu,andS.Ye.2007.NUSatDUC2007:UsingEvolu-tionaryModelsofText.InDocumentUnderstandingConference(DUC).Lin,C.Y.2004.Rouge:APackageforAutomaticEvaluationofSummaries.InWorkshoponTextSum-marizationBranchesOut,pages25–26.Mani,I.andM.T.Maybury.1999.AdvancesinAuto-maticTextSummarization.MITPress.Mani,I.andG.Wilson.2000.Robusttemporalpro-cessingofnews.In38thAnnualMeetingonAsso-ciationforComputationalLinguistics,pages69–76.AssociationforComputationalLinguisticsMorris-town,NJ,USA.Murray,G.,S.Renals,andJ.Carletta.2005.ExtractiveSummarizationofMeetingRecordings.InNinthEu-ropeanConferenceonSpeechCommunicationandTechnology.ISCA.Salton,G.,A.Wong,andC.S.Yang.1975.Avectorspacemodelforautomaticindexing.Communica-tionsoftheACM,18(11):613–620.Swan,R.andJ.Allan.2000.Automaticgenerationofoverviewtimelines.In23rdannualinternationalACMSIGIRconferenceonResearchanddevelop-mentininformationretrieval,pages49–56.Winkler,W.E.1999.Thestateofrecordlinkageandcurrentresearchproblems.InSurveyMethodsSec-tion,pages73–79.Witte,R.,R.Krestel,andS.Bergler.2007.Generat-ingUpdateSummariesforDUC2007.InDocumentUnderstandingConference(DUC).Ye,S.,L.Qiu,T.S.Chua,andM.Y.Kan.2005.NUSatDUC2005:Understandingdocumentsviacon-ceptlinks.InDocumentUnderstandingConference(DUC).</introduction>
		<corps>ofthecorpus(littleredundancybetweenclusters).ROUGE-1ROUGE-2ROUGE-SU4Baseline0.262320.045430.082473rdsystem0.357150.096220.132452ndsystem0.369650.098510.13509cosine+JWe0.359050.101610.13701NR0.362070.100420.13781SMMR0.363230.102230.138861stsystem0.370320.111890.14306Worsthuman0.404970.105110.14779Table1:ROUGEaveragerecallscorescomputedontheDUC2007updatecorpus.4DiscussionandFutureWorkInthispaperwehavedescribedSMMR,ascal-ablesentencescoringmethodbasedonMMRthatachievesverypromisingresults.Animportantas-pectofoursentencescoringmethodisthatitdoesnotrequiresre-rankingnorlinguisticknowledge,whichmakesitasimpleandfastapproachtotheissueofupdatesummarization.ItwaspointedoutattheDUC2007workshopthatQuestionAnswer-ingandquery-orientedsummarizationhavebeenconvergingonacommontask.Thevalueaddedbysummarizationliesinthelinguisticquality.Ap-proachesmixingIRtechniquesarewellsuitedforquery-orientedsummarizationbuttheyrequirein-tensiveworkformakingthesummaryﬂuentandcoherent.Amongtheothers,thisisapointthatwethinkisworthyoffurtherinvestigation.AcknowledgmentsThisworkwassupportedbytheAgenceNationaledelaRecherche,France,projectRPM2.ReferencesBoudin,F.andJ.M.Torres-Moreno.2007.ACo-sineMaximization-MinimizationapproachforUser-OrientedMulti-DocumentUpdateSummarization.InRecentAdvancesinNaturalLanguageProcessing(RANLP),pages81–87.Carbonell,J.andJ.Goldstein.1998.TheuseofMMR,diversity-basedrerankingforreorderingdocumentsandproducingsummaries.In21stannualinterna-tionalACMSIGIRconferenceonResearchandde-velopmentininformationretrieval,pages335–336.ACMPressNewYork,NY,USA.Hachey,B.,G.Murray,andD.Reitter.2005.TheEmbraSystematDUC2005:Query-orientedMulti-documentSummarizationwithaVeryLargeLatentSemanticSpace.InDocumentUnderstandingCon-ference(DUC).Hickl,A.,K.Roberts,andF.Lacatusu.2007.LCC’sGISTexteratDUC2007:MachineReadingforUp-dateSummarization.InDocumentUnderstandingConference(DUC).Lin,Z.,T.S.Chua,M.Y.Kan,W.S.Lee,L.Qiu,andS.Ye.2007.NUSatDUC2007:UsingEvolu-tionaryModelsofText.InDocumentUnderstandingConference(DUC).Lin,C.Y.2004.Rouge:APackageforAutomaticEvaluationofSummaries.InWorkshoponTextSum-marizationBranchesOut,pages25–26.Mani,I.andM.T.Maybury.1999.AdvancesinAuto-maticTextSummarization.MITPress.Mani,I.andG.Wilson.2000.Robusttemporalpro-cessingofnews.In38thAnnualMeetingonAsso-ciationforComputationalLinguistics,pages69–76.AssociationforComputationalLinguisticsMorris-town,NJ,USA.Murray,G.,S.Renals,andJ.Carletta.2005.ExtractiveSummarizationofMeetingRecordings.InNinthEu-ropeanConferenceonSpeechCommunicationandTechnology.ISCA.Salton,G.,A.Wong,andC.S.Yang.1975.Avectorspacemodelforautomaticindexing.Communica-tionsoftheACM,18(11):613–620.Swan,R.andJ.Allan.2000.Automaticgenerationofoverviewtimelines.In23rdannualinternationalACMSIGIRconferenceonResearchanddevelop-mentininformationretrieval,pages49–56.Winkler,W.E.1999.Thestateofrecordlinkageandcurrentresearchproblems.InSurveyMethodsSec-tion,pages73–79.Witte,R.,R.Krestel,andS.Bergler.2007.Generat-ingUpdateSummariesforDUC2007.InDocumentUnderstandingConference(DUC).Ye,S.,L.Qiu,T.S.Chua,andM.Y.Kan.2005.NUSatDUC2005:Understandingdocumentsviacon-ceptlinks.InDocumentUnderstandingConference(DUC).</corps>
		<conclusion>Aucune conclusion trouvée.</conclusion>
		<discussion>Aucune discussion trouvée.</discussion>
		<biblio>Aucune bibliographie trouvée.</biblio>
	</article>
	<article>
		<preamble>compression.txt</preamble>
		<titre>Multi-Candidate Reduction: Sentence Compression as a Tool for Document Summarization Tasks∗</titre>
		<auteur>David Zajic1, Bonnie J. Dorr1, Jimmy Lin1, Richard Schwartz2 1University of Maryland</auteur>
		<abstract>This article examines the application of two single-document sentence compression techniques to the problem of multi-document summarization—a “parse-and-trim” approach and a statisti- cal noisy-channel approach. We introduce the Multi-Candidate Reduction (MCR) framework for multi-document summarization, in which many compressed candidates are generated for each source sentence. These candidates are then selected for inclusion in the ﬁnal summary based on a com- bination of static and dynamic features. Evaluations demonstrate that sentence compression is a valuable component of a larger multi-document summarization framework. Keywords: headline generation, summarization, parse-and-trim, Hidden Markov Model PACS: Artiﬁcial intelligence, 07.05.Mh; Computer science and technology, 89.20.Ff; Spoken lan- guages, processing of, 43.71.Sy</abstract>
		<introduction>This article presents an application of two diﬀerent single-document sentence compression methods to the problem of multi-document summarization. The ﬁrst, a “parse-and-trim” approach, has been implemented in a system called Trimmer and its extended version called Topiary. The second, an HMM-based approach, has been implemented in a system called HMM Hedge. These systems share the basic premise that a textual summary can be constructed by selecting a subset of words, in order, from the original text, with morphological variation allowed for some word classes. Trimmer selects subsequences of words using a linguistically-motivated algorithm to trim syntactic constituents from sentences until a desired length has been reached. In the context of ∗Please cite as: David Zajic, Bonnie Dorr, Jimmy Lin, and Richard Schwartz. Multi-Candidate Reduction: Sentence Compression as a Tool for Document Summarization Tasks. Information Processing and Management, 43(6):1549-1570, 2007. [DOI: 10.1016/j.ipm.2007.01.016] This is the pre-print version of a published article. Citations to and quotations from this work should reference that publication. If you cite this work, please check that the published form contains precisely the material to which you intend to refer. (Received 18 July 2006; revised 3 January 2007; accepted 8 January 2007) This document prepared August 31, 2007, and may have minor diﬀerences from the published version. 1 Figure 1: The Multi-Candidate Reduction (MCR) framework. very short single-document summarization, or headline generation, this approach is applied to the lead sentence (the ﬁrst non-trivial sentence) in order to produce a short headline tens of characters long. HMM Hedge (Hidden Markov Model HEaDline GEnerator), on the other hand, is a statistical headline generation system that ﬁnds the subsequence of words most likely to be a headline for a given story. This approach is similar to techniques used in statistical machine translation in that the observed story is treated as the garbled version of an unseen headline transmitted through a noisy channel. In their original conﬁgurations, both Trimmer and HMM Hedge create sentence compressions that mimic the style of Headlinese, a form of compressed English associated with newspaper headlines (M˚ardh, 1980). The goal of our work is to transition from a single-document summarization task with sentence- level character-based length constraint to a multi-document summarization task with summary-level word-based length constraint. In headline generation, summaries should ﬁll as much of the available space as possible, without going over the limit. Headlines that fall below the length limit miss the opportunity to include additional information. Headlines that exceed the length limit must be trun- cated, resulting in ungrammatical sentences and loss of information. When the length constraint is applied to a collection of sentences instead of a single sentence, concerns about individual sentence length become less important. A short sentence does not result in wasted space and it is acceptable to include a lengthy sentence as long as it includes important, non-redundant information. To transition from single-document to multi-document summarization, we examined diﬀerent com- binations of possible sentence compressions to construct the best summary. A key innovation of this work—tested for the ﬁrst time in 2005 at the Document Understanding Conference (DUC-2005) (Zajic et al., 2005b) and Multilingual Summarization Evaluation (MSE-2005) (Zajic et al., 2005a)—is the use of multiple compressed candidates for sentences in the source documents. Sentence compression is used for diﬀerent purposes in single- and multi-document summarization tasks. In the ﬁrst case, the goal is to ﬁt long sentences into the available space while preserving important information. In multi- document summarization, sentence compression is used to generate multiple candidates that capture relevant, non-redundant information. Sentence selection algorithms can then be applied to determine which compressed candidates provide the best combination of topic coverage and brevity. We introduce a framework for extractive multi-document summarization called Multi-Candidate Reduction (MCR), whose architecture is shown in Figure 1. Sentence compression techniques are em- ployed in the intermediate stage of the processing pipeline to generate multiple compressed variants of source sentences. A sentence selector then builds the ﬁnal summary by choosing among these candi- dates, based on features propagated from the sentence compression method, features of the candidates 2 Sentence FilteringSentence CompressionSentence SelectionDocumentsSummarySentencesCandidatesTask-Specific Features(e.g., query) themselves, and features of the present summary state. Under this framework, our previously-developed systems, Trimmer and HMM Hedge, are employed to generate the compressed candidates. The MCR architecture also includes a ﬁltering module that chooses the source sentences from which compressed candidates are generated. This work does not examine the ﬁltering process in detail, as we only employ very simple approaches, e.g., retain ﬁrst n sentences from each document. Sentence compression supports extractive multi-document summarization by reducing the length of summary candidates while preserving their relevant content, thus allowing space for the inclusion of additional material. However, given the interaction of relevance and redundancy in a multi-sentence summary, it is unlikely that a single algorithm or scoring metric can provide the “best” compression of a sentence. This is the motivation for MCR, which provides several alternative candidates for each source sentence. Subsequent processes can then select among many alternatives when constructing the ﬁnal summary. This article is organized as follows: The next section relates our approach to other existing summa- rization systems. In Section 3, we describe Trimmer, a variant of Trimmer called Topiary, and HMM Hedge. These systems implement two fundamentally diﬀerent approaches to sentence compression. Section 4 describes how these tools can be applied to a multi-document summarization task. Section 5 describes evaluations of our framework. Our systems have also been applied to several diﬀerent types of texts; some of these applications will be brieﬂy covered in Section 6. Finally, we propose future work in the area before concluding.</introduction>
		<corps>2 Related Work

We have developed a “parse-and-trim” approach to sentence compression, implemented in a system
called Trimmer (Dorr et al., 2003b). The system generates a headline for a news story by compressing
the lead (or main) topic sentence according to a linguistically-motivated algorithm that operates on
syntactic structures. Syntactic approaches to compression have been used in single-document sum-
marization systems such as Cut-and-Paste (Jing and McKeown, 2000) and also in multi-document
summarization systems such as SC (Blair-Goldensohn et al., 2004) and CLASSY (Conroy et al., 2005;
Conroy et al., 2006). The SC system pre-processes input to remove appositives and relative clauses.
CLASSY uses an HMM sentence selection approach combined with a conservative sentence compression
method based on shallow parsing to detect lexical cues to trigger phrase eliminations. The approach
taken by Trimmer is most similar to that of Jing and McKeown (2000), in which algorithms for remov-
ing syntactic constituents from sentences are informed by analysis of human summarizers. Trimmer
diﬀers from these systems in that multiple compressed candidates of each sentence are generated. The
potential of multiple alternative compressions has also been explored by Vanderwende et al. (2006).

Summaries may also contain lists of words or short phrases that denote important topics or concepts
in the document. In particular, extractive topic summaries consist of keywords or key phrases that
occur in the document. We have built Topiary, an extension to Trimmer, that constructs headlines
combining compressed sentences with topic terms. This approach is similar to the work of Euler (2002),
except that Euler uses topic lists to guide sentence selection and compression toward a query-speciﬁc
summary, whereas Topiary uses topics to augment the concept coverage of a generic summary. Our
system was demonstrated to be the top-performing single-document summarization system in the
DUC-2004 evaluation (Zajic et al., 2004).

In contrast to Topiary, which combines sentence compression with topic terms, others have con-
structed summaries directly from topic terms. For example, Bergler et al. (2003) choose noun phrases
that represent the most important entities as determined by noun phrase coreference chains. Wang et
al. (2005) propose a baseline system that constructs headlines from topic descriptors identiﬁed using
term frequency counts; this system was reported to outperform LexTrim, their Topiary-style system.
Zhou and Hovy (2003) construct ﬂuent summaries from a topic list by ﬁnding phrase clusters early in

3


the text that contain important topic words found throughout the text.

The task of assigning topic terms to documents is related to text categorization, in which documents
are assigned to pre-deﬁned categories. The categories can be labeled with topic terms, so that the
decision to put a document in a category is equivalent to assigning that category’s label to the document.
Assigning topic terms to documents by categorization permits the assignment of terms that do not occur
in the document. Lewis (1999) describes a probabilistic feature-based method for assigning Reuters
topics to news stories. Along these lines, OnTopicTM (Schwartz et al., 1997) uses an HMM to assign
topics to a document.

In addition to Trimmer, we have implemented HMM Hedge (Hidden Markov Model HEaDline
GEnerator), a statistical headline generation system that ﬁnds the most likely headline for a given story.
This approach is similar to techniques used in statistical machine translation in that the observed story
is treated as the garbled version of an unseen headline transmitted through a noisy channel. The noisy-
channel approach has been used for a wide range of Natural Language Processing (NLP) applications
including speech recognition (Bahl et al., 1983); machine translation (Brown et al., 1990); spelling
correction (Mays et al., 1990); language identiﬁcation (Dunning, 1994); and part-of-speech tagging
(Cutting et al., 1992). Of those who have taken a noisy-channel approach to sentence compression
for text summarization Banko et al. (2000), Knight and Marcu (2000), Knight and Marcu (2002),
and Turner and Charniak (2005) have used the technique to ﬁnd the most probable short string that
generated the observed full sentence.

Like other summarization systems based on the noisy-channel model, HMM Hedge treats the ob-
served data (the story) as the result of unobserved data (headlines) that have been distorted by trans-
mission through a noisy channel. The eﬀect of the noisy channel is to add story words between the
headline words. Our approach diﬀers from Knight and Marcu (2000), Banko et al. (2000), and Turner
and Charniak (2005), in that HMM Hedge does not require a corpus of paired stories and summaries.
HMM Hedge uses distinct language models of news stories and headlines, but does not require explicit
pairings of stories and summaries.

To transition our single-sentence summarization techniques to the problem of multi-document sum-
marization, we must consider how to select candidates for inclusion in the ﬁnal summary. A common
approach is to rank candidate sentences according to a set of features and iteratively build the sum-
mary, appropriately re-ranking the candidates at each step to avoid redundancy. MEAD (Radev et al.,
2004) scores source sentences according to a linear combination of features including centroid, position,
and ﬁrst-sentence overlap. These scores are then reﬁned to consider cross-sentence dependencies, such
as redundancy, chronological order, and source preferences. MCR diﬀers in that multiple variants of
the same source sentences are available as candidates for inclusion in the ﬁnal summary.

Minimization of redundancy is an important element of a multi-document summarization system.
Carbonell and Goldstein (1998) propose a technique called Maximal Marginal Relevance (MMR) for
ranking documents returned by an information retrieval system so that the front of the ranked list will
contain diversity as well as high relevance. Goldstein et al. (2000) extend MMR to multi-document
summarization. MCR borrows the ranking approach of MMR, but uses a diﬀerent set of features. Like
MEAD, these approaches use feature weights that are optimized to maximize an automatic metric on
training data.

Several researchers have shown the importance of summarization in domains other than written
news (Muresan et al., 2001; Clarke and Lapata, 2006). Within the MCR framework, we discuss the
portability of Trimmer and HMM Hedge to a variety of diﬀerent texts: written news, broadcast news
transcriptions, email threads, and text in foreign language.

4


3 Single-Sentence Compression

Our general approach to the generation of a summary from a single document is to produce a headline
by selecting words in order from the text of the story. Consider the following excerpt from a news story
and corresponding headline:

(1)

(i) After months of debate following the Sept. 11 terrorist hijackings, the Transportation
Department has decided that airline pilots will not be allowed to have guns in the
cockpits.

(ii) Pilots not allowed to have guns in cockpits.

The bold words in (1i) form a ﬂuent and accurate headline, as shown in (1ii).
This basic approach has been realized in two ways. The ﬁrst, Trimmer, uses a linguistically-
motivated algorithm to remove grammatical constituents from the lead sentence until a length threshold
is met. Topiary is a variant of Trimmer that combines ﬂuent text from a compressed sentence with
topic terms to produce headlines. The second, HMM Hedge, employs a noisy-channel model to ﬁnd
the most likely headline for a given story. The remainder of this section will present Trimmer, Topiary,
and HMM Hedge in more detail.

3.1 Trimmer

Our ﬁrst approach to sentence compression involves iteratively removing grammatical constituents from
the parse tree of a sentence using linguistically-motivated rules until a length threshold has been met.
When applied to the lead sentence, or ﬁrst non-trivial sentence of a story, our algorithm generates a
very short summary, or headline. This idea is implemented in our Trimmer system, which can leverage
the output of any constituency parser that uses the Penn Treebank conventions. At present we use
Charniak’s parser (Charniak, 2000).

The insights that form the basis and justiﬁcation for the Trimmer rules come from our previous
study, which compared the relative prevalence of certain constructions in human-written summaries
and lead sentences in stories. This study used 218 human-written summaries of 73 documents from
the TIPSTER corpus (Harman and Liberman, 1993) dated January 1, 1989. The 218 summaries and
the lead sentences of the 73 stories were parsed using the BBN SIFT parser (Miller et al., 2000). The
parser produced 957 noun phrases (NP nodes in the parse trees) and 315 clauses (S nodes in the parse
trees) for the 218 summaries. For the 73 lead sentences, the parser produced 817 noun phrases and
316 clauses.

At each level (sentence, clause, and noun phrase), diﬀerent types of linguistic phenomena were

counted.

• At the sentence level, the numbers of preposed adjuncts, conjoined clauses, and conjoined verb
phrases were counted. Children of the root S node that occur to the left of the ﬁrst NP are
considered to be preposed adjuncts. The bracketed phase in “[According to police] the crime rate
has gone down” is a prototypical example of a preposed adjunct.

• At the clause level, temporal expressions, trailing SBAR nodes, and trailing PP nodes were

counted. Trailing constituents are those not designated as an argument of a verb phrase.

• At both the sentence and clause levels, conjoined S nodes and conjoined VP nodes were counted.

• At the NP level, determiners and relative clauses were counted.

The counts and prevalence of the phenomena in the human-generated headlines and lead sentences
are shown in Table 1. The results of this analysis illuminated the opportunities for trimming con-
stituents and guided the development of our Trimmer rules, detailed below.

5


Summary

Level
Sentence

Clause

Noun Phrase

Phenomenon
preposed adjuncts
conjoined S
conjoined VP
temporal expression
trailing PP
trailing SBAR
relative clause
determiner

0/218
1/218
7/218
5/315
165/315
24/315
3/957
31/957

Lead Sentence
2.7%
2/73
0%
4%
0.5% 3/73
27%
3%
20/73
24%
1.5% 77/316
58%
52% 184/316
16%
8%
49/316
3.5%
0.3% 29/817
25%
3%

205/817

Table 1: Counts and prevalence of phenomena found in summaries and lead sentences.

3.1.1 Trimmer Algorithm

Trimmer applies syntactic compression rules to a parse tree according the following algorithm:

1. Remove temporal expressions

2. Select Root S node

3. Remove preposed adjuncts

4. Remove some determiners

5. Remove conjunctions

6. Remove modal verbs

7. Remove complementizer that

8. Apply the XP over XP rule

9. Remove PPs that do not contain named entities

10. Remove all PPs under SBARs

11. Remove SBARSs

12. Backtrack to state before Step 9

13. Remove SBARs

14. Remove PPs that do not contain named entities

15. Remove all PPs

Steps 1 and 4 of the algorithm remove low-content units from the parse tree.
Temporal expressions—although certainly not content-free—are not usually vital for summarizing
the content of an article. Since the goal is to provide an informative headline, the identiﬁcation and
elimination of temporal expressions (Step 1) allow other more important details to remain in the length-
constrained headline. The use of BBN’s IdentiFinderTM (Bikel et al., 1999) for removal of temporal
expressions is described in Section 3.1.2.

The determiner rule (Step 4) removes leaf nodes that are assigned the part-of-speech tag DT and
have the surface form the, a, or an. The intuition for this rule is that the information carried by

6


articles is expendable in summaries, even though this makes the summaries ungrammatical for general
English. Omitting articles is one of the most salient features of newspaper headlines. Sentences (2)
and (3), taken from the New York Times website on September 27, 2006, illustrate this phenomenon.
The italicized articles did not occur in the actual newspaper headlines.

(2) The Gotti Case Ends With a Mistrial for the Third Time in a Year

(3) A Texas Case Involving Marital Counseling Is the Latest to Test the Line Between Church and

State

Step 2 identiﬁes nodes in the parse tree of a sentence that could serve as the root of a compression
for the sentence. Such nodes will be referred to as Root S nodes. A node in a tree is a Root S node if it
is labeled S in the parse tree and has children labeled NP and VP, in that order. The human-generated
headlines we studied always conform to this rule. It has been adopted as a constraint in the Trimmer
algorithm that the lowest leftmost Root S node is taken to be the root node of the headline. An
example of this rule application is shown in (4). The boldfaced material in the parse is retained and
the italicized material is eliminated.

(4)

(i)

Input: Rebels agreed to talks with government oﬃcials, international observers said Tuesday.

(ii) Parse: [S [S [NP Rebels][VP agreed to talks with government oﬃcials]], interna-

tional observers said Tuesday.]

(iii) Output: Rebels agreed to talks with government oﬃcials.

When the parser produces a usable parse tree, this rule selects a valid starting point for compression.

However, the parser sometimes produces incorrect output, as in the cases below (from DUC-2003):

(5)

(i)

Parse: [S[SBAR What started as a local controversy][VP has evolved into an
international scandal.]]

(ii) Parse: [NP[NP Bangladesh][CC and][NP[NP India][VP signed a water sharing

accord.]]]

In (5i), an S exists, but it does not conform to the requirements of the Root S rule because it does
not have as children an NP followed by a VP. The problem is resolved by selecting the lowest leftmost
In (5ii), no S is present in the parse. This problem is
S, ignoring the constraints on the children.
resolved by selecting the root of the entire parse tree as the root of the headline. These parsing errors
occur infrequently—only 6% of the sentences in the DUC-2003 evaluation data exhibit these problems,
based on parses generated by the BBN SIFT parser.

The motivation for removing preposed adjuncts (Step 3) is that all of the human-generated headlines
omit what we refer to as the preamble of the sentence. Preposed adjuncts are constituents that precede
the ﬁrst NP (the subject) under the Root S chosen in Step 2; the preamble of a sentence consists of its
preposed adjuncts. The impact of preposed adjunct removal can be seen in example (6).

(6)

(i)

Input: According to a now ﬁnalized blueprint described by U.S. oﬃcials and other sources,
the Bush administration plans to take complete, unilateral control of a post-Saddam Hussein
Iraq.

(ii) Parse: [S[PP According to a now ﬁnalized blueprint described by U.S. oﬃcials and other
sources], [Det the] Bush administration plans to take complete, unilateral control
of[Det a] post-Saddam Hussein Iraq.]

7


(iii) Output: Bush administration plans to take complete unilateral control of post-Saddam

Hussein Iraq.

The remaining steps of the algorithm remove linguistically peripheral material through successive
deletions of constituents until the sentence is shorter than a length threshold. Each stage of the
algorithm corresponds to the application of one of the rules. Trimmer ﬁrst ﬁnds the pool of nodes in
the parse to which a rule can be applied. The rule is then iteratively applied to the deepest, rightmost
remaining node in the pool until the length threshold is reached or the pool is exhausted. After a rule
has been applied at all possible nodes in the parse tree, the algorithm moves to the next step.

In the case of a conjunction with two children (Step 5), one of the children will be removed. If the
conjunction is and , the second child is removed. If the conjunction is but, the ﬁrst child is removed.
This rule is illustrated by the following examples, where the italicized text is trimmed.

(7) When Sotheby’s sold a Harry S Truman signature that turned out to be a reproduction, the

prestigious auction house apologized and bought it back .

(8) President Clinton expressed sympathy after a car-bomb explosion in a Jerusalem market wounded
24 people but said the attack should not derail the recent land-for-security deal between Israel
and the Palestinians.

The modal verb rule (Step 6) applies to verb phrases in which the head is a modal verb and the
head of the child verb phrase is a form of have or be. In such cases, the modal and auxiliary verbs
are removed. Sentences (9) and (10) show examples of this rule application. Note that although in
Sentence (10) the omission of trimmed material changes the meaning, given a tight space constraint,
the loss of the modality is preferable to the loss of other content information.

(9) People’s palms and ﬁngerprints may be used to diagnose schizophrenia.

(10) Agents may have ﬁred potentially ﬂammable tear gas cannisters.

The complementizer rule (Step 7) removes the word that when it occurs as a complementizer.

Sentence (11) shows an example in which two complementizers can be removed.

(11) Hoﬀman stressed that study is only preliminary and can’t prove that treatment useful.

The XP-over-XP rule (Step 8) is a linguistic generalization that allows a single rule to cover two
diﬀerent phenomena. XP in the name of the rule is a variable that can take two values: NP and VP.
In constructions of the form [XP [XP ...] ...], the other children of the higher XP are removed. Note
that the child XP must be the ﬁrst child of the parent XP. When XP = NP the rule removes relative
clauses (as in Sentence (12)) and appositives (as in Sentence (13)).

(12) Schizophrenia patients whose medication couldn’t stop the imaginary voices in their heads gained

some relief.

(13) A team led by Dr. Linda Brzustowicz, assistant professor of neuroscience at Rutgers University’s
Center for Molecular and Behavioral Neuroscience in Newark, studied DNA of dozens of members
of 22 families.

The rules that remove prepositional phrases and subordinate clauses (Steps 9 through 15) are
sometimes prone to removing important content. Thus, these rules are applied last, only when there
are no other types of rules to apply. Moreover, these rules are applied with a backoﬀ option to avoid

8


over-trimming the parse tree. First, the PP rule is applied (Steps 9 and 10),1 followed by the SBAR
rule (Step 11). If the desired sentence length has not been reached, the system reverts to the parse tree
as it was before any prepositional phrases were removed (Step 12) and applies the SBAR rule (Step
13). If the desired length still has not been reached, the PP rule is applied again (Steps 14 and 15).

The intuition behind this ordering is that, when removing constituents from a parse tree, it is prefer-
able to remove smaller fragments before larger ones and prepositional phrases tend to be smaller than
subordinate clauses. Thus, Trimmer ﬁrst attempts to achieve the desired length by removing smaller
constituents (PPs), but if this cannot be accomplished, the system restores the smaller constituents,
removes a larger constituent, and then resumes the deletion of the smaller constituents. To reduce the
risk of removing prepositional phrases that contain important information, BBN’s IdentiFinder is used
to distinguish PPs containing temporal expressions and named entities, as described next.

3.1.2 Use of BBN’s IdentiFinder in Trimmer

BBN’s IdentiFinder is used both for the elimination of temporal expressions and for conservative
deletion of PPs containing named entities. The elimination of temporal expressions (Step 1) is a two-
[NP [X] ...]
step process: (a) use IdentiFinder to mark temporal expressions; and (b) remove [PP ...
...] and [NP [X]] where X is tagged as part of a temporal expression. The following examples illustrate
the application of temporal expression removal rule:

(14) (i)

Input: The State Department on Friday lifted the ban it had imposed on foreign ﬂiers.
(ii) Parse: [S [NP[Det The] State Department [PP [IN on] [NP [NNP Friday]]] [VP lifted

[Det the] ban it had imposed on foreign ﬂiers.]]

(iii) Output: State Department lifted ban it had imposed on foreign ﬂiers.

(15) (i)

Input: An international relief agency announced Wednesday that it is withdrawing from
North Korea.

(ii) Parse: [S [NP [Det An] international relief agency][VP announced [NP [NNP Wednes-

day]] that it is withdrawing from North Korea.]]

(iii) Output: International relief agency announced that it is withdrawing from North Korea.

IdentiFinder is also used to ensure that prepositional phrases containing named entities are not
removed during the ﬁrst round of PP removal (Step 9). However, prepositional phrases containing
named entities that are descendants of SBARs are removed before the parent SBAR is removed, since
we should remove a smaller constituent before removing a larger constituent that subsumes it. Sentence
(16) shows an example of a SBAR subsuming two PPs, one of which contains a named entity.

(16) The commercial ﬁshing restrictions in Washington will not be lifted [SBAR unless the salmon

population increases [PP to a sustainable number] [PP in the Columbia River]].

If the PP rule were not sensitive to named entities, the PP in the Columbia River would be the
ﬁrst prepositional phrase to be removed, because it is the lowest rightmost PP in the parse. However,
this PP provides an important piece of information: the location of the salmon population. The rule in
Step 9 will skip the last prepositional phrase and remove the penultimate PP to a sustainable number .
This concludes an overview of the Trimmer rules and our syntactic sentence compression algorithm.
Given a length limit, the system will produce a single compressed version of the target sentence.
Multiple compressions can be generated by setting the length limit to be very small and storing the state
of the sentence after each rule application as a compressed variant. In section 4, we will describe how
multiple compressed candidates generated by Trimmer are used as a component of a multi-document
summarization system.

1The reason for breaking PP removal into two stages is discussed in Section 3.1.2.

9


3.2 Topiary

We have used the Trimmer approach to compression in another variant of single-sentence summarization
called Topiary. This system combines Trimmer with a topic discovery approach (described next) to
produce a ﬂuent summary along with additional context.

The Trimmer algorithm is constrained to build a headline from a single sentence. However, it is
often the case that no single sentence contains all the important information in a story. Relevant
information can be spread over multiple sentences, linked by anaphora or ellipsis.
In addition, the
choice of lead sentence may not be ideal and our trimming rules are imperfect.

On the other hand, approaches that construct headlines from lists of topic terms (Lewis, 1999;
Schwartz et al., 1997) also have limitations. For example, Unsupervised Topic Discovery (UTD)—
described below—rarely generates any topic terms that are verbs. Thus, topic lists are good at indi-
cating the general subject but rarely give any direct indication of what events took place. Intuitively,
we need both ﬂuent text to tell what happened and topic terms to provide context.

3.2.1 Topic Term Generation: UTD and OnTopic

OnTopic (Schwartz et al., 1997) uses an HMM to assign topics to a document; topic models are derived
from an annotated corpus. However, it is often diﬃcult to acquire such data, especially for a new genre
or language. UTD (Sista et al., 2002) was developed to overcome this limitation: it takes as input a
large unannotated corpus and automatically creates a set of topic models with meaningful names.

The UTD algorithm has several stages. First, it analyzes the corpus to ﬁnd multi-word sequences
that can be treated as single tokens.
It does this using two methods. One method is a minimum
description length criterion that detects phrases that occur frequently relative to the individual words.
The second method uses BBN’s IdentiFinder to detect multi-word names. These names are added to
the text as additional tokens. They are also likely to be chosen as potential topic names. In the second
stage of UTD, we ﬁnd those terms (both single-word and multi-word) with high tf.idf. Only those
topic names that occur as high-content terms in at least four diﬀerent documents are kept. The third
stage trains topic models corresponding to these topic terms. The modiﬁed Expectation Maximization
procedure of BBN’s OnTopic system is used to determine which words in the documents often signify
these topic names. This produces topic models. Fourth, these topic models are used to ﬁnd the most
likely topics for each document, which is equivalent to assigning the name of the topic model to the
document as a topic term. This often assigns topics to documents where the topic name does not occur
in the document text.

We found, in various experiments (Sista et al., 2002), that the topic names derived by this procedure
were usually meaningful and that the topic assignment was about as good as when the topics were
derived from a corpus that was annotated by people. We have also used this procedure on diﬀerent
languages and shown the same behavior. Since UTD is unsupervised, it can run equally well on a new
language, as long as the documents can be divided into strings that approximate words.

The topic list in (17) was generated by UTD and OnTopic for a story about the FBI investigation

of the 1998 bombing of the U.S. embassy in Nairobi.

(17) BIN LADEN, EMBASSY, BOMBING, POLICE OFFICIALS, PRISON, HOUSE, FIRE, KA-

BILA

Topiary uses UTD to generate topic terms for the collection of documents to be summarized and
uses OnTopic to assign the topic terms to the documents. The next section will describe how topic
terms and sentence compressions are combined to form Topiary summaries.

10


3.2.2 Topiary Algorithm

As each Trimmer rule is applied to a sentence, the resulting state of the sentence is stored as a
compressed variant of the source sentence. Topiary selects from the variants the longest one such
that there is room to prepend the highest scoring non-redundant topic term. Suppose the highest
scoring topic term is “terrorism” and the length threshold is 75 characters. To make room for the topic
“terrorism”, the length threshold is lowered by 10 characters: 9 characters for the topic and 1 character
as a separator. Thus, Topiary chooses the longest trimmed variant under 65 characters that does not
contain the word “terrorism”, If there is no such candidate, i.e., all the trimmed variants contain the
word terrorism, Topiary would consider the second highest scoring topic word, “bomb”. Topiary would
select the longest trimmed variant under 70 characters that does not contain the word “bomb”. After
Topiary has selected a trimmed variant and prepended a topic to it, it checks to see how much unused
space remains under the threshold. Additional topic words are added between the ﬁrst topic word and
the compressed sentence until all space is exhausted.

This process results in a headline that contains one or more main topics about the story and a
short sentence that says what happened concerning them. The combination is often more concise than
a fully ﬂuent sentence and compensates for the fact that the information content from the topic and
the compressed sentence do not occur together in any single sentence from the source text.

As examples, sentences (18) and (19) are the outputs of Trimmer and Topiary, respectively, for the

same story in which UTD selected the topic terms in (17).

(18) FBI agents this week began questioning relatives of the victims

(19) BIN LADEN, EMBASSY, BOMBING: FBI agents this week began questioning relatives

By combining topics and parse-and-trim compression, Topiary achieved the highest score on the
single-document summarization task (i.e., headline generation task) in DUC-2004 (Zajic et al., 2004).

3.3 HMM Hedge

Our second approach to sentence compression, implemented in HMM Hedge, treats the observed data
(the story) as the result of unobserved data (headlines) that have been distorted by transmission
through a noisy channel. The eﬀect of the noisy channel is to add story words between the headline
words. The model is biased by parameters to make the resulting headlines more like Headlinese, the
observed language of newspaper headlines created by copy editors.

Formally, we consider a story S to be a sequence of N words. We want to ﬁnd a headline H, a

subsequence of words from S, that maximizes the likelihood that H generated the story S, or:

It is diﬃcult to directly estimate P (H|S), but this probability can be expressed in terms of other
probabilities that are easier to compute, using Bayes’ rule:

argmaxH P (H|S)

P (H|S) = P (S|H)P (H)

P (S)

Since the goal is to maximize this expression over H, and P (S) is constant with respect to H, the
denominator of the above expression can be omitted. Thus we wish to ﬁnd:

argmaxH P (S|H)P (H)

Let H be a headline consisting of words h1, h2, ..., hn. Let the special symbols start and end represent
the beginning and end of a headline, respectively. P (H) can be estimated using a bigram model of
Headlinese:

P (H) = P (h1|start)P (h2|h1)...P (end|hn)

11


The bigram probabilities of the words in the headline language were computed from a corpus of
English headlines taken from 242,918 AP newswire stories in the TIPSTER corpus. The headlines
contain 2,848,194 words from a vocabulary of 88,627 distinct words.

Given a story S and a headline H, the action of the noisy channel is to form S by adding non-headline
words to H. Let G be the non-headline words added by the channel to the headline: g1, g2, ..., gm. For
the moment, we assume that the headline words are transmitted through the channel with probability
1. We estimate P (S|H), the probability that the channel added non-headline words G to headline H
to form story S. This is accomplished using a unigram model of newspaper stories that we will refer
to as the general language, in contrast to the headline language. Let Pgl(g) be the probability of
non-headline word g in the general language and Pch(h) = 1 be the probability that headline word h
is transmitted through the channel as story word h.

P (S|H) = Pgl(g1)Pgl(g2)...Pgl(gm)Pch(h1)Pch(h2)...Pch(hn)

= Pgl(g1)Pgl(g2)...Pgl(gm)

The unigram probabilities of the words in the general language were computed from 242,918 English
AP news stories in the TIPSTER corpus. The stories contain 135,150,288 words from a vocabulary of
428,633 distinct words.

The process by which the noisy channel generates a story from a headline can be represented by a
Hidden Markov Model (HMM) (Baum, 1972). An HMM is a weighted ﬁnite-state automaton in which
each state probabilistically emits a string. The simplest HMM for generating headlines consists of two
states: an H state that emits words that occur in the headline and a G state that emits all the other
words in the story.

Since we use a bigram model of headlines, each state that emits headline words must “remember”
the previously emitted headline word. If we did not constrain headline words to actually occur in the
story, we would need an H state for each word in the headline vocabulary. However, because headline
words are chosen from the story words, it is suﬃcient to have an H state for each story word. For any
story, the HMM consists of a start state S, an end state E, an H state for each word in the story, a
corresponding G state for each H state, and a state Gstart that emits words that occur before the ﬁrst
headline word in the story. An H state can emit only the word it represents. The corresponding G
state remembers which word was emitted by its H state and can emit any word in the story language.
A headline corresponds to a path through the HMM from S to E that emits all the words in the story
in the correct order. In practice the HMM is constructed with states for only the ﬁrst N words of the
story, where N is a constant (60), or N is the number of words in the ﬁrst sentence.2

In example (1i), given earlier, the H states will emit the words in bold (pilots, not, allowed, to,
have, guns, in, cockpits) and the G states will emit all the other words. The HMM will transition
between the H and G states as needed to generate the words of the story. In the current example,
the model will have states Start, Gstart, End, and 28 H states with 28 corresponding G states.3 The
headline given in example (1ii) corresponds to the following sequence of states: Start, Gstart 17 times,
Hpilots, Gpilots, Hnot, Gnot, Hallowed, Hto, Hhave, Hguns, Hin, Gin, Hcockpits, End. This path is not the
only one that could generate the story in (1i). Other possibilities are:

(20) (i) Transportation Department decided airline pilots not to have guns.

(ii) Months of the terrorist has to have cockpits.

2Limiting consideration of headline words to the early part of the story is justiﬁed in Dorr et al. (2003a) where it
was shown that more than half of the headline words are chosen from the ﬁrst sentence of the story. Other methods for
selecting the window of story words are possible and will be explored in future research.

3The subscript of a G state indicates the H state it is associated with, not the story word it emits. In the example,

Gpilots emits story word will , Gnot emits story word be, and Gin emits story word the.

12


Although (20i) and (20ii) are possible headlines for (1i), the conditional probability of (20ii) given (1i)
will be lower than the conditional probability of (20i) given (1i).

The Viterbi algorithm (Viterbi, 1967) is used to select the most likely headline for a given story.
We use length constraints to ﬁnd the most likely headlines consisting of W words, where W ranges
from 5 to 15. Multiple backpointers are used so that we can ﬁnd the n most likely headlines at each
length.

HMM Hedge is enhanced by three additional decoding parameters to help the system choose outputs
that best mimic actual headlines: a position bias, a clump bias, and a gap bias. The incorporation
of these biases changes the score produced by the decoder from a probability to a relative desirability
score. The three parameters were motivated by analysis of system output and their values were set by
trial and error. A logical extension to this work would be to learn the best setting of these biases, e.g.,
through Expectation Maximization.

The position bias favors headlines that include words near the front of the story. This reﬂects our
observations of human-constructed headlines, in which headline words tend to appear near the front
of the story. The initial position bias p is a positive number less than one. The story word in the
nth position is assigned a position bias of log(pn). When an H state emits a story word, the position
bias is added to the desirability score. Thus, words near the front of the story carry less of a position
bias than words farther along. Note that this generalization often does not hold in the case of human
interest and sports stories, which may start with a hook to get the reader’s attention, rather than a
topic sentence.

We also observed that human-constructed headlines tend to contain contiguous blocks of story
words. Example (1ii), given earlier, illustrates this with the string “allowed to have guns”. The
string bias is used to favor “clumpiness”. i.e., the tendency to generate headlines composed of clumps
of contiguous story words. The log of the clump bias is added to the desirability score with each
transition from an H state to its associated G state. With high clump biases, the system will favor
headlines consisting of fewer but larger clumps of contiguous story words.

The gap bias is used to disfavor headline “gappiness”, i.e., large gaps of non-headline words in the
story between clumps of headline words. Although humans are capable of constructing ﬂuent headlines
by selecting widely spaced words, we observed that HMM Hedge was more likely to combine unrelated
material by doing this. At each transition from a G state to an H state, corresponding to the end
of a sequence of non-headline words in the story, a gap bias is applied that increases with the size of
the gap between the current headline and the last headline word to be emitted. This can also be seen
as a penalty for spending too much time in one G state. With high gap biases, the system will favor
headlines with few large gaps.

One characteristic diﬀerence between newspaper headline text and newspaper story text is that
headlines tend to be in present tense while story sentences tend to be in the past tense. Past tense
verbs occur more rarely in the headline language than in the general language. HMM Hedge mimics this
aspect of Headlinese by allowing morphological variation between headline verbs and the corresponding
story verbs. Morphological variation for verbs is achieved by creating an H state for each available
variant of a story verb. These H states still emit the story verb but they are labeled with the variant.
HMM Hedge can generate a headline in which proposes is the unobserved headline word that emits the
observed story word proposed , even though proposes does not occur in the story.

(21) (i) A group has proposed awarding $1 million in every general election to one randomly chosen

voter.

(ii) Group proposes awarding $1 million to randomly chosen voter.

Finally, we constrain each headline to contain at least one verb. That is to say, we ignore headlines

that do not contain at least one verb, no matter how desirable the decoding is.

13


Although we have described an application of HMM Hedge to blocks of story words without reference
to sentence boundaries, it is also possible to use HMM as a single sentence compressor by limiting the
block to the words in a single sentence.

Also, as we will see shortly, multiple alternative compressions of a sentence may be generated with
HMM Hedge. The Viterbi algorithm is capable of discovering n-best compressions of a window of
story words and can be constrained to consider only paths that include a speciﬁc number of H states,
corresponding to compressions that contain a speciﬁc number of words. We use HMM Hedge to generate
55 compressions for each sentence by computing the ﬁve best headlines at each length, from 5 to 15
words. In Section 4 we will describe how HMM Hedge is used as a component of a multi-document
summarization system.

4 Multi-Document Summarization

The sentence compression tools we developed for single-document summarization have been incor-
porated into our Multi-Candidate Reduction framework for multi-document summarization. MCR
produces a textual summary from a collection of relevant documents in three steps. First, sentences
are selected from the source documents for compression. The most important information occurs near
the front of the stories, so we select the ﬁrst ﬁve sentences of each document for compression. Second,
multiple compressed versions of each sentence are produced using Trimmer or HMM Hedge to create a
pool of candidates for inclusion in the summary. Finally, a sentence selector constructs the summary
by iteratively choosing from the pool of candidates based on a linear combination of features until the
summary reaches a desired length.

At present, weights for the features are determined by manually optimizing on a set of training
data to maximize the Rouge-2 recall score (Lin and Hovy, 2003), using Rouge version 1.5.5. A
typical summarization task might call for the system to generate a 250-word summary from a couple
of dozen news stories. These summaries may be query-focused, in the sense that the summaries should
be responsive to a particular information need, or they may be generic, in that a broad overview of the
documents is desired.

Our sentence selector adopts certain aspects of Maximal Marginal Relevance (MMR) (Carbonell
and Goldstein, 1998), an approach that attempts to balance relevance and anti-redundancy. In MCR’s
selection module, the highest scoring sentence from the pool of eligible candidates is chosen for inclusion
in the summary. Features that contribute to a candidate’s score can be divided into two types: dynamic
and static. When a candidate is chosen, all other compressed variants of that sentence are eliminated.
After a candidate is added to the summary, the dynamic features are re-computed, and the candidates
are re-ranked. Candidates are added to the summary until the desired length is achieved. The ordering
of candidates in the summary is the same as the order in which they were selected for inclusion. The
ﬁnal sentence of the summary is truncated if it causes the summary to exceed the length limit.

4.1 Static Features

Static features are calculated before sentence selection begins and do not change during the process of
summary construction:

• Position. The zero-based position of the sentence in the document.

• Sentence Relevance. The relevance score of the sentence to the query.

• Document Relevance. The relevance score of the document to the query.

• Sentence Centrality. The centrality score of the sentence to the topic cluster.

14


• Document Centrality. The centrality score of the document to the topic cluster.

• Scores from the Compression Modules:

– Trim rule application counts. For Trimmer-based MCR, the number of Trimmer rule in-

stances applied to produce the candidate.

– Negative Log Desirability. For HMM-based MCR, the relative desirability score of the

candidate.

We use the Uniform Retrieval Architecture (URA), University of Maryland’s software infrastructure
for information retrieval tasks, to compute relevance and centrality scores for each compressed candi-
date. There are four such scores: the relevance score between a compressed sentence and the query,
the relevance score between the document containing the compressed sentence and the query, the cen-
trality score between a compressed sentence and the topic cluster, and the centrality score between
the document containing the compressed sentence and the topic cluster. We deﬁne the topic cluster
to be the entire collection of documents relevant to this particular summarization task. Centrality is
a concept that quantiﬁes how similar a piece of text is to all other texts that discuss the same general
topic. We assume that sentences having higher term overlap with the query and sources more “central”
to the topic cluster are preferred for inclusion in the ﬁnal summary.

The relevance score between a compressed sentence and the query is an idf-weighted count of
overlapping terms (number of terms shared by the two text segments). Inverse document frequency
(idf), a commonly-used measure in the information retrieval literature, roughly captures term salience.
The idf of a term t is deﬁned by log(N/ct), where N is the total number of documents in a particular
corpus and ct is the number of documents containing term t; these statistics were calculated from one
year’s worth of LA Times articles. Weighting term overlap by inverse document frequency captures
the intuition that matching certain terms is more important than matching others.

Lucene, a freely-available oﬀ-the-shelf information retrieval system, is used to compute the three
other scores. The relevance score between the document containing the compressed sentence and
the query is computed using Lucene’s built-in similarity function. The centrality score between the
compressed sentence and the topic cluster is the mean of the similarity between the sentence and each
document comprising the cluster (once again, as computed by Lucene’s built-in similarity function).
The document-cluster centrality score is also computed in much the same way, by taking the mean
In order to
of the similarity of the particular document with every other document in the cluster.
obtain an accurate distribution of term frequencies to facilitate the similarity calculation, we indexed
all relevant documents (i.e., the topic cluster) along with a comparable corpus (one year of the LA
Times)—this additional text essentially serves as a background model for non-relevant documents.

Some features are derived from the sentence compression modules used to generate candidates. For
Trimmer, the rule application count feature of a candidate is the number of rules that were applied to
a source sentence to produce the candidate. The rules are not presumed to be equally eﬀective, so the
rule application counts are broken down by rule type. For HMM Hedge, we use the relative desirability
score calculated by the decoder, expressed as a negative log.

The features discussed in this section are assigned to the candidates before summary generation
begins and remain ﬁxed throughout the process of summary sentence selection. The next section
discusses how candidate features are assigned new values as summary geneneration proceeds.

4.2 Dynamic Features

Dynamic features change during the process of sentence selection to reﬂect changes in the state of the
summary as sentences are added.4 The dynamic features are:

4At present the dynamic features are properties of the candidates, calculated with respect to the current summary
state. There are no features directly relating to the amount of space left in the summary, so there is no mechanism that

15


• Redundancy. A measure of how similar the sentence is to the current summary.

• Sentence-from-doc. The number of sentences already selected from the sentence’s document.

The intuition behind our redundancy measure is that candidates containing words that occur much
more frequently in the current state of the summary than they do in general English are redundant
to the summary. We imagine that sentences in the summary are generated by the underlying word
distribution of the summary rather than the distribution of words in the general language. If a sentence
appears to have been generated by the summary rather than by the general language, we take it to
be redundant to the summary. Suppose we have a summary about earthquakes. The presence in a
candidate of words like earthquake, seismic, and Richter Scale, which have a high likelihood in the
summary, will make us think that the candidate is redundant to the summary.

To estimate the extent to which a candidate is more likely to have been generated by a summary
than by the general language, we consider the probabilities of the words in the candidate. We estimate
that the probability that a word w occurs in a candidate generated by the summary is

P (w) = λP (w|D) + (1 − λ)P (w|C)

where D is the summary, C is the general language corpus5, λ is a parameter estimating the probability
that the word was generated by the summary and (1−λ) is the probability that the word was generated
by the general language. We have set λ = 0.3, as a general estimate of the portion of words in a text
that are speciﬁc to the text’s topic. We estimate the probabilities by counting the words6 in the current
summary and the general language corpus:

P (w|D) =

count of w in D
size of D

P (w|C) =

count of w in C
size of C

We take the probability of a sentence to be the product of the probabilities of its words, so we calculate
the probability that a sentence was generated by the summary, i.e. our redundancy metric, as:

Redundancy(S) =

Y

s∈S

λP (s|D) + (1 − λ)P (s|C)

For ease of computation, we actually use log probabilities:

log(λP (s|D) + (1 − λ)P (s|C))

X

s∈S

Redundancy is a dynamic feature because the word distribution of the current summary changes with
every iteration of the sentence selector.

4.3 Examples of System Output

We applied our MCR framework to test data from the DUC-2006 evaluation (Dang and Harman, 2006).
Given a topic description and a set of 25 documents related to the topic (drawn from AP newswire,
the New York Times, and the Xinhua News Agency English Service), the system’s task was to create

would aﬀect the distribution of compressed candidates over the iterations of the sentence selector. This issue will be
addressed as future work in Section 7.

5The documents in the set being summarized are used to estimate the general language model.
6Actually, preprocessing for redundancy includes stopword removal and applying the Porter Stemmer (Porter, 1980).

16


Title: Native American Reservation System—pros and cons
Narrative Description: Discuss conditions on American Indian reservations or among Native
American communities. Include the beneﬁts and drawbacks of the reservation system. Include
legal privileges and problems.

Figure 2: Topic D0601A from the DUC-2006 multi-document summarization task.

a 250-word summary that addressed the information need expressed in the topic. One of the topic
descriptions is shown in Figure 2. The 25 documents in the document set have an average size of 1170
words, so a 250-word summary represents a compression ratio of 0.86%.

Figures 3, 4 and 5 show examples of MCR output using Trimmer compression, HMM Hedge com-
pression, or no compression. For readability, we use ◦ as a sentence delimiter; this is not part of the
actual system output. The sentences compressed by Trimmer mimic Headlinese by omitting determin-
ers and auxiliary verbs. For example, the ﬁrst sentence in Figure 3 is a compression of the following
source sentence:

Seeking to get a more accurate count of the country’s American Indian population, the
Census Bureau is turning to tribal leaders and residents on reservations to help overcome
long-standing feelings of wariness or anger toward the federal government.

Three determiners and a form of be have been removed from the source sentence in the compression
that appears in the summary. The removal of this material makes the sentence appear more like a
headline.

In comparison with Trimmer compressions, HMM compressions are generally less readable and

more likely to be misleading. Consider the ﬁnal sentence in Figure 4.

(22) main purpose of reservation to pay American Indians by poverty proposals

This is a compression of the following source sentence:

(23) But the main purpose of the visit—the ﬁrst to a reservation by a president since Franklin
Roosevelt—was simply to pay attention to American Indians, who are so raked by grinding
poverty that Clinton’s own advisers suggested he come up with special proposals geared speciﬁ-
cally to the Indians’ plight.

Because HMM Hedge uses a bigram model of Headlines, it is unable to capture sentence-level
grammaticality. The same limitation makes it diﬃcult to prevent misleading or incorrect compressions.
For example, the third sentence from the end of Figure 4 seems to say that a court legalized gambling

on Indian reservations:

(24) Supreme Court allows legalized gambling Indian reservations

However, it is a compression of the following source sentence:

(25) Only Monday, the California Supreme Court overturned a ballot measure that would have allowed

expansion of legalized gambling on Indian reservations.

Nevertheless, we can see from the examples that sentence compression allows a summary to include

more material from other sources. This increases the topic coverage of system output.

17


Seeking to get more accurate count of country’s American Indian population, Census Bureau turning to
tribal leaders and residents on reservations to help overcome long-standing feelings. ◦ American Indian
reservations would get infusion. ◦ Smith and thousands seeking help for substance abuse at American
Indian Community House, largest of handful of Native American cultural institutions. ◦ Clinton going
to Pine Ridge Reservation for visit with Oglala Sioux nation and to participate in conference on Native
American homeownership and economic development. ◦ Said Glen Revere, nutritionist with Indian Health
Services on 2.8 million-acre Wind River Reservation, about 100 miles east of Jackson, Wyo. “Then we
came up with idea for this community garden, and it been bigger than we ever expected.” ◦ Road leading
into Shinnecock Indian reservation is not welcoming one But main purpose of visit – ﬁrst to reservation
by president since Franklin Roosevelt – was simply to pay attention to American Indians, who raked by
grinding poverty Clinton’s own advisers suggested he come up with special proposals geared speciﬁcally to
Indians’ plight. ◦ “This highlights what going on out there, since beginning of reservation system,” said
Sidney Harring, professor at City University of New York School of Law and expert on Indian crime and
criminal law. ◦ American Indians are victims. ◦ President Clinton turned attention to arguably poorest,
most forgotten ◦ U.S. citizens: American Indians. ◦ When American Indians began embracing gambling,
Hualapai tribe moved quickly to open casino. ◦ members of Northern Arapaho Tribe on Wind River
Reservation started seven-acre community garden with donated land, seeds and

Figure 3: MCR Summary for DUC-2006 Topic D0601A, using Trimmer for sentence compression.

David Rocchio deputy legal counsel to Vermont Gov. Howard Dean who has been involved in discussions
on Indian gambling through the National Governors Association said that the concern that governors have
is not with the beneﬁt casinos bring to tribes ◦ Native Americans living on reservations that maintain 50
percent or more unemployment are exempt from the national ﬁve year family limit on welfare beneﬁts ◦
Smith and thousands like her are seeking help for their substance abuse at the American Indian Community
House the largest of a handful of Native American cultural institutions in the New York area ◦ Juvenile
crime is one strand in the web of social problems facing urban and reservation Indian communities the
report said ◦ Soldierwolf’s family represents the problems that plague many of the 1.3 million American
Indians who live on reservations of whom 49 percent are unemployed ◦ Powless said the Onondaga people
want to work with the community outside the reservation to improve the economy of the region perhaps
creating tourism destinations that might include Indian culture or setting up a free trade zone at unused
manufacturing sites ◦ As Indian communities across the nation struggle with short funds and a long list
of problems they are watching the Navajo Nation’s legal battle with the federal government ◦ recognize
Indians not only Native Americans as Americans ◦ go on reservation system Harring Indian ◦ Supreme
Court allows legalized gambling Indian reservations ◦ American Indian reservations tribal colleges rise faster
than ◦ main purpose of reservation to pay American Indians by poverty proposals

Figure 4: MCR Summary for DUC-2006 Topic D0601A, using HMM Hedge for sentence compression

18


Seeking to get a more accurate count of the country’s American Indian population, the Census Bureau is
turning to tribal leaders and residents on reservations to help overcome long-standing feelings of wariness
or anger toward the federal government. ◦ American Indian reservations would get an infusion of $1.2
billion in federal money for education, health care and law enforcement under President Clinton’s proposed
2001 budget ◦ Smith and thousands like her are seeking help for their substance abuse at the American
Indian Community House, the largest of a handful of Native American cultural institutions in the New
York area. ◦ Clinton was going to the Pine Ridge Reservation for a visit with the Oglala Sioux nation
and to participate in a conference on Native American homeownership and economic development. ◦ said
Glen Revere, a nutritionist with the Indian Health Services on the 2.8 million-acre Wind River Reservation,
about 100 miles east of Jackson, Wyo. “Then we came up with the idea for this community garden, and
it’s been bigger than we ever expected in so many ways.” ◦ The road leading into the Shinnecock Indian
reservation is not a welcoming one ◦ But the main purpose of the visit – the ﬁrst to a reservation by a
president since Franklin Roosevelt – was simply to pay attention to American Indians, who are so raked by
grinding poverty that Clinton’s own advisers suggested he come up with special proposals geared speciﬁcally
to the Indians’ plight. ◦ “This highlights what has been going on out there for 130 years,

Figure 5: MCR Summary for DUC-2006 Topic D0601A, with no sentence compression

19


R1 Recall

R1 Precision

R1 F

R2 Recall

R2 Precision

R2 F

HMM
Sentence
0.23552
(0.23014-
0.24082)
0.21896
(0.21386-
0.22384)
0.22496
(0.21983-
0.22978)
0.06838
(0.06546-
0.07155)
0.06287
(0.06017-
0.06576)
0.06488
(0.06209-
0.06785)

HMM
60 Block
0.21381
(0.20912-
0.21827)
0.18882
(0.18444-
0.19301)
0.19966
(0.19505-
0.20391)
0.06133
(0.05848-
0.06414)
0.05351
(0.05097-
0.05588)
0.05686
(0.05420-
0.05942)

Trimmer

Topiary

0.21014
(0.20436-
0.21594)
0.20183
(0.19627-
0.20722)
0.20179
(0.19612-
0.20718)
0.06337
(0.06030-
0.06677)
0.06230
(0.05887-
0.06617)
0.06079
(0.05788-
0.06401)

0.25143
(0.24632-
0.25663)
0.23038
(0.22567-
0.23522)
0.23848
(0.23373-
0.24328)
0.06637
(0.06345-
0.06958)
0.06024
(0.05747-
0.06326)
0.06252
(0.05976-
0.06561)

Table 2: Rouge scores and 95% conﬁdence intervals for 624 documents from DUC-2003 test set.

5 System Evaluations

We tested four single-document summarization systems on the DUC-2003 Task 1 test set:

• HMM Hedge using the ﬁrst sentence of each document (HMM Sentence)

• HMM Hedge using the ﬁrst 60 words of each document (HMM 60 block)

• Trimmer

• Topiary

Task 1 from DUC-2003 was to construct generic 75-byte summaries for 624 documents drawn from AP
Newswire and the New York Times. The average size of the documents was 3,997 bytes, so a 75-byte
summary represents a compression ratio of 1.9%.

An automatic summarization evaluation tool, Rouge (Lin and Hovy, 2003), was used to evaluate
the results. The system parameters were optimized by hand to maximize the Rouge-1 recall on
a comparable training corpus, 500 AP Newswire and New York Times articles from the DUC-2004
single-document short summary test data.

The Rouge results are shown in Table 2. Results show that HMM Hedge 60 scored signiﬁcantly
lower than most other systems and that Topiary scored higher than all other systems for all R1
In addition, HMM Hedge Sentence scored signiﬁcantly higher than Trimmer for the R1
measures.
measures.

We also evaluated Trimmer and HMM Hedge as components in our Multi-Candidate Reduction
framework, along with a baseline that uses the same sentence selector but does not use sentence
compression. All three systems considered the ﬁrst ﬁve sentences of each document and used the
sentence selection algorithm presented in Section 4. The feature weights were manually optimized to
maximize Rouge-2 recall on a comparable training corpus, 1,593 Financial Times and Los Angeles
Times articles grouped into 50 topics from the DUC-2005 query-focused multi-document summarization

20


Trimmer

HMM Hedge No

R1 Recall

R2 Recall

0.29391
(0.28560-
0.30247)
0.06718
(0.06332-
0.07111)

0.27311
(0.26554-
0.28008)
0.06251
(0.05873-
0.06620)

Compression
0.27576
(0.26772-
0.28430)
0.06126
(0.05767-
0.06519)

Table 3: Rouge scores and 95% conﬁdence intervals for 50 DUC-2006 test topics, comparing three
MCR variants.

MCR Score
Higher
Not Diﬀerent
Range
Lower

Rouge-2
0.0805
1
23
0.0678-0.0899
11

Rouge-SU4
0.1360
1
24
0.1238-0.1475
10

BE-HM
0.0413
0
27
0.0318-0.0508
8

Table 4: Oﬃcial DUC-2006 Automatic Metrics for our MCR submission (System 32).

test data. The systems were used to generate query-focused, 250-word summaries using the DUC-2006
test data, described in Section 4.3.

The systems were evaluated using Rouge, conﬁgured to omit stopwords from the calculation.7
Results are shown in Table 3. MCR using Trimmer compressions scored signiﬁcantly higher than
MCR using HMM Hedge compressions and the baseline for Rouge-1, but there was not a signiﬁcant
diﬀerence among the three systems for Rouge-2.

Finally, the University of Maryland and BBN submitted a version of MCR to the oﬃcial DUC-
2006 evaluation. This version used Trimmer as the source of sentence compressions. Results show
that use of sentence compression hurt the system on human evaluation of grammaticality. This is not
surprising, since Trimmer aims to produce compressions that are grammatical in Headlinese, rather
than standard English. Our MCR run scored signiﬁcantly lower than 23 systems on NIST’s human
evaluation of grammaticality. However, the system did not score signiﬁcantly lower than any other
system on NIST’s human evaluation of content responsiveness. A second NIST evaluation of content
responsiveness asked evaluators to take readability into consideration. In this evaluation, MCR scored
signiﬁcantly lower than only two systems. The evaluators recognized that Trimmer compressions are
not grammatical in standard English; yet, the content coverage was not signiﬁcantly diﬀerent from the
best automatic systems and only two systems were found to be signiﬁcantly more readable.

NIST computed three “oﬃcial” automatic evaluation metrics for DUC-2006: Rouge-2, Rouge-
SU4 and BE-HM. Table 4 shows the oﬃcial scores of the submitted MCR system for these three
metrics, along with numbers of systems that scored signiﬁcantly higher, signiﬁcantly lower, or were not
signiﬁcantly diﬀerent from our MCR run. Also shown is the range of scores for the systems that were
not signiﬁcantly diﬀerent from MCR. These results show that the performance of our MCR run was
comparable to most other systems submitted to DUC-2006.

7This is a change in the Rouge conﬁguration from the oﬃcial DUC-2006 evaluation. We note that the removal of
non-essential stopwords (typical of Headlinese) is an important component of Trimmer-based sentence compression. For
internal system comparisons, we conﬁgure Rouge in a way that will allow us to detect system diﬀerences relevant to our
research focus. For reporting of oﬃcial Rouge results on submitted systems we use the community’s accepted Rouge
conﬁgurations.

21


The evaluation in Table 3 suggests that Trimmer sentence compression is preferable to HMM
Hedge sentence compression for generation of English summaries of collections of document in English.
However, HMM Hedge may prove to have value with noisier data, as we discuss in the next section.
Nevertheless, sentence compression appears to be a valuable component of our framework for multi-
document summarization, thus validating the ideas behind Multi-Candidate Reduction.

6 Applications to Diﬀerent Types of Texts

We have applied the MCR framework to summarizing diﬀerent types of texts. In this section we brieﬂy
touch on genre-speciﬁc issues that are the subject of ongoing work. Trimmer, Topiary, and HMM Hedge
were designed for summarization of written news. In this genre, the lead sentence is almost always
the ﬁrst non-trivial sentence of the document. More sophisticated methods for ﬁnding lead sentences
did not outperform the baseline of simply selecting the ﬁrst sentence for AP wire “hard” news stories.
However, some types of articles, such as sports stories, opinion pieces, and movie reviews often do
not have informative lead sentences and will require additional work in ﬁnding the best sentence for
compression.

MCR has also been applied to summarizing transcripts of broadcast news—another input form
where lead sentences are often not informative. The conventions of broadcast news introduce categories
of story-initial light content sentences, such as “I’m Dan Rather” or “We have an update on the story
we’ve been following”. These present challenges for the ﬁltering stage of our MCR framework.

Such texts are additionally complicated by a range of problems not encountered in written news:
noise introduced by automatic speech recognizers or other faulty transcription, issues associated with
sentence boundary detection and story boundary detection. If word error rate is high, parser failures
In this context, HMM Hedge becomes more
can prevent Trimmer from producing useful output.
attractive, since our language models are more resilient to noisy input.

We have performed an initial evaluation of Trimmer, Topiary, and a baseline consisting of the
ﬁrst 75 characters of a document, on the task of creating 75-character headlines for broadcast news
transcriptions (Zajic, 2007). The corpus for this task consisted of 560 broadcast news stories from
ABC, CNN, NBC, Public Radio International, and Voice of America. We used Rouge-1 recall to
evaluate the summaries and found that both systems scored higher than the baseline and that Topiary
scored higher than Trimmer. However there were no signiﬁcant diﬀerences among the systems.

Another application of our framework is the summarization of email threads—collections of emails
that share a common topic or were written as responses to each other. This task can essentially be
treated as a multi-document summarization problem, albeit email thread structure introduces some
constraints with respect to the ordering of summary sentences. Noisy data is inherent in this problem
and pre-processing to remove quoted text, attachments, and headers is crucial. We have found that
metadata, such as the name of the sender of each included extract help make email summaries easier
to read.

We performed an initial evaluation of HMM Hedge and Trimmer as the source of sentence com-
pressions for an email thread summarization system based on the MCR framework (Zajic, 2007). The
corpus for this task consisted of 10 manually constructed email threads from the Enron Corpus (Klimt
and Yang, 2004). We used Rouge-1 and Rouge-2 recall with jackkniﬁng to compare the automatic
systems and the human summarizers. We did not observe a signiﬁcant diﬀerence between the two sys-
tems, but we found that the task of summarizing email threads was extremely diﬃcult for the humans
(one summarizer scored signiﬁcantly worse than the automatic systems). This application of MCR to
email thread summarization is an initial eﬀort. The diﬃculty of the task for the humans suggests that
the community needs to develop a clearer understanding of what makes a good email thread summary
and to explore practical uses for them.

22


Finally, Trimmer and HMM Hedge have been applied to Hindi-English cross-language summariza-
In this case, Trimmer was applied to the output of machine translation. We adapted HMM
tion.
Hedge to cross-lingual summarization by using the mechanism developed for morphological variation
to represent translation probabilities from Hindi story words to English headline words. For more
details, see Dorr et al. (2003a).</corps>
		<conclusion>8 Conclusion

This work presents Multi-Candidate Reduction, a general architecture for multi-document summa-
rization. The framework integrates successful single-document compression techniques that we have
previously developed. MCR is motivated by the insight that multiple candidate compressions of source
sentences should be made available to subsequent processing modules, which may have access to more
information for summary construction. This is implemented in a dynamic feature-based sentence se-
lector that iteratively builds a summary from compressed variants. Evaluations show that sentence
compression plays an important role in multi-document summarization and that our MCR framework
is both ﬂexible and extensible.

Acknowledgments

This work has been supported, in part, under the GALE program of the Defense Advanced Research
Projects Agency, Contract No. HR0011-06-2-0001, the TIDES program of the Defense Advanced
Research Projects Agency, BBNT Contract No. 9500006806, and the University of Maryland Joint
Institute for Knowledge Discovery. Any opinions, ﬁndings, conclusions or recommendations expressed
in this paper are those of the authors and do not necessarily reﬂect the views of DARPA. The ﬁrst
author would like to thank Naomi for proofreading, support, and encouragement. The second author
would like to thank Steve, Carissa, and Ryan for their energy enablement. The third author would like
to thank Esther and Kiri for their kind support.</conclusion>
		<discussion>Aucune discussion trouvée.</discussion>
		<biblio>L. Bahl, F. Jelinek, and R. Mercer. 1983. A maximum likelihood approach to continuous speech
recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 5(2):179–190.

M. Banko, V. Mittal, and M. Witbrock. 2000. Headline generation based on statistical translation.
In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL
2000), pages 318–325, Hong Kong.

R. Barzilay, N. Elhadad, and K. McKeown. 2002. Inferring strategies for sentence ordering in multi-

document news summarization. Journal of Artiﬁcial Intelligence Research, 17:35–55.

L. Baum. 1972. An inequality and associated maximization technique in statistical estimation of

probabilistic functions of a Markov process. Inequalities, 3:1–8.

S. Bergler, R. Witte, M. Khalife, Z. Li, and F. Rudzicz. 2003. Using knowledge-poor coreference
resolution for text summarization. In Proceedings of the HLT-NAACL 2003 Text Summarization
Workshop and Document Understanding Conference (DUC 2003), pages 85–92, Edmonton, Alberta.

D. Bikel, R. Schwartz, and R. Weischedel. 1999. An algorithm that learns what’s in a name. Machine

Learning, 34(1/3):211–231.

S. Blair-Goldensohn, D. Evans, V. Hatzivassiloglou, K. McKeown, A. Nenkova, R. Passonneau,
2004. Columbia University
In Proceedings of the 2004 Document Understanding Conference (DUC 2004) at

B. Schiﬀman, A. Schlaikjer, A. Siddharthan, and S. Siegelman.
at DUC 2004.
HLT/NAACL 2004, pages 23–30, Boston, Massachusetts.

24


P. Brown, J. Cocke, S. Pietra, V. Pietra, F. Jelinek, J. Laﬀerty, R. Mercer, and P. Roossin. 1990. A

statistical approach to machine translation. Computational Linguistics, 16(2):79–85.

J. Carbonell and J. Goldstein. 1998. The use of MMR, diversity-based reranking for reordering
documents and producing summaries. In Proceedings of the 21st Annual International ACM SIGIR
Conference on Research and Development in Information Retrieval (SIGIR 1998), pages 335–336,
Melbourne, Australia.

Eugene Charniak. 2000. A Maximum-Entropy-Inspired Parser. In Proceedings of the First Meeting
of the North American Chapter of the Association for Computational Linguistics (NAACL 2000),
pages 132–139, Seattle, Washington.

J. Clarke and M. Lapata. 2006. Models for sentence compression: A comparison across domains,
training requirements and evaluation measures. In Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th Annual Meeting of the Association for Computational
Linguistics (COLING/ACL 2006), pages 377–384, Sydney, Australia.

J. Conroy, J. Schlesinger, and J. Goldstein. 2005. CLASSY query-based multi-document summariza-
tion. In Proceedings of the 2005 Document Understanding Conference (DUC-2005) at NLT/EMNLP
2005, Vancouver, Canada.

J. Conroy, J. Schlesinger, D. O’Leary, and J. Goldstein. 2006. Back to basics: CLASSY 2006. In
Proceedings of the 2006 Document Understanding Conference (DUC 2006) at HLT/NAACL 2006,
New York, New York.

D. Cutting, J. Pedersen, and P. Sibun. 1992. A practical part-of-speech tagger. In Proceedings of the

Third Conference on Applied Natural Language Processing, Trento, Italy.

Hoa Dang and Donna Harman. 2006. Proceedings of the 2006 Document Understanding Conference

(DUC 2006) at HLT/NAACL 2006.

B. Dorr and T. Gaasterland. this special issue, 2007. Exploiting aspectual features and connecting
words for summarization-inspired temporal-relation extraction. Information Processing and Man-
agement.

B. Dorr, D. Zajic, and R. Schwartz. 2003a. Cross-language headline generation for Hindi. ACM

Transactions on Asian Language Information Processing (TALIP), 2(3):270–289.

B. Dorr, D. Zajic, and R. Schwartz. 2003b. Hedge Trimmer: A parse-and-trim approach to headline
generation. In Proceedings of the HLT-NAACL 2003 Text Summarization Workshop and Document
Understanding Conference (DUC 2003), pages 1–8, Edmonton, Alberta.

T. Dunning. 1994. Statistical identiﬁcation of language. Technical Report MCCS 94-273, New Mexico

State University.

T. Euler. 2002. Tailoring text using topic words: Selection and compression. In Proceedings of 13th
International Workshop on Database and Expert Systems Applications (DEXA 2002), pages 215–
222, Aix-en-Provence, France.

J. Goldstein, V. Mittal, J. Carbonell, and M. Kantrowitz. 2000. Multi-document summarization by
sentence extraction. In Proceedings of ANLP/NAACL 2000 Workshop on Automatic Summariza-
tion, pages 40–48.

D. Harman and M. Liberman. 1993. TIPSTER Complete. Linguistic Data Consortium (LDC),

Philadelphia.

25


H. Jing and K. McKeown. 2000. Cut and paste based text summarization. In Proceedings of the First
Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL
2000), pages 178–185, Seattle, Washington.

B. Klimt and Y. Yang. 2004. Introducing the Enron Corpus. In Proceedings of the First Conference

on Email and Anti-Spam (CEAS), Mountain View, California.

K. Knight and D. Marcu. 2000. Statistics-based summarization—step one: Sentence compression. In
Proceedings of the Seventeenth National Conference on Artiﬁcial Intelligence (AAAI-2000), Austin,
Texas.

K. Knight and D. Marcu. 2002. Summarization beyond sentence extraction: A probabilistic approach

to sentence compression. Artiﬁcial Intelligence, 139(1):91–107.

M. Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proceedings
of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL 2004), pages
545–552, Barcelona, Spain.

David Dolan Lewis. 1999. An evaluation of phrasal and clustered representations on a text categoriza-
tion task. In Proceedings of the 15th Annual International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR 1992), pages 37–50, Copenhagen, Denmark.

C.-Y. Lin and E. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the 2003 Human Language Technology Conference and the North American
Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2003),
pages 71–78, Edmonton, Alberta.

I. M˚ardh. 1980. Headlinese: On the Grammar of English Front Page Headlines. Malmo.

E. Mays, F. Damerau, and R. Mercer. 1990. Context-based spelling correction. In Proceedings of IBM

Natural Language ITL, pages 517–522, Paris, France.

S. Miller, L. Ramshaw, H. Fox, and R. Weischedel. 2000. A novel use of statistical parsing to extract
information from text. In Proceedings of the First Meeting of the North American Chapter of the
Association for Computational Linguistics (NAACL 2000), pages 226–233, Seattle, Washington.

S. Muresan, E. Tzoukermann, and J. Klavans. 2001. Combining linguistic and machine learning
techniques for email. In Proceedings of the ACL/EACL 2001 Workshop on Computational Natural
Language Learning (ConLL), pages 290–297, Toulouse, France.

N. Okazaki, Y. Matsuo, and M. Ishizuka. 2004. Improving chronological sentence ordering by prece-
dence relation. In Proceedings of the 20th International Conference on Computational Linguistics
(COLING 2004), pages 750–756, Geneva, Switzerland.

M. Porter. 1980. An algorithm for suﬃx stripping. Program, 14(3):130–137.

D. Radev, T. Allison, S. Blair-Goldensohn, J. Blitzer, A. C¸ elebi, S. Dimitrov, E. Drabek, A. Hakim,
W. Lam, D. Liu, J. Otterbacher, H. Qi, H. Saggion, S. Teufel, M. Topper, A. Winkel, and Z. Zhang.
2004. MEAD—a platform for multidocument multilingual text summarization. In Proceedings of
the 4th International Conference on Language Resources and Evaluation (LREC 2004), Lisbon,
Portugal.

26


R. Schwartz, T. Imai, F. Jubala, L. Nguyen, and J. Makhoul. 1997. A maximum likelihood model
for topic classiﬁcation of broadcast news. In Proceedings of the Fifth European Speech Communica-
tion Association Conference on Speech Communication and Technology (Eurospeech-97), Rhodes,
Greece.

S. Sista, R. Schwartz, T. Leek, and J. Makhoul. 2002. An algorithm for unsupervised topic discovery
from broadcast news stories. In Proceedings of the 2002 Human Language Technology Conference
(HLT), pages 99–103, San Diego, California.

J. Turner and E. Charniak. 2005. Supervised and unsupervised learning for sentence compression.
In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL
2005), pages 290–297, Ann Arbor, Michigan.

L. Vanderwende, H. Suzuki, and C. Brockett. 2006. Microsoft Research at DUC2006: Task-focused
summarization with sentence simpliﬁcation and lexical expansion. In Proceedings of the 2006 Doc-
ument Understanding Conference (DUC 2006) at HLT/NAACL 2006, New York, New York.

A. Viterbi. 1967. Error bounds for convolution codes and an asymptotically optimal decoding algo-

rithm. IEEE Transactions on Information Theory, 13:260–269.

R. Wang, N. Stokes, W. Doran, E. Newman, J. Carthy, and J. Dunnion. 2005. Comparing Topiary-
In Lecture Notes in Computer Science: Advances in
style approaches to headline generation.
Information Retrieval: 27th European Conference on IR Research (ECIR 2005), volume 3408,
Santiago de Compostela, Spain. Springer Berlin / Heidelberg.

D. Zajic, B. Dorr, and R. Schwartz. 2004. BBN/UMD at DUC-2004: Topiary.

In Proceedings of
the 2004 Document Understanding Conference (DUC 2004) at NLT/NAACL 2004, pages 112–119,
Boston, Massachusetts.

D. Zajic, B. Dorr, J. Lin, and R. Schwartz. 2005a. UMD/BBN at MSE2005. In Proceedings of the
MSE2005 Track of the ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for
MT and/or Summarization, Ann Arbor, Michigan.

D. Zajic, B. Dorr, R. Schwartz, C. Monz, and J. Lin. 2005b. A sentence-trimming approach to
multi-document summarization. In Proceedings of the 2005 Document Understanding Conference
(DUC-2005) at NLT/EMNLP 2005, pages 151–158, Vancouver, Canada.

D. Zajic. 2007. Multiple Alternative Sentence Compressions (MASC) as a Tool for Automatic Sum-

marization Tasks. Ph.D. thesis, University of Maryland, College Park.

L. Zhou and E. Hovy. 2003. Headline summarization at ISI.

In Proceedings of the HLT-NAACL
2003 Text Summarization Workshop and Document Understanding Conference (DUC 2003), pages
174–178, Edmonton, Alberta.

27</biblio>
	</article>
	<article>
		<preamble>compression_phrases_Prog-Linear-jair.txt</preamble>
		<titre>Global Inference for Sentence Compression An Integer Linear Programming Approach</titre>
		<auteur>James Clarke</auteur>
		<abstract>Sentence compression holds promise for many applications ranging from summarization to subtitle generation. Our work views sentence compression as an optimization problem and uses integer linear programming (ILP) to infer globally optimal compressions in the presence of linguistically motivated constraints. We show how previous formulations of sentence compression can be recast as ILPs and extend these models with novel global constraints. Experimental results on written and spoken texts demonstrate improvements over state-of-the-art models.</abstract>
		<introduction>The computational treatment of sentence compression has recently attracted much attention in the literature. The task can be viewed as producing a summary of a single sentence that retains the most important information and remains grammatical (Jing, 2000). A sentence compression mechanism would greatly beneﬁt a wide range of applications. For example, in summarization, it could improve the conciseness of the generated summaries (Jing, 2000; Lin, 2003; Zajic, Door, Lin, & Schwartz, 2007). Other examples include compressing text to be displayed on small screens such as mobile phones or PDAs (Corston-Oliver, 2001), subtitle generation from spoken transcripts (Vandeghinste & Pan, 2004), and producing audio scanning devices for the blind (Grefenstette, 1998). Sentence compression is commonly expressed as a word deletion problem: given an in- put source sentence of words x = x1, x2, . . . , xn, the aim is to produce a target compression by removing any subset of these words (Knight & Marcu, 2002). The compression prob- lem has been extensively studied across diﬀerent modeling paradigms, both supervised and unsupervised. Supervised models are typically trained on a parallel corpus of source sen- tences and target compressions and come in many ﬂavors. Generative models aim to model the probability of a target compression given the source sentence either directly (Galley & McKeown, 2007) or indirectly using the noisy-channel model (Knight & Marcu, 2002; Turner & Charniak, 2005), whereas discriminative formulations attempt to minimize error rate on a training set. These include decision-tree learning (Knight & Marcu, 2002), maxi- mum entropy (Riezler, King, Crouch, & Zaenen, 2003), support vector machines (Nguyen, Shimazu, Horiguchi, Ho, & Fukushi, 2004), and large-margin learning (McDonald, 2006). c(cid:13)2008 AI Access Foundation. All rights reserved. Clarke & Lapata Unsupervised methods dispense with the parallel corpus and generate compressions either using rules (Turner & Charniak, 2005) or a language model (Hori & Furui, 2004). Despite diﬀerences in formulation, all these approaches model the compression process using local information. For instance, in order to decide which words to drop, they exploit information about adjacent words or constituents. Local models can do a good job at producing grammatical compressions, however they are somewhat limited in scope since they cannot incorporate global constraints on the compression output. Such constraints consider the sentence as a whole instead of isolated linguistic units (words or constituents). To give a concrete example we may want to ensure that each target compression has a verb, provided that the source had one in the ﬁrst place. Or that verbal arguments are present in the compression. Or that pronouns are retained. Such constraints are fairly intuitive and can be used to instill not only linguistic but also task speciﬁc information into the model. For instance, an application which compresses text to be displayed on small screens would presumably have a higher compression rate than a system generating subtitles from spoken text. A global constraint could force the former system to generate compressions with a ﬁxed rate or a ﬁxed number of words. Existing approaches do not model global properties of the compression problem for a good reason. Finding the best compression for a source sentence given the space of all possible compressions1 (this search process is often referred to as decoding or inference) can become intractable for too many constraints and overly long sentences. Typically, the decoding problem is solved eﬃciently using dynamic programming often in conjunction with heuristics that reduce the search space (e.g., Turner & Charniak, 2005). Dynamic programming guarantees we will ﬁnd the global optimum provided the principle of optimal- ity holds. This principle states that given the current state, the optimal decision for each of the remaining stages does not depend on previously reached stages or previously made decisions (Winston & Venkataramanan, 2003). However, we know this to be false in the case of sentence compression. For example, if we have included modiﬁers to the left of a noun in a compression then we should probably include the noun too or if we include a verb we should also include its arguments. With a dynamic programming approach we cannot easily guarantee such constraints hold. In this paper we propose a novel framework for sentence compression that incorporates constraints on the compression output and allows us to ﬁnd an optimal solution. Our formulation uses integer linear programming (ILP), a general-purpose exact framework for NP-hard problems. Speciﬁcally, we show how previously proposed models can be recast as integer linear programs. We extend these models with constraints which we express as linear inequalities. Decoding in this framework amounts to ﬁnding the best solution given a linear (scoring) function and a set of linear constraints that can be either global or local. Although ILP has been previously used for sequence labeling tasks (Roth & Yih, 2004; Punyakanok, Roth, Yih, & Zimak, 2004), its application to natural language generation is less widespread. We present three compression models within the ILP framework, each representative of an unsupervised (Knight & Marcu, 2002), semi-supervised (Hori & Furui, 2004), and fully supervised modeling approach (McDonald, 2006). We propose a small number of constraints ensuring that the compressions are structurally and semantically 1. There are 2 n possible compressions where n is the number of words in a sentence. 400 Global Inference for Sentence Compression valid and experimentally evaluate their impact on the compression task. In all cases, we show that the added constraints yield performance improvements. The remainder of this paper is organized as follows. Section 2 provides an overview of related work. In Section 3 we present the ILP framework and the compression models we employ in our experiments. Our constraints are introduced in Section 3.5. Section 4.3 discusses our experimental set-up and Section 5 presents our results. Discussion of future work concludes the paper.</introduction>
		<corps>2. Related Work

In this paper we develop several ILP-based compression models. Before presenting these
models, we brieﬂy summarize previous work addressing sentence compression with an em-
phasis on data-driven approaches. Next, we describe how ILP techniques have been used
in the past to solve other inference problems in natural language processing (NLP).

2.1 Sentence Compression

Jing (2000) was perhaps the ﬁrst to tackle the sentence compression problem. Her approach
uses multiple knowledge sources to determine which phrases in a sentence to remove. Central
to her system is a grammar checking module that speciﬁes which sentential constituents
are grammatically obligatory and should therefore be present in the compression. This
is achieved using simple rules and a large-scale lexicon. Other knowledge sources include
WordNet and corpus evidence gathered from a parallel corpus of source-target sentence
pairs. A phrase is removed only if it is not grammatically obligatory, not the focus of the
local context and has a reasonable deletion probability (estimated from a parallel corpus).
In contrast to Jing (2000), the bulk of the research on sentence compression relies ex-
clusively on corpus data for modeling the compression process without recourse to exten-
sive knowledge sources (e.g., WordNet). A large number of approaches are based on the
noisy-channel model (Knight & Marcu, 2002). These approaches consist of a language
model P (y) (whose role is to guarantee that compression output is grammatical), a channel
model P (x|y) (capturing the probability that the source sentence x is an expansion of the
target compression y), and a decoder (which searches for the compression y that maximizes
P (y)P (x|y)). The channel model is acquired from a parsed version of a parallel corpus; it
is essentially a stochastic synchronous context-free grammar (Aho & Ullman, 1969) whose
rule probabilities are estimated using maximum likelihood. Modiﬁcations of this model are
presented by Turner and Charniak (2005) and Galley and McKeown (2007) with improved
RESULTS.

In discriminative models (Knight & Marcu, 2002; Riezler et al., 2003; McDonald, 2006;
Nguyen et al., 2004) sentences are represented by a rich feature space (also induced from
parse trees) and the goal is to learn which words or word spans should be deleted in a given
context. For instance, in Knight and Marcu’s (2002) decision-tree model, compression is
performed deterministically through a tree rewriting process inspired by the shift-reduce
parsing paradigm. Nguyen et al. (2004) render this model probabilistic through the use
of support vector machines. McDonald (2006) formalizes sentence compression in a large-
margin learning framework without making reference to shift-reduce parsing. In his model
compression is a classiﬁcation task: pairs of words from the source sentence are classiﬁed

401


Clarke & Lapata

as being adjacent or not in the target compression. A large number of features are deﬁned
over words, parts-of-speech, phrase structure trees and dependencies. These features are
gathered over adjacent words in the compression and the words in-between which were
dropped (see Section 3.4.3 for a more detailed account).

While most compression models have been developed with written text in mind, Hori
and Furui (2004) propose a model for automatically transcribed spoken text. Their model
generates compressions through word deletion without using parallel data or syntactic in-
formation in any way. Assuming a ﬁxed compression rate, it searches for the compression
with the highest score using a dynamic programming algorithm. The scoring function con-
sists of a language model responsible for producing grammatical output, a signiﬁcance score
indicating whether a word is topical or not, and a score representing the speech recognizer’s
conﬁdence in transcribing a given word correctly.

2.2 Integer Linear Programming in NLP

ILPs are constrained optimization problems where both the objective function and the
constraints are linear equations with integer variables (see Section 3.1 for more details). ILP
techniques have been recently applied to several NLP tasks, including relation extraction
(Roth & Yih, 2004), semantic role labeling (Punyakanok et al., 2004), the generation of
route directions (Marciniak & Strube, 2005), temporal link analysis (Bramsen, Deshpande,
Lee, & Barzilay, 2006), set partitioning (Barzilay & Lapata, 2006), syntactic parsing (Riedel
& Clarke, 2006), and coreference resolution (Denis & Baldridge, 2007).

Most of these approaches combine a local classiﬁer with an inference procedure based
on ILP. The classiﬁer proposes possible answers which are assessed in the presence of global
ILP is used to make a ﬁnal decision that is consistent with the constraints
constraints.
and likely according to the classiﬁer. For example, the semantic role labeling task involves
identifying the verb-argument structure for a given sentence. Punyakanok et al. (2004) ﬁrst
use SNOW, a multi-class classiﬁer2 (Roth, 1998), to identify and label candidate arguments.
They observe that the labels assigned to arguments in a sentence often contradict each other.
To resolve these conﬂicts they propose global constraints (e.g., each argument should be
instantiated once for a given verb, every verb should have at least one argument) and use
ILP to reclassify the output of SNOW.

Dras (1999) develops a document paraphrasing model using ILP. The key premise of
his work is that in some cases one may want to rewrite a document so as to conform to
some global constraints such as length, readability, or style. The proposed model has three
ingredients: a set of sentence-level paraphrases for rewriting the text, a set of global con-
straints, and an objective function which quantiﬁes the eﬀect incurred by the paraphrases.
Under this formulation, ILP can be used to select which paraphrases to apply so that the
global constraints are satisﬁed. Paraphrase generation falls outside the scope of the ILP
model – sentence rewrite operations are mainly syntactic and provided by a module based
on synchronous tree adjoining grammar (S-TAG, Shieber & Schabes, 1990). Unfortunately,
only a proof-of-concept is presented; implementation and evaluation of this module are left
to future work.

2. SNOW’s learning algorithm is a variation of the Winnow update rule.

402


Global Inference for Sentence Compression

Our work models sentence compression as an optimization problem. We show how pre-
viously proposed models can be reformulated in the context of integer linear programming
which allows us to easily incorporate constraints during the decoding process. Our con-
straints are linguistically and semantically motivated and are designed to bring less local
syntactic knowledge into the model and help preserve the meaning of the source sentence.
Previous work has identiﬁed several important features for the compression task (Knight
& Marcu, 2002; McDonald, 2006); however, the use of global constraints is novel to our
knowledge. Although sentence compression has not been explicitly formulated in terms of
optimization, previous approaches rely on some optimization procedure for generating the
best compression. The decoding process in the noisy-channel model searches for the best
compression given the source and channel models. However, the compression found is usu-
ally sub-optimal as heuristics are used to reduce the search space or is only locally optimal
due to the search method employed. For example, in the work of Turner and Charniak
(2005) the decoder ﬁrst searches for the best combination of rules to apply. As it traverses
the list of compression rules, it removes sentences outside the 100 best compressions (ac-
cording to the channel model). This list is eventually truncated to 25 compressions. In
other models (Hori & Furui, 2004; McDonald, 2006) the compression score is maximized
using dynamic programming which however can yield suboptimal results (see the discussion
in Section 1).

Contrary to most other NLP work using ILP (a notable exception is Roth & Yih, 2005),
we do not view compression generation as a two stage process where learning and inference
are carried out sequentially (i.e., ﬁrst a local classiﬁer hypothesizes a list of possible an-
swers and then the best answer is selected using global constraints). Our models integrate
learning with inference in a uniﬁed framework where decoding takes place in the presence
of all available constraints, both local and global. Moreover, we investigate the inﬂuence
of our constraint set across models and learning paradigms. Previous work typically for-
mulates constraints for a single model (e.g., the SNOW classiﬁer) and learning paradigm
(e.g., supervised). We therefore assess how the constraint-based framework advocated in
this article inﬂuences the performance of expressive models (which require large amounts of
parallel data) and non-expressive ones (which use very little parallel data or none at all). In
other words, we are able to pose and answer the following question: what kinds of models
beneﬁt most from constraint-based inference?

Our work is close in spirit but rather diﬀerent in content to Dras (1999). We concentrate
on compression, a speciﬁc paraphrase type, and apply our models on the sentence-level. Our
constraints thus do not aﬀect the document as a whole but individual sentences. Further-
more, compression generation is an integral part of our ILP models, whereas Dras assumes
that paraphrases are generated by a separate process.

3. Framework

In this section we present the details of the proposed framework for sentence compression.
As mentioned earlier, our work models sentence compression directly as an optimization
problem. There are 2n possible compressions for each source sentence and while many
of these will be unreasonable, it is unlikely that only one compression will be satisfac-
tory (Knight & Marcu, 2002). Ideally, we require a function that captures the operations

403


Clarke & Lapata

(or rules) that can be performed on a sentence to create a compression while at the same
time factoring how desirable each operation makes the resulting compression. We can then
perform a search over all possible compressions and select the best one, as determined by
how desirable it is. A wide range of models can be expressed under this framework. The
prerequisites for implementing these are fairly low, we only require that the decoding pro-
cess be expressed as a linear function with a set of linear constraints. In practice, many
models rely on a Markov assumption for factorization which is usually solved with a dy-
namic programming-based decoding process. Such algorithms can be formulated as integer
linear programs with little eﬀort.

We ﬁrst give a brief introduction into integer linear programming, an extension of linear
programming for readers unfamiliar with mathematical programming. Our compression
models are next described in Section 3.4 and constraints in Section 3.5.

3.1 Linear Programming

Linear programming (LP) problems are optimization problems with constraints. They
consist of three parts:

• Decision variables. These are variables under our control which we wish to assign

optimal values to.

• A linear function (the objective function). This is the function we wish to minimize or
maximize. This function is inﬂuences by the values assigned to the decision variables.

• Constraints. Most problems will only allow the decision variables to take certain

values. These restrictions are the constraints.

These terms are best demonstrated with a simple example taken from Winston and
Venkataramanan (2003). Imagine a manufacturer of tables and chairs which we shall call
the Telfa Corporation. To produce a table, 1 hour of labor and 9 square board feet of wood
is required. Chairs require 1 hour of labor and 5 square board feet of wood. Telfa have
6 hours of labor and 45 square board feet of wood available. The proﬁt made from each
table is 8 GBP and 5 GBP for chairs. We wish to determine the number of tables and
chairs that should be manufactured to maximize Telfa’s proﬁt.

First, we must determine the decision variables. In our case we deﬁne:

x1 = number of tables manufactured
x2 = number of chairs manufactured

Our objective function is the value we wish to maximize, namely the proﬁt.

Proﬁt = 8x1 + 5x2

There are two constraints in this problem: we must not exceed 6 hours of labor and no
more than 45 square board feet of wood must be used. Also, we cannot create a negative
amount of chairs or tables:

404


Global Inference for Sentence Compression

Labor constraint
Wood constraint
Variable constraints

x1 + x2 ≤ 6
9x1 + 5x2 ≤ 45
x1 ≥ 0
x2 ≥ 0

Once the decision variables, objective function and constraints have been determined we
can express the LP model:

subject to (s.t.)

max z = 8x1 + 5x2

(Objective function)

x1 + x2 ≤ 6 (Labor constraint)
9x1 + 5x2 ≤ 45 (Wood constraint)

x1 ≥ 0
x2 ≥ 0

Two of the most basic concepts involved in solving LP problems are the feasibility region
and optimal solution. The optimal solution is one in which all constraints are satisﬁed
and the objective function is minimized or maximized. A speciﬁcation of the value for
each decision variable is referred to as a point. The feasibility region for a LP is a region
consisting of the set of all points that satisfy all the LP’s constraints. The optimal solution
lies within this feasibility region, it is the point with the minimum or maximum objective
function value.

A set of points satisfying a single linear inequality is a half-space. The feasibility region
is deﬁned by a the intersection of m half-spaces (for m linear inequalities) and forms a
polyhedron. Our Telfa example forms a polyhedral set (a polyhedral convex set) from
the intersection of our four constraints. Figure 1a shows the feasible region for the Telfa
example. To ﬁnd the optimal solution we graph a line (or hyperplane) on which all points
have the same objective function value. In maximization problems it is called the isoproﬁt
line and in minimization problems the isocost line. One isoproﬁt line is represented by the
dashed black line in Figure 1a. Once we have one isoproﬁt line we can ﬁnd all other isoproﬁt
lines by moving parallel to the original isoproﬁt line.

The extreme points of the polyhedral set are deﬁned as the intersections of the lines
that form the boundaries of the polyhedral set (points A B C and D in Figure 1a). It can
be shown that any LP that has an optimal solution, has an extreme point that is globally
optimal. This reduces the search space of the optimization problem to ﬁnding the extreme
point with the highest or lowest value. The simplex algorithm (Dantzig, 1963) solves LPs
by exploring the extreme points of a polyhedral set. Speciﬁcally, it moves from one extreme
point to an adjacent extreme point (extreme points that lie on the same line segment) until
an optimal extreme point is found. Although the simplex algorithm has an exponential
worst-case complexity, in practice the algorithm is very eﬃcient.
The optimal solution for the Telfa example is z = 165

4 . Thus, to
achieve a maximum proﬁt of 41.25 GBP they must build 3.75 tables and 2.25 chairs. This
is obviously impossible as we would not expect people to buy fractions of tables and chairs.
Here, we want to be able to constrain the problem such that the decision variables can only
take integer values. This can be done with Integer Linear Programming.

4 , x1 = 15

4 , x2 = 9

405


Clarke & Lapata

9x1 + 5x2 = 45

= LP’s feasible region

B

Optimal LP solution

C

b.

10

9

8

7

6

5

4

3

2

x2

a.

10

x2

9

8

7

6

5

4

3

2

1

A

0

0

1

2

3

4

x1

x1 + x2 = 6

D
5

6

7

1

1

0

0

9x1+ 5x2 = 45

= IP feasible point
= IP relaxation’s feasible region

Optimal LP solution

x1 + x2 = 6

1

2

3

x1

4

5

6

7

Figure 1: Feasible region for the Telfa example using linear (graph (a)) and integer linear

(graph (b)) programming

3.2 Integer Linear Programming

Integer linear programming (ILP) problems are LP problems in which some or all of the
variables are required to be non-negative integers. They are formulated in a similar manner
to LP problems with the added constraint that all decision variables must take non-negative
integer values.

To formulate the Telfa problem as an ILP model we merely add the constraints that x1

and x2 must be integer. This gives:

max z = 8x1 + 5x2

(Objective function)

subject to (s.t.)

x1 + x2 ≤
9x1 + 5x2 ≤

6 (Labor constraint)
45 (Wood constraint)

x1 ≥ 0; x1 integer
x2 ≥ 0; x2 integer

For LP models, it can be proved that the optimal solution lies on an extreme point of
the feasible region. In the case of integer linear programs, we only wish to consider points
that are integer values. This is illustrated in Figure 1b for the Telfa problem. In contrast to
linear programming, which can be solved eﬃciently in the worst case, integer programming
problems are in many practical situations NP-hard (Cormen, Leiserson, & Rivest, 1992).

406


Global Inference for Sentence Compression

Fortunately, ILPs are a well studied optimization problem and a number of techniques have
been developed to ﬁnd the optimal solution. Two such techniques are the cutting planes
method (Gomory, 1960) and the branch-and-bound method (Land & Doig, 1960). We
brieﬂy discuss these methods here. For a more detailed treatment we refer the interested
reader to Winston and Venkataramanan (2003) or Nemhauser and Wolsey (1988).

The cutting planes method adds extra constraints to slice parts of the feasible region
until it contains only integer extreme points. However, this process can be diﬃcult or
impossible (Nemhauser & Wolsey, 1988). The branch-and-bound method enumerates all
points in the ILP’s feasible region but prunes those sections in the region which are known
to be sub-optimal. It does this by relaxing the integer constraints and solving the resulting
LP problem (known as the LP relaxation). If the solution of the LP relaxation is integral,
then it is the optimal solution. Otherwise, the resulting solution provides an upper bound
on the solution for the ILP. The algorithm proceeds by creating two new sub-problems based
on the non-integer solution for one variable at a time. These are solved and the process
repeats until the optimal integer solution is found.

Using the branch-and-bound method, we ﬁnd that the optimal solution to the Telfa
problem is z = 40, x1 = 5, x2 = 0; thus, to achieve a maximum proﬁt of 40 GBP, Telfa
must manufacture 5 tables and 0 chairs. This is a relatively simple problem, which could be
solved merely by inspection. Most ILP problems will involve many variables and constraints
resulting in a feasible region with a large number of integer points. The branch-and-bound
procedure can eﬃciently solve such ILPs in a matter of seconds and forms part of many
commercial ILP solvers. In our experiments we use lp solve 3, a free optimization package
which relies on the simplex algorithm and brand-and-bound methods for solving ILPs.

Note that under special circumstances other solving methods may be applicable. For
example, implicit enumeration can be used to solve ILPs where all the variables are binary
(also known as pure 0−1 problems).
Implicit enumeration is similar to the branch-and-
bound method, it systematically evaluates all possible solutions, without however explicitly
solving a (potentially) large number of LPs derived from the relaxation. This removes
much of the computational complexity involved in determining if a sub-problem is infea-
sible. Furthermore, for a class of ILP problems known as minimum cost network ﬂow
problems (MCNFP), the LP relaxation always yields an integral solution. These problems
can therefore be treated as LP problems.

In general, a model will yield an optimal solution in which all variables are integers if
the constraint matrix has a property known as total unimodularity. A matrix A is totally
unimodular if every square sub-matrix of A has its determinant equal to 0, +1 or −1.
It is the case that the more the constraint matrix looks totally unimodular, the easier
the problem will be to solve by branch-and-bound methods.
In practice it is good to
formulate ILPs where as many variables as possible have coeﬃcients of 0, +1 or −1 in the
constraints (Winston & Venkataramanan, 2003).

3.3 Constraints and Logical Conditions

Although integer variables in ILP problems may take arbitrary values, these are frequently
are restricted to 0 and 1. Binary variables (0−1 variables) are particularly useful for rep-

3. The software is available from http://lpsolve.sourceforge.net/.

407


Clarke & Lapata

Condition
Implication
Iﬀ
Or
Xor
And
Not

Statement
if a then b
a if and only if b
a or b or c
a xor b xor c
a and b
not a

Constraint
b − a ≥ 0
a − b = 0
a + b + c ≥ 1
a + b + c = 1
a = 1; b = 1
1 − a = 1

Table 1: How to represent logical conditions using binary variables and constraints in ILP.

resenting a variety of logical conditions within the ILP framework through the use of con-
straints. Table 1 lists several logical conditions and their equivalent constraints.

We can also express transitivity, i.e., “c if and only if a and b”. Although it is of-
ten thought that transitivity can only be expressed as a polynomial expression of binary
variables (i.e., ab = c), it is possible to replace the latter by the following linear inequali-
ties (Williams, 1999):

(1 − c) + a ≥ 1
(1 − c) + b ≥ 1
c + (1 − a) + (1 − b) ≥ 1

This can be easily extended to model indicator variables representing whether a set of binary
variables can take certain values.

3.4 Compression Models

In this section we describe three compression models which we reformulate as integer linear
programs. Our ﬁrst model is a simple language model which has been used as a baseline in
previous research (Knight & Marcu, 2002). Our second model is based on the work of Hori
and Furui (2004); it combines a language model with a corpus-based signiﬁcance scoring
function (we omit here the conﬁdence score derived from the speech recognizer since our
models are applied to text only). This model requires a small amount of parallel data to
learn weights for the language model and the signiﬁcance score.

Our third model is fully supervised, it uses a discriminative large-margin framework
(McDonald, 2006), and is trained trained on a larger parallel corpus. We chose this model
instead of the more popular noisy-channel or decision-tree models, for two reasons, a practi-
cal one and a theoretical one. First, McDonald’s (2006) model delivers performance superior
to the decision-tree model (which in turn performs comparably to the noisy-channel). Sec-
ond, the noisy channel is not an entirely appropriate model for sentence compression. It
uses a language model trained on uncompressed sentences even though it represents the
probability of compressed sentences. As a result, the model will consider compressed sen-
tences less likely than uncompressed ones (a further discussion is provided by Turner &
Charniak, 2005).

408


Global Inference for Sentence Compression

3.4.1 Language Model

A language model is perhaps the simplest model that springs to mind. It does not require
a parallel corpus (although a relatively large monolingual corpus is necessary for training),
and will naturally prefer short sentences to longer ones. Furthermore, a language model can
be used to drop words that are either infrequent or unseen in the training corpus. Knight
and Marcu (2002) use a bigram language model as a baseline against their noisy-channel
and decision-tree models.

Let x = x1, x2, . . . , xn denote a source sentence for which we wish to generate a target
compression. We introduce a decision variable for each word in the source and constrain it
to be binary; a value of 0 represents a word being dropped, whereas a value of 1 includes
the word in the target compression. Let:

δi =

(

1 if xi is in the compression
0 otherwise

∀i ∈ [1 . . . n]

If we were using a unigram language model, our objective function would maximize the
overall sum of the decision variables (i.e., words) multiplied by their unigram probabilities
(all probabilities throughout this paper are log-transformed):

n

max

δi · P (xi)

i=1
X

(1)

Thus, if a word is selected, its corresponding δi is given a value of 1, and its probability
P (xi) according to the language model will be counted in our total score.

A unigram language model will probably generate many ungrammatical compressions.
We therefore use a more context-aware model in our objective function, namely a trigram
model. Dynamic programming would be typically used to decode a language model by
traversing the sentence in a left-to-right manner. Such an algorithm is eﬃcient and provides
all the context required for a conventional language model. However, it can be diﬃcult
or impossible to incorporate global constraints into such a model as decisions on word
inclusion cannot extend beyond a three word window. By formulating the decoding process
for a trigram language model as an integer linear program we are able to take into account
constraints that aﬀect the compressed sentence more globally. This process is a much more
involved task than in the unigram case where there is no context, instead we must now
make decisions based on word sequences rather than isolated words. We ﬁrst create some
additional decision variables:

αi =

(

1 if xi starts the compression
0 otherwise

∀i ∈ [1 . . . n]

βij = 



γijk = 




1 if sequence xi, xj ends

the compression

0 otherwise

∀i ∈ [0 . . . n − 1]
∀j ∈ [i + 1 . . . n]

1 if sequence xi, xj, xk

∀i ∈ [0 . . . n − 2]

is in the compression ∀j ∈ [i + 1 . . . n − 1]

0 otherwise

∀k ∈ [j + 1 . . . n]

409


Clarke & Lapata

Our objective function is given in Equation (2). This is the sum of all possible trigrams
that can occur in all compressions of the source sentence where x0 represents the ‘start’
token and xi is the ith word in sentence x. Equation (3) constrains the decision variables
to be binary.

n

max z =

αi · P (xi|start)

i=1
X

n−2

n−1

n

+

+

γijk · P (xk|xi, xj)

i=1
X
n−1

j=i+1
X
n

Xk=j+1

βij · P (end|xi, xj)

subject to:

i=0
X

j=i+1
X

δi, αi, βij, γijk = 0 or 1

(2)

(3)

The objective function in (2) allows any combination of trigrams to be selected. This
means that invalid trigram sequences (e.g., two or more trigrams containing the ‘end’ token)
could appear in the target compression. We avoid this situation by introducing sequential
constraints (on the decision variables δi, γijk, αi, and βij) that restrict the set of allowable
trigram combinations.

Constraint 1 Exactly one word can begin a sentence.

αi = 1

n

i=1
X

(4)

Constraint 2 If a word is included in the sentence it must either start the sentence or be
preceded by two other words or one other word and the ‘start’ token x0.

k−2

k−1

δk − αk −

γijk = 0

j=1
X

i=0
X
∀k : k ∈ [1 . . . n]

(5)

Constraint 3 If a word is included in the sentence it must either be preceded by one
word and followed by another or it must be preceded by one word and end the sentence.

j−1

n

j−1

δj −

Xi=0

Xk=j+1

γijk −

βij = 0

Xi=0
∀j : j ∈ [1 . . . n]

(6)

Constraint 4 If a word is in the sentence it must be followed by two words or followed
by one word and then the end of the sentence or it must be preceded by one word and end
the sentence.

n−1

n

n

i−1

δi −

γijk −

βij −

βhi = 0

Xj=i+1

Xk=j+1

Xj=i+1

Xh=0

∀i : i ∈ [1 . . . n]

(7)

410


Global Inference for Sentence Compression

Constraint 5 Exactly one word pair can end the sentence.

n−1

n

i=0
X

j=i+1
X

βij = 1

(8)

The sequential constraints described above ensure that the second order factorization (for
trigrams) holds and are diﬀerent from our compression-speciﬁc constraints which are pre-
sented in Section 3.5.

Unless normalized by sentence length, a language model will naturally prefer one-word
output. This normalization is however non-linear and cannot be incorporated into our ILP
Instead, we impose a constraint on the length of the compressed sentence.
formulation.
Equation (9) below forces the compression to contain at least b tokens.

δi ≥ b

n

i=1
X

(9)

Alternatively, we could force the compression to be exactly b tokens (by substituting the
inequality with an equality in (9)) or to be less than b tokens (by replacing ≥ with ≤).4
The constraint in (9) is language model-speciﬁc and is not used elsewhere.

3.4.2 Significance Model

The language model just described has no notion of which content words to include in the
compression and thus prefers words it has seen before. But words or constituents will be of
diﬀerent relative importance in diﬀerent documents or even sentences.

Inspired by Hori and Furui (2004), we add to our objective function (see Equation (2))
In Hori and Furui’s
a signiﬁcance score designed to highlight important content words.
original formulation each word is weighted by a score similar to un-normalized tf ∗ idf . The
signiﬁcance score is not applied indiscriminately to all words in a sentence but solely to
topic-related words, namely nouns and verbs. Our score diﬀers in one respect. It combines
document-level with sentence-level signiﬁcance. So in addition to tf ∗ idf , each word is
weighted by its level of embedding in the syntactic tree.

Intuitively, in a sentence with multiply nested clauses, more deeply embedded clauses
tend to carry more semantic content. This is illustrated in Figure 2 which depicts the
clause embedding for the sentence “Mr Field has said he will resign if he is not reselected,
a move which could divide the party nationally”. Here, the most important information is
conveyed by clauses S3 (he will resign) and S4 (if he is not reselected) which are embedded.
Accordingly, we should give more weight to words found in these clauses than in the main
clause (S1 in Figure 2). A simple way to enforce this is to give clauses weight proportional
to the level of embedding. Our modiﬁed signiﬁcance score becomes:

I(xi) =

l
N

· fi log

Fa
Fi

(10)

where xi is a topic word, fi and Fi are the frequency of xi in the document and corpus
respectively, Fa is the sum of all topic words in the corpus, l is the number of clause

4. Compression rate can be also limited to a range by including two inequality constraints.

411


Clarke & Lapata

S1

S2
Mr Field has said

S3
he will resign

S4
if he is not reselected

, a move

SBAR
which could divide the party nationally

Figure 2: The clause embedding of the sentence “Mr Field has said he will resign if he is
not reselected, a move which could divide the party nationally”; nested boxes
correspond to nested clauses.

constituents above xi, and N is the deepest level of clause embedding. Fa and Fi are
estimated from a large document collection, fi is document-speciﬁc, whereas l
N is sentence-
speciﬁc. So, in Figure 2 the term l
N is 1.0 (4/4) for clause S4, 0.75 (3/4) for clause S3, and
so on. Individual words inherit their weight from their clauses.

The modiﬁed objective function with the signiﬁcance score is given below:

n

n

max z =

δi · λI(xi) +

αi · P (xi|start)

Xi=1

n−2

n−1

n

Xi=1

+

+

γijk · P (xk|xi, xj)

i=1
X
n−1

j=i+1
X
n

Xk=j+1

βij · P (end|xi, xj)

(11)

i=0
X

j=i+1
X

We also add a weighting factor (λ) to the objective, in order to counterbalance the impor-
tance of the language model and the signiﬁcance score. The weight is tuned on a small
parallel corpus. The sequential constraints from Equations (4)–(8) are again used to ensure
that the trigrams are combined in a valid way.

3.4.3 Discriminative Model

As a fully supervised model, we used the discriminative model presented by McDonald
(2006). This model uses a large-margin learning framework coupled with a feature set
deﬁned on compression bigrams and syntactic structure.

Let x = x1, . . . , xn denote a source sentence with a target compression y = y1, . . . , ym
where each yj occurs in x. The function L(yi) ∈ {1 . . . n} maps word yi in the target com-

412


Global Inference for Sentence Compression

pression to the index of the word in the source sentence, x. We also include the constraint
that L(yi) < L(yi+1) which forces each word in x to occur at most once in the compression
y. Let the score of a compression y for a sentence x be:

s(x, y)

(12)

This score is factored using a ﬁrst-order Markov assumption on the words in the target
compression to give:

|y|

s(x, y) =

s(x, L(yj−1), L(yj))

(13)

The score function is deﬁned to be the dot product between a high dimensional feature
representation and a corresponding weight vector:

j=2
X

|y|

s(x, y) =

w · f (x, L(yj−1), L(yj))

(14)

Xj=2

Decoding in this model amounts to ﬁnding the combination of bigrams that maximizes
the scoring function in (14). McDonald (2006) uses a dynamic programming approach
where the maximum score is found in a left-to-right manner. The algorithm is an extension
of Viterbi for the case in which scores factor over dynamic sub-strings (Sarawagi & Cohen,
2004; McDonald, Crammer, & Pereira, 2005a). This allows back-pointers to be used to
reconstruct the highest scoring compression as well as the k-best compressions.

Again this is similar to the trigram language model decoding process (see Section 3.4.1),
except that here a bigram model is used. Consequently, the ILP formulation is slightly
simpler than that of the trigram language model. Let:

δi =

(

1 if xi is in the compression
0 otherwise

(1 ≤ i ≤ n)

We then introduce some more decision variables:

αi =

(

1 if xi starts the compression
0 otherwise

∀i ∈ [1 . . . n]

βi =

(

1 if word xi ends the compression
0 otherwise

∀i ∈ [1 . . . n]

γij =

(

1 if sequence xi, xj is in the compression ∀i ∈ [1 . . . n − 1]
∀j ∈ [i + 1 . . . n]
0 otherwise

The discriminative model can be now expressed as:

n

max z =

αi · s(x, 0, i)

Xi=1

n−1

n

+

+

γij · s(x, i, j)

i=1
X
n

j=i+1
X

βi · s(x, i, n + 1)

i=1
X

413

(15)


Clarke & Lapata

Constraint 1 Exactly one word can begin a sentence.

αi = 1

n

i=1
X

(16)

Constraint 2 If a word is included in the sentence it must either start the compression
or follow another word.

j

δj − αj −

γij = 0

Xi=1
∀j : j ∈ [1 . . . n]

(17)

Constraint 3 If a word is included in the sentence it must be either followed by another
word or end the sentence.

n

δi −

γij − βi = 0

j=i+1
X

∀i : i ∈ [1 . . . n]

Constraint 4 Exactly one word can end a sentence.

βi = 1

n

i=1
X

(18)

(19)

Again, the sequential constraints in Equations (16)–(19) are necessary to ensure that the
resulting combination of bigrams are valid.

The current formulation provides a single optimal compression given the model. How-
ever, McDonald’s (2006) dynamic programming algorithm is capable of returning the k-best
compressions; this is useful for their learning algorithm described later. In order to produce
k-best compressions, we must rerun the ILP with extra constraints which forbid previous
solutions. In other words, we ﬁrst formulate the ILP as above, solve it, add its solution to
the k-best list, and then create a set of constraints that forbid the conﬁguration of δi decision
variables which form the current solution. The procedure is repeated until k compressions
are found.

The computation of the compression score crucially relies on the dot product between
a high dimensional feature representation and a corresponding weight vector (see Equa-
tion (14)). McDonald (2006) employs a rich feature set deﬁned over adjacent words and
individual parts-of-speech, dropped words and phrases from the source sentence, and de-
pendency structures (also of the source sentence). These features are designed to mimic the
information presented in the previous noisy-channel and decision-tree models of Knight and
Marcu (2002). Features over adjacent words are used as a proxy to the language model of
the noisy channel. Unlike other models, which treat the parses as gold standard, McDonald
uses the dependency information as another form of evidence. Faced with parses that are
noisy the learning algorithm can reduce the weighting given to those features if they prove

414


Global Inference for Sentence Compression

poor discriminators on the training data. Thus, the model should be much more robust
and portable across diﬀerent domains and training corpora.

The weight vector, w is learned using the Margin Infused Relaxed Algorithm (MIRA,
Crammer & Singer, 2003) a discriminative large-margin online learning technique (McDon-
ald, Crammer, & Pereira, 2005b). This algorithm learns by compressing each sentence and
comparing the result with the gold standard. The weights are updated so that the score of
the correct compression (the gold standard) is greater than the score of all other compres-
sions by a margin proportional to their loss. The loss function is the number of words falsely
retained or dropped in the incorrect compression relative to the gold standard. A source
sentence will have exponentially many compressions and thus exponentially many margin
constraints. To render learning computationally tractable, McDonald et al. (2005b) create
constraints only on the k compressions that currently have the highest score, bestk(x; w).

3.5 Constraints

We are now ready to describe our compression-speciﬁc constraints. The models presented
in the previous sections contain only sequential constraints and are thus equivalent to their
original formulation. Our constraints are linguistically and semantically motivated in a
similar fashion to the grammar checking component of Jing (2000). However, they do
not rely on any additional knowledge sources (such as a grammar lexicon or WordNet)
beyond the parse and grammatical relations of the source sentence. We obtain these from
RASP (Briscoe & Carroll, 2002), a domain-independent, robust parsing system for English.
However, any other parser with broadly similar output (e.g., Lin, 2001) could also serve our
purposes. Our constraints revolve around modiﬁcation, argument structure, and discourse
related factors.

Modiﬁer Constraints Modiﬁer constraints ensure that relationships between head words
and their modiﬁers remain grammatical in the compression:

δi − δj ≥ 0
∀i, j : xj ∈ xi’s ncmods
δi − δj ≥ 0
∀i, j : xj ∈ xi’s detmods

(20)

(21)

Equation (20) guarantees that if we include a non-clausal modiﬁer5 (ncmod) in the compres-
sion (such as an adjective or a noun) then the head of the modiﬁer must also be included;
this is repeated for determiners (detmod) in (21). In Table 2 we illustrate how these con-
straints disallow the deletion of certain words (starred sentences denote compressions that
would not be possible given our constraints). For example, if the modiﬁer word Pasok from
sentence (1a) is in the compression, then its head Party will also included (see (1b)).

We also want to ensure that the meaning of the source sentence is preserved in the
compression, particularly in the face of negation. Equation (22) implements this by forcing
not in the compression when the head is included (see sentence (2b) in Table 2). A similar
constraint is added for possessive modiﬁers (e.g., his, our), including genitives (e.g., John’s

5. Clausal modiﬁers (cmod) are adjuncts modifying entire clauses. In the example “he ate the cake because

he was hungry”, the because-clause is a modiﬁer of the sentence “he ate the cake”.

415


Clarke & Lapata

1a.

He became a power player in Greek Politics in 1974, when he founded the
socialist Pasok Party.

1b.

*He became a power player in Greek Politics in 1974, when he founded the

Pasok.

2a. We took these troubled youth who don’t have fathers, and brought them into

a room to Dads who don’t have their children.

2b.

*We took these troubled youth who do have fathers, and brought them into a

room to Dads who do have their children.

2c.

*We took these troubled youth who don’t have fathers, and brought them into

a room to Dads who don’t have children.
The chain stretched from Uganda to Grenada and Nicaragua, since the 1970s.

*Stretched from Uganda to Grenada and Nicaragua, since the 1970s.
*The chain from Uganda to Grenada and Nicaragua, since the 1970s.
*The chain stretched Uganda to Grenada and Nicaragua, since the 1970s.
*The chain stretched from to Grenada and Nicaragua, since the 1970s.
*The chain stretched from Uganda to Grenada Nicaragua, since the 1970s.

3a.
3b.
3c.
3d.
3e.
3f.

Table 2: Examples of compressions disallowed by our set of constraints.

gift), as shown in Equation (23). An example of the possessive constraint is given in
sentence (2c) in Table 2.

δi − δj = 0
∀i, j : xj ∈ xi’s ncmods ∧ xj = not
δi − δj = 0
∀i, j : xj ∈ xi’s possessive mods

(22)

(23)

Argument Structure Constraints We also deﬁne a few intuitive constraints that take
the overall sentence structure into account. The ﬁrst constraint (Equation (24)) ensures
that if a verb is present in the compression then so are its arguments, and if any of the
arguments are included in the compression then the verb must also be included. We thus
force the program to make the same decision on the verb, its subject, and object (see
sentence (3b) in Table 2).

δi − δj = 0
∀i, j : xj ∈ subject/object of verb xi

(24)

Our second constraint forces the compression to contain at least one verb provided the
source sentence contains one as well:

δi ≥ 1

Xi:xi∈verbs

(25)

The constraint entails that it is not possible to drop the main verb stretched from sen-
tence (3a) (see also sentence (3c) in Table 2).

416


Global Inference for Sentence Compression

Other sentential constraints include Equations (26) and (27) which apply to prepo-
sitional phrases and subordinate clauses. These constraints force the introducing term
(i.e., the preposition, or subordinator) to be included in the compression if any word from
within the syntactic constituent is also included. By subordinator we mean wh-words
(e.g., who, which, how, where), the word that, and subordinating conjunctions (e.g., after,
although, because). The reverse is also true, i.e., if the introducing term is included, at
least one other word from the syntactic constituent should also be included.

δi − δj ≥ 0
∀i, j : xj ∈ PP/SUB
∧xi starts PP/SUB
δi − δj ≥ 0

Xi:xi∈PP/SUB
∀j : xj starts PP/SUB

(26)

(27)

As an example consider sentence (3d) from Table 2. Here, we cannot drop the preposition
from if Uganda is in the compression. Conversely, we must include from if Uganda is in the
compression (see sentence (3e)).

We also wish to handle coordination.

If two head words are conjoined in the source
sentence, then if they are included in the compression the coordinating conjunction must
also be included:

(1 − δi) + δj ≥ 1
(1 − δi) + δk ≥ 1
δi + (1 − δj) + (1 − δk) ≥ 1
∀i, j, k : xj ∧ xk conjoined by xi

(28)

(29)

(30)

Consider sentence (3f) from Table 2.
compression, then we must include the conjunction and.

If both Uganda and Nicaragua are present in the

Finally, Equation (31) disallows anything within brackets in the source sentence from
being included in the compression. This is a somewhat superﬁcial attempt at excluding
parenthetical and potentially unimportant material from the compression.

δi = 0
∀i : xi ∈ bracketed words (inc parentheses)

(31)

Discourse Constraints Our discourse constraint concerns personal pronouns. Specif-
ically, Equation (32) forces personal pronouns to be included in the compression. The
constraint is admittedly more important for generating coherent documents (as opposed to
individual sentences). It nevertheless has some impact on sentence-level compressions, in
particular when verbal arguments are missed by the parser. When these are pronominal,
constraint (32) will result in more grammatical output since some of the argument structure
of the source sentence will be preserved in the compression.

δi = 1
∀i : xi ∈ personal pronouns

417

(32)


Clarke & Lapata

We should note that some of the constraints described above would be captured by
models that learn synchronous deletion rules from a corpus. For example, the noisy-channel
model of Knight and Marcu (2002) learns not to drop the head when the latter is modiﬁed
by an adjective or a noun, since the transformations DT NN → DT or AJD NN → ADJ are
almost never seen in the data. Similarly, the coordination constraint (Equations (28)–(30))
would be enforced using Turner and Charniak’s (2005) special rules — they enhance their
parallel grammar with rules modeling more structurally complicated deletions than those
attested in their corpus. In designing our constraints we aimed at capturing appropriate
deletions for many possible models, including those that do not rely on a training corpus
or do not have an explicit notion of a parallel grammar (e.g., McDonald, 2006). The
modiﬁcation constraints would presumably be redundant for the noisy-channel model, which
could otherwise beneﬁt from more specialized constraints, e.g., targeting sparse rules or
noisy parse trees, however we leave this to future work.

Another feature of the modeling framework presented here is that deletions (or non-
deletions) are treated as unconditional decisions. For example, we require not to drop the
noun in adjective-noun sequences if the adjective is not deleted as well. We also require to
always include a verb in the compression if the source sentence has one. These hardwired de-
cisions could in some cases prevent valid compressions from being considered. For instance,
it is not possible to compress the sentence “this is not appropriate behavior” to “this is
not appropriate” or“Bob loves Mary and John loves Susan” to “Bob loves Mary and John
Susan”. Admittedly we lose some expressive power, yet we ensure that the compressions
will be broadly grammatically, even for unsupervised or semi-supervised models. Further-
more, in practice we ﬁnd that our models consistently outperform non-constraint-based
alternatives, without extensive constraint engineering.

3.6 Solving the ILP

As we mentioned earlier (Section 3.1), solving ILPs is NP-hard.
In cases where the co-
eﬃcient matrix is unimodular, it can be shown that the optimal solution to the linear
program is integral. Although the coeﬃcient matrix in our problems is not unimodular, we
obtained integral solutions for all sentences we experimented with (approximately 3,000,
see Section 4.1 for details). We conjecture that this is due to the fact that all of our vari-
ables have 0, +1 or −1 coeﬃcients in the constraints and therefore our constraint matrix
shares many properties of a unimodular matrix. We generate and solve an ILP for every
sentence we wish to compress. Solve times are less than a second per sentence (including
input-output overheads) for all models presented here.

4. Experimental Set-up

Our evaluation experiments were motivated by three questions: (1) Do the constraint-
based compression models deliver performance gains over non-constraint-based ones? We
expect better compressions for the model variants which incorporate compression-speciﬁc
constraints. (2) Are there diﬀerences among constraint-based models? Here, we would like
to investigate how much modeling power is gained by the addition of the constraints. For
example, it may be the case that a state-of-the-art model like McDonald’s (2006) does not
beneﬁt much from the addition of constraints. And that their eﬀect is much bigger for less

418


Global Inference for Sentence Compression

sophisticated models. (3) How do the models reported in this paper port across domains?
In particular, we are interested in assessing whether the models and proposed constraints
are general and robust enough to produce good compressions for both written and spoken
texts.

We next describe the data sets on which our models were trained and tested (Section 4.1),
explain how model parameters were estimated (Section 4.2) and present our evaluation setup
(Section 4.3). We discuss our results in Section 5.

4.1 Corpora

Our intent was to assess the performance of the models just described on written and spoken
text. The appeal of written text is understandable since most summarization work today
focuses on this domain. Speech data not only provides a natural test-bed for compression
applications (e.g., subtitle generation) but also poses additional challenges. Spoken utter-
ances can be ungrammatical, incomplete, and often contain artefacts such as false starts,
interjections, hesitations, and disﬂuencies. Rather than focusing on spontaneous speech
which is abundant in these artefacts, we conduct our study on the less ambitious domain
of broadcast news transcripts. This lies in-between the extremes of written text and spon-
taneous speech as it has been scripted beforehand and is usually read oﬀ on autocue.

Previous work on sentence compression has almost exclusively used the Ziﬀ-Davis corpus
for training and testing purposes. This corpus originates from a collection of news articles
on computer products. It was created automatically by matching sentences that occur in
an article with sentences that occur in an abstract (Knight & Marcu, 2002). The abstract
sentences had to contain a subset of the source sentence’s words and the word order had
In earlier work (Clarke & Lapata, 2006) we have argued that the
to remain the same.
Ziﬀ-Davis corpus is not ideal for studying compression for several reasons. First, we showed
that human-authored compressions diﬀer substantially from the Ziﬀ-Davis which tends to
be more aggressively compressed. Second, humans are more likely to drop individual words
than lengthy constituents. Third, the test portion of the Ziﬀ-Davis contains solely 32 sen-
tences. This is an extremely small data set to reveal any statistically signiﬁcant diﬀerences
among systems. In fact, previous studies relied almost exclusively on human judgments for
assessing the well-formedness of the compressed output, and signiﬁcance tests are reported
for by-subjects analyses only.

We thus focused in the present study on manually created corpora. Speciﬁcally, we
asked annotators to perform sentence compression by removing tokens on a sentence-by-
sentence basis. Annotators were free to remove any words they deemed superﬂuous provided
their deletions: (a) preserved the most important information in the source sentence, and
(b) ensured the compressed sentence remained grammatical. If they wished, they could leave
a sentence uncompressed by marking it as inappropriate for compression. They were not
allowed to delete whole sentences even if they believed they contained no information content
with respect to the story as this would blur the task with abstracting. Following these
guidelines, our annotators produced compressions of 82 newspaper articles (1,433 sentences)
from the British National Corpus (BNC) and the American News Text corpus (henceforth
written corpus) and 50 stories (1,370 sentences) from the HUB-4 1996 English Broadcast
News corpus (henceforth spoken corpus). The written corpus contains articles from The LA

419


Clarke & Lapata

Times, Washington Post, Independent, The Guardian and Daily Telegraph. The spoken
corpus contains broadcast news from a variety of networks (CNN, ABC, CSPAN and NPR)
which have been manually transcribed and segmented at the story and sentence level. Both
corpora have been split into training, development and testing sets6 randomly on article
boundaries (with each set containing full stories) and are publicly available from http:
//homepages.inf.ed.ac.uk/s0460084/data/.

4.2 Parameter Estimation

In this work we present three compression models ranging from unsupervised to semi-
supervised, and fully supervised. The unsupervised model simply relies on a trigram lan-
guage model for driving compression (see Section 3.4.1). This was estimated from 25 mil-
lion tokens of the North American corpus using the CMU-Cambridge Language Modeling
Toolkit (Clarkson & Rosenfeld, 1997) with a vocabulary size of 50,000 tokens and Good-
Turing discounting. To discourage one-word output we force the ILP to generate compres-
sions whose length is no less than 40% of the source sentence (see the constraint in (9)).
The semi-supervised model is the weighted combination of a word-based signiﬁcance score
with a language model (see Section 3.4.2). The signiﬁcance score was calculated using
25 million tokens from the American News Text corpus. We optimized its weight (see
Equation (11)) on a small subset of the training data (three documents in each case) us-
ing Powell’s method (Press, Teukolsky, Vetterling, & Flannery, 1992) and a loss function
based on the F-score of the grammatical relations found in the gold standard compression
and the system’s best compression (see Section 4.3 for details). The optimal weight was
approximately 1.8 for the written corpus and 2.2 for the spoken corpus.

McDonald’s (2006) supervised model was trained on the written and spoken training
sets. Our implementation used the same feature sets as McDonald, the only diﬀerence
being that our phrase structure and dependency features were extracted from the output of
Roark’s (2001) parser. McDonald uses Charniak’s (2000) parser which performs comparably.
The model was learnt using k-best compressions. On the development data, we found that
k = 10 provided the best performance.

4.3 Evaluation

Previous studies have relied almost exclusively on human judgments for assessing the well-
formedness of automatically derived compressions. These are typically rated by naive sub-
jects on two dimensions, grammaticality and importance (Knight & Marcu, 2002). Although
automatic evaluation measures have been proposed (Riezler et al., 2003; Bangalore, Ram-
bow, & Whittaker, 2000) their use is less widespread, we suspect due to the small size of
the test portion of the Ziﬀ-Davis corpus which is commonly used in compression work.

We evaluate the output of our models in two ways. First, we present results using
an automatic evaluation measure put forward by Riezler et al. (2003). They compare
the grammatical relations found in the system compressions against those found in a gold
standard. This allows us to measure the semantic aspects of summarization quality in terms
of grammatical-functional information and can be quantiﬁed using F-score. Furthermore,

6. The splits are 908/63/462 sentences for the written corpus and 882/78/410 sentences for the spoken

corpus.

420


Global Inference for Sentence Compression

in Clarke and Lapata (2006) we show that relations-based F-score correlates reliably with
human judgments on compression output. Since our test corpora are larger than Ziﬀ-
Davis (by more than a factor of ten), diﬀerences among systems can be highlighted using
signiﬁcance testing.

Our implementation of the F-score measure used the grammatical relations annotations
provided by RASP (Briscoe & Carroll, 2002). This parser is particularly appropriate for the
compression task since it provides parses for both full sentences and sentence fragments and
is generally robust enough to analyze semi-grammatical sentences. We calculated F-score
over all the relations provided by RASP (e.g., subject, direct/indirect object, modiﬁer; 15
in total).

In line with previous work we also evaluate our models by eliciting human judgments.
Following the work of Knight and Marcu (2002), we conducted two separate experiments.
In the ﬁrst experiment participants were presented with a source sentence and its target
compression and asked to rate how well the compression preserved the most important
information from the source sentence. In the second experiment, they were asked to rate
the grammaticality of the compressed outputs. In both cases they used a ﬁve point rating
scale where a high number indicates better performance. We randomly selected 21 sentences
from the test portion of each corpus. These sentences were compressed automatically by
the three models presented in this paper with and without constraints. We also included
gold standard compressions. Our materials thus consisted of 294 (21 × 2 × 7) source-
target sentences. A Latin square design ensured that subjects did not see two diﬀerent
compressions of the same sentence. We collected ratings from 42 unpaid volunteers, all self
reported native English speakers. Both studies were conducted over the Internet using a
custom build web interface. Examples of our experimental items are given in Table 3.

5. Results

Let us ﬁrst discuss our results when compression output is evaluated in terms of F-score.
Tables 4 and 5 illustrate the performance of our models on the written and spoken corpora,
respectively. We also present the compression rate7 for each system.
In all cases the
constraint-based models (+Constr) yield better F-scores than the non-constrained ones.
The diﬀerence is starker for the semi-supervised model (Sig). The constraints bring an
improvement of 17.2% on the written corpus and 18.3% on the spoken corpus. We further
examined whether performance diﬀerences among models are statistically signiﬁcant, using
the Wilcoxon test. On the written corpus all constraint models signiﬁcantly outperform the
models without constraints. The same tendency is observed on the spoken corpus except for
the model of McDonald (2006) which performs comparably with and without constraints.
We also wanted to establish which is the best constraint model. On both corpora we
ﬁnd that the language model performs worst, whereas the signiﬁcance model and McDonald
perform comparably (i.e., the F-score diﬀerences are not statistically signiﬁcant). To get
a feeling for the diﬃculty of the task, we calculated how much our annotators agreed in
their compression output. The inter-annotator agreement (F-score) on the written corpus
was 65.8% and on the spoken corpus 73.4%. The agreement is higher on spoken texts since
they consists of many short utterances (e.g., Okay, That’s it for now, Good night) that can

7. The term refers to the percentage of words retained from the source sentence in the compression.

421


Clarke & Lapata

Source

The aim is to give councils some control over the future growth of second
homes.
The aim is to give councils control over the growth of homes.
The aim is to the future.

Gold
LM
LM+Constr The aim is to give councils control.
Sig
The aim is to give councils control over the future growth of homes.
Sig+Constr The aim is to give councils control over the future growth of homes.
McD
McD+Constr The aim is to give councils some control over the growth of homes.
Source

The aim is to give councils.

The Clinton administration recently unveiled a new means to encourage
brownﬁelds redevelopment in the form of a tax incentive proposal.
The Clinton administration unveiled a new means to encourage brown-
ﬁelds redevelopment in a tax incentive proposal.
The Clinton administration in the form of tax.

LM
LM+Constr The Clinton administration unveiled a means to encourage redevelop-

Gold

Sig

ment in the form.
The Clinton administration unveiled a encourage brownﬁelds redevelop-
ment form tax proposal.

Sig+Constr The Clinton administration unveiled a means to encourage brownﬁelds

McD

redevelopment in the form of tax proposal.
The Clinton unveiled a means to encourage brownﬁelds redevelopment
in a tax incentive proposal.

McD+Constr The Clinton administration unveiled a means to encourage brownﬁelds

redevelopment in the form of a incentive proposal.

Table 3: Example compressions produced by our systems (Source: source sentence, Gold:
gold-standard compression, LM: language model compression, LM+Constr: lan-
guage model compression with constraints, Sig: signiﬁcance model, Sig+Constr:
signiﬁcance model with constraints, McD: McDonald’s (2006) compression model,
McD+Constr: McDonald’s (2006) compression model with constraints).

be compressed only very little or not all. Note that there is a marked diﬀerence between the
automatic and human compressions. Our best performing systems are inferior to human
output by more than 20 F-score percentage points.

Diﬀerences between the automatic systems and the human output are also observed
with respect to the compression rate. As can be seen the language model compresses most
aggressively, whereas the signiﬁcance model and McDonald tend to be more conservative
and closer to the gold standard. Interestingly, the constraints do not necessarily increase
the compression rate. The latter increases for the signiﬁcance model but decreases for
the language model and remains relatively constant for McDonald. It is straightforward to
impose the same compression rate for all constraint-based models (e.g., by forcing the model
n
i=1 δi = b). However, we refrained from doing this since we wanted our
to retain b tokens

P

422


Global Inference for Sentence Compression

Models
LM
Sig
McD
LM+Constr
Sig+Constr
McD+Constr
Gold

CompR F-score
18.4
23.3
36.0
28.2∗
40.5∗†
40.8∗†
—

46.2
60.6
60.1
41.2
72.0
63.7
70.3

Table 4: Results on the written corpus; compression rate (CompR) and grammatical re-
lation F-score (F-score); ∗: +Constr model is signiﬁcantly diﬀerent from model
without constraints; †: signiﬁcantly diﬀerent from LM+Constr.

Models
LM
Sig
McD
LM+Constr
Sig+Constr
McD+Constr
Gold

CompR F-score
25.4
30.4
47.6
34.8∗
48.7∗†
50.1†
—

52.0
60.9
68.6
49.5
78.4
68.5
76.1

Table 5: Results on the spoken corpus; compression rate (CompR) and grammatical rela-
tion F-score (F-score); ∗: +Constr model is signiﬁcantly diﬀerent from without
constraints; †: signiﬁcantly diﬀerent from LM+Constr.

models to regulate the compression rate for each sentence individually according to its
speciﬁc information content and structure.

We next consider the results of our human study which assesses in more detail the quality
of the generated compressions on two dimensions, namely grammaticality and information
content. F-score conﬂates these two dimensions and therefore in theory could unduly reward
a system that produces perfectly grammatical output without any information loss. Tables 6
and 7 show the mean ratings8 for each system (and the gold standard) on the written and
spoken corpora, respectively. We ﬁrst performed an Analysis of Variance (Anova) to
examine the eﬀect of diﬀerent system compressions. The Anova revealed a reliable eﬀect
on both grammaticality and importance for each corpus (the eﬀect was signiﬁcant by both
subjects and items (p < 0.01)).

We next examine the impact of the constraints (+Constr in the tables). In most cases
we observe an increase in ratings for both grammaticality and importance when a model
is supplemented constraints. Post-hoc Tukey tests reveal that the grammaticality and
importance ratings of the language model and signiﬁcance model signiﬁcantly improve with

8. All statistical tests reported subsequently were done using the mean ratings.

423


Clarke & Lapata

Models

LM
Sig
McD

LM+Constr
Sig+Constr
McD+Constr
Gold

Grammar
2.25†$
2.26†$
3.05†
3.47∗†
3.76∗
3.50†
4.25

Importance
1.82†$
2.99†$
2.84†
2.37∗†$
3.53∗
3.17†
3.98

Table 6: Results on the written text corpus; average grammaticality score (Grammar) and
average importance score (Importance) for human judgments; ∗: +Constr model
is signiﬁcantly diﬀerent from model without constraints; †: signiﬁcantly diﬀerent
from gold standard; $: signiﬁcantly diﬀerent from McD+Constr.

Models

LM
Sig
McD

LM+Constr
Sig+Constr
McD+Constr
Gold

Grammar
2.20†$
2.29†$
3.33†
3.18∗†
3.80∗†
3.60†
4.45

Importance
1.56†
2.64†
3.32†
2.49∗†$
3.69∗†
3.31†
4.25

Table 7: Results on the spoken text corpus; average grammaticality score (Grammar) and
average importance score (Importance) for human judgments; ∗: +Constr model
is signiﬁcantly diﬀerent from model without constraints; †: signiﬁcantly diﬀerent
from gold standard; $: signiﬁcantly diﬀerent from McD+Constr.

the constraints (α < 0.01). In contrast, McDonald’s system sees a numerical improvement
with the additional constraints, but this diﬀerence is not statistically signiﬁcant. These
tendencies are observed on the spoken and written corpus.

Upon closer inspection, we can see that the constraints inﬂuence considerably the
grammaticality of the unsupervised and semi-supervised systems. Tukey tests reveal that
LM+Constr and Sig+Constr are as grammatical as McD+Constr. In terms of importance,
Sig+Constr and McD+Constr are signiﬁcantly better than LM+Constr (α < 0.01). This
is not surprising given that LM+Constr is a very simple model without a mechanism for
highlighting important words in a sentence.
Interestingly, Sig+Constr performs as well
as McD+Constr in retaining the most important words, despite the fact that it requires
minimal supervision. Although constraint-based models overall perform better than mod-
els without constraints, they receive lower ratings (for grammaticality and importance) in
comparison to the gold standard. And the diﬀerences are signiﬁcant in most cases.

424


Global Inference for Sentence Compression

In summary, we observe that the constraints boost performance. This is more pro-
nounced for compression models that are either unsupervised or use small amounts of
parallel data. For example, a simple model like Sig yields performance comparable to
McDonald (2006) when constraints are taken into account. This is an encouraging result
suggesting that ILP can be used to create good compression models with relatively little
eﬀort (i.e., without extensive feature engineering or elaborate knowledge sources). Per-
formance gains are also obtained for competitive models like McDonald’s that are fully
supervised. But these gains are smaller, presumably because the initial model contains a
rich feature representation consisting of syntactic information and generally does a good job
at producing grammatical output. Finally, our improvements are consistent across corpora
and evaluation paradigms.</corps>
		<conclusion>6. Conclusions

In this paper we have presented a novel method for automatic sentence compression. A key
aspect of our approach is the use of integer linear programming for inferring globally optimal
compressions in the presence of linguistically motivated constraints. We have shown how
previous formulations of sentence compression can be recast as ILPs and extended these
models with local and global constraints ensuring that the compressed output is structurally
and semantic well-formed. Contrary to previous work that has employed ILP solely for
decoding, our models integrate learning with inference in a uniﬁed framework.

Our experiments have demonstrated the advantages of the approach. Constraint-based
models consistently bring performance gains over models without constraints. These im-
provements are more impressive for models that require little or no supervision. A case
in point here is the signiﬁcance model discussed above. The no-constraints incarnation of
this model performs poorly and considerably worse than McDonald’s (2006) state-of-the-art
model. The addition of constraints improves the output of this model so that its perfor-
mance is indistinguishable from McDonald. Note that the signiﬁcance model requires a
small amount of training data (50 parallel sentences), whereas McDonald is trained on hun-
dreds of sentences. It also presupposes little feature engineering, whereas McDonald utilizes
thousands of features. Some eﬀort is associated with framing the constraints, however these
are created once and are applied across models and corpora. We have also observed small
performance gains for McDonald’s system when the latter is supplemented with constraints.
Larger improvements are possible with more sophisticated constraints, however our intent
was to devise a set of general constraints that are not tuned to the mistakes of any speciﬁc
system in particular.

Future improvements are many and varied. An obvious extension concerns our con-
straint set. Currently our constraints are mostly syntactic and consider each sentence in
isolation. By incorporating discourse constraints we could highlight words that are impor-
tant at the document-level. Presumably words topical in a document should be retained in
the compression. Other constraints could manipulate the compression rate. For example,
we could encourage a higher compression rate for longer sentences. Another interesting
direction includes the development of better objective functions for the compression task.
The objective functions presented so far rely on ﬁrst or second-order Markov assumptions.
Alternative objectives could take into account the structural similarity between the source

425


Clarke & Lapata

sentence and its target compression; or whether they share the same content which could
be operationalized in terms of entropy.

Beyond the task and systems presented in this paper, we believe the approach holds
promise for other generation applications using decoding algorithms for searching the space
of possible outcomes. Examples include sentence-level paraphrasing, headline generation,
and summarization.

Acknowledgments

We are grateful to our annotators Vasilis Karaiskos, Beata Kouchnir, and Sarah Luger.
Thanks to Jean Carletta, Frank Keller, Steve Renals, and Sebastian Riedel for helpful
comments and suggestions and to the anonymous referees whose feedback helped to sub-
stantially improve the present paper. Lapata acknowledges the support of EPSRC (grant
GR/T04540/01). A preliminary version of this work was published in the proceedings of
ACL 2006.</conclusion>
		<discussion>Aucune discussion trouvée.</discussion>
		<biblio>Aho, A. V., & Ullman, J. D. (1969). Syntax directed translations and the pushdown assem-

bler. Journal of Computer and System Sciences, 3, 37–56.

Bangalore, S., Rambow, O., & Whittaker, S. (2000). Evaluation metrics for generation.
In Proceedings of the ﬁrst International Conference on Natural Language Generation,
pp. 1–8, Mitzpe Ramon, Israel.

Barzilay, R., & Lapata, M. (2006). Aggregation via set partitioning for natural language
In Proceedings of the Human Language Technology Conference of the
generation.
North American Chapter of the Association for Computational Linguistics, pp. 359–
366, New York, NY, USA.

Bramsen, P., Deshpande, P., Lee, Y. K., & Barzilay, R. (2006). Inducing temporal graphs.
In Proceedings of the 2006 Conference on Empirical Methods in Natural Language
Processing, pp. 189–198, Sydney, Australia.

Briscoe, E. J., & Carroll, J. (2002). Robust accurate statistical annotation of general text. In
Proceedings of the Third International Conference on Language Resources and Eval-
uation, pp. 1499–1504, Las Palmas, Gran Canaria.

Charniak, E. (2000). A maximum-entropy-inspired parser. In Proceedings of the 1st North
American Annual Meeting of the Association for Computational Linguistics, pp. 132–
139, Seattle, WA, USA.

Clarke, J., & Lapata, M. (2006). Models for sentence compression: A comparison across
domains, training requirements and evaluation measures. In Proceedings of the 21st
International Conference on Computational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, pp. 377–384, Sydney, Australia.

Clarkson, P., & Rosenfeld, R. (1997). Statistical language modeling using the CMU–
Cambridge toolkit. In Proceedings of Eurospeech’97, pp. 2707–2710, Rhodes, Greece.

426


Global Inference for Sentence Compression

Cormen, T. H., Leiserson, C. E., & Rivest, R. L. (1992). Intoduction to Algorithms. The

MIT Press.

Corston-Oliver, S. (2001). Text Compaction for Display on Very Small Screens. In Proceed-
ings of the Workshop on Automatic Summarization at the 2nd Meeting of the North
American Chapter of the Association for Computational Linguistics, pp. 89–98, Pitts-
burgh, PA, USA.

Crammer, K., & Singer, Y. (2003). Ultraconservative online algorithms for multiclass prob-

lems. Journal of Machine Learning Research, 3, 951–991.

Dantzig, G. B. (1963). Linear Programming and Extensions. Princeton University Press,

Princeton, NJ, USA.

Denis, P., & Baldridge, J. (2007). Joint determination of anaphoricity and coreference
resolution using integer programming. In Human Language Technologies 2007: The
Conference of the North American Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pp. 236–243, Rochester, NY.

Dras, M. (1999). Tree Adjoining Grammar and the Reluctant Paraphrasing of Text. Ph.D.

thesis, Macquarie University.

Galley, M., & McKeown, K. (2007). Lexicalized markov grammars for sentence compression.
In In Proceedings of the North American Chapter of the Association for Computational
Linguistics, pp. 180–187, Rochester, NY, USA.

Gomory, R. E. (1960). Solving linear programming problems in integers.

In Bellman,
R., & Hall, M. (Eds.), Combinatorial analysis, Proceedings of Symposia in Applied
Mathematics, Vol. 10, Providence, RI, USA.

Grefenstette, G. (1998). Producing Intelligent Telegraphic Text Reduction to Provide an
Audio Scanning Service for the Blind. In Hovy, E., & Radev, D. R. (Eds.), Proceedings
of the AAAI Symposium on Intelligent Text Summarization, pp. 111–117, Stanford,
CA, USA.

Hori, C., & Furui, S. (2004). Speech summarization: an approach through word extraction
and a method for evaluation. IEICE Transactions on Information and Systems, E87-
D (1), 15–25.

Jing, H. (2000). Sentence reduction for automatic text summarization. In Proceedings of
the 6th Applied Natural Language Processing Conference, pp. 310–315, Seattle,WA,
USA.

Knight, K., & Marcu, D. (2002). Summarization beyond sentence extraction: a probabilistic

approach to sentence compression. Artiﬁcial Intelligence, 139 (1), 91–107.

Land, A. H., & Doig, A. G. (1960). An automatic method for solving discrete programming

problems. Econometrica, 28, 497–520.

Lin, C.-Y. (2003). Improving summarization performance by sentence compression — a pilot
study. In Proceedings of the 6th International Workshop on Information Retrieval with
Asian Languages, pp. 1–8, Sapporo, Japan.

Lin, D. (2001). LaTaT: Language and text analysis tools. In Proceedings of the ﬁrst Human

Language Technology Conference, pp. 222–227, San Francisco, CA, USA.

427


Clarke & Lapata

Marciniak, T., & Strube, M. (2005). Beyond the pipeline: Discrete optimization in NLP. In
Proceedings of the Ninth Conference on Computational Natural Language Learning,
pp. 136–143, Ann Arbor, MI, USA.

McDonald, R. (2006). Discriminative sentence compression with soft syntactic constraints.
In Proceedings of the 11th Conference of the European Chapter of the Association for
Computational Linguistics, Trento, Italy.

McDonald, R., Crammer, K., & Pereira, F. (2005a). Flexible text segmentation with struc-
tured multilabel classiﬁcation. In Proceedings of Human Language Technology Con-
ference and Conference on Empirical Methods in Natural Language Processing, pp.
987–994, Vancouver, BC, Canada.

McDonald, R., Crammer, K., & Pereira, F. (2005b). Online large-margin training of de-
In 43rd Annual Meeting of the Association for Computational

pendency parsers.
Linguistics, pp. 91–98, Ann Arbor, MI, USA.

Nemhauser, G. L., & Wolsey, L. A. (1988). Integer and Combinatorial Optimization. Wiley-
Interscience series in discrete mathematicals and opitmization. Wiley, New York, NY,
USA.

Nguyen, M. L., Shimazu, A., Horiguchi, S., Ho, T. B., & Fukushi, M. (2004). Probabilistic
sentence reduction using support vector machines. In Proceedings of the 20th inter-
national conference on Computational Linguistics, pp. 743–749, Geneva, Switzerland.

Press, W. H., Teukolsky, S. A., Vetterling, W. T., & Flannery, B. P. (1992). Numerical
Recipes in C: The Art of Scientiﬁc Computing. Cambridge University Press, New
York, NY, USA.

Punyakanok, V., Roth, D., Yih, W., & Zimak, D. (2004). Semantic role labeling via inte-
ger linear programming inference. In Proceedings of the International Conference on
Computational Linguistics, pp. 1346–1352, Geneva, Switzerland.

Riedel, S., & Clarke, J. (2006). Incremental integer linear programming for non-projective
dependency parsing. In Proceedings of the 2006 Conference on Empirical Methods in
Natural Language Processing, pp. 129–137, Sydney, Australia.

Riezler, S., King, T. H., Crouch, R., & Zaenen, A. (2003). Statistical sentence condensation
using ambiguity packing and stochastic disambiguation methods for lexical-functional
In Human Language Technology Conference and the 3rd Meeting of the
grammar.
North American Chapter of the Association for Computational Linguistics, pp. 118–
125, Edmonton, Canada.

Roark, B. (2001). Probabilistic top-down parsing and language modeling. Computational

Linguistics, 27 (2), 249–276.

Roth, D. (1998). Learning to resolve natural language ambiguities: A uniﬁed approach. In
In Proceedings of the 15th of the American Association for Artiﬁcial Intelligence, pp.
806–813, Madison, WI, USA.

Roth, D., & Yih, W. (2004). A linear programming formulation for global inference in
natural language tasks. In Proceedings of the Annual Conference on Computational
Natural Language Learning, pp. 1–8, Boston, MA, USA.

428


Global Inference for Sentence Compression

Roth, D., & Yih, W. (2005). Integer linear programming inference for conditional random
ﬁelds. In Proceedings of the International Conference on Machine Learning, pp. 737–
744, Bonn.

Sarawagi, S., & Cohen, W. W. (2004). Semi-markov conditional random ﬁelds for informa-
tion extraction. In Advances in Neural Information Processing Systems, Vancouver,
BC, Canada.

Shieber, S., & Schabes, Y. (1990). Synchronous tree-adjoining grammars.

In Proceed-
ings of the 13th International Conference on Computational Linguistics, pp. 253–258,
Helsinki, Finland.

Turner, J., & Charniak, E. (2005). Supervised and unsupervised learning for sentence
compression. In Proceedings of the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pp. 290–297, Ann Arbor, MI, USA.

Vandeghinste, V., & Pan, Y. (2004). Sentence compression for automated subtitling: A
hybrid approach. In Marie-Francine Moens, S. S. (Ed.), Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop, pp. 89–95, Barcelona, Spain.

Williams, H. P. (1999). Model Building in Mathematical Programming (4th edition). Wiley.

Winston, W. L., & Venkataramanan, M. (2003). Introduction to Mathematical Program-

ming: Applications and Algorithms (4th edition). Duxbury.

Zajic, D., Door, B. J., Lin, J., & Schwartz, R. (2007). Multi-candidate reduction: Sentence
Information Processing

compression as a tool for document summarization tasks.
Management Special Issue on Summarization, 43 (6), 1549–1570.

429</biblio>
	</article>
	<article>
		<preamble>hybrid_approach.txt</preamble>
		<titre>Sentence Compression for Automated Subtitling: A Hybrid Approach</titre>
		<auteur>Vincent Vandeghinste and Yi Pan</auteur>
		<abstract>In this paper a sentence compression tool is de- scribed. We describe how an input sentence gets analysed by using a.o. a tagger, a shallow parser and a subordinate clause detector, and how, based on this analysis, several compressed versions of this sentence are generated, each with an associated es- timated probability. These probabilities were esti- mated from a parallel transcript/subtitle corpus. To avoid ungrammatical sentences, the tool also makes use of a number of rules. The evaluation was done on three different pronunciation speeds, averaging sentence reduction rates of 40% to 17%. The num- ber of reasonable reductions ranges between 32.9% and 51%, depending on the average estimated pro- nunciation speed.</abstract>
		<introduction>A sentence compression tool has been built with the purpose of automating subtitle generation for the deaf and hard-of-hearing. Verbatim transcrip- tions cannot be presented as the subtitle presentation time is between 690 and 780 characters per minute, which is more or less 5.5 seconds for two lines (ITC, 1997), (Dewulf and Saerens, 2000), while the aver- age speech rate contains a lot more than the equiva- lent of 780 characters per minute. The actual amount of compression needed de- pends on the speed of the speaker and on the amount of time available after the sentence. In documen- taries, for instance, there are often large silent in- tervals between two sentences, the speech is often slower and the speaker is off-screen, so the avail- able presentation time is longer. When the speaker is off-screen, the synchrony of the subtitles with the speech is of minor importance. When subti- tling the news the speech rate is often very high so the amount of reduction needed to allow the synchronous presentation of subtitles and speech is much greater. The sentence compression rate is a parameter which can be set for each sentence. Note that the sentence compression tool de- scribed in this paper is not a subtitling tool. When subtitling, only when a sentence needs to be re- duced, and the amount of reduction is known, the sentence is sent to the sentence compression tool. So the sentence compression tool is a module of an automated subtitling tool. The output of the sen- tence compression tool needs to be processed ac- cording to the subtitling guidelines like (Dewulf and Saerens, 2000), in order to be in the correct lay-out which makes it usable for actual subtitling. Manu- ally post-editing the subtitles will still be required, as for some sentences no automatic compression is generated. In real subtitling it often occurs that the sentences are not compressed, but to keep the subtitles syn- chronized with the speech, some sentences are en- tirely removed. In section 2 we describe the processing of a sen- tence in the sentence compressor, from input to out- put. In section 3 we describe how the system was evaluated and the results of the evaluation. Section</introduction>
		<corps>evaluated and the results of the evaluation. Section
4 contains the conclusions.

2 From Full Sentence to Compressed

Sentence

The sentence compression tool is inspired by (Jing,
2001). Although her goal is text summarization
and not subtitling, her sentence compression system
could serve this purpose.

She uses multiple sources of knowledge on which
her sentence reduction is based. She makes use of
a corpus of sentences, aligned with human-written
sentence reductions which is similar to the parallel
corpus we use (Vandeghinste and Tjong Kim Sang,
2004). She applies a syntactic parser to analyse the
syntactic structure of the input sentences. As there
was no syntactic parser available for Dutch (Daele-
mans and Strik, 2002), we created ShaRPA (Van-
deghinste, submitted), a shallow rule-based parser
which could give us a shallow parse tree of the
Jing uses several other knowl-
input sentence.
edge sources, which are not used (not available for
Dutch) or not yet used in our system (like WordNet).


In ﬁgure 1 the processing ﬂow of an input sen-

tence is sketched.

Input Sentence

Tagger

Abbreviator

Numbers to Digits

Chunker

Subordinate Clause Detector

Shallow Parse
Tree

Compressor

Grammar
Rules

Removal,
Non−removal,
Reduction
Database

Compressed
Sentence

Word Reducer

Figure 1: Sentence Processing Flow Chart

First we describe how the sentence is analysed
(2.1), then we describe how the actual sentence
compression is done (2.2), and after that we de-
scribe how words can be reduced for extra compres-
sion (2.3). The ﬁnal part describes the selection of
the ouput sentence (2.4).

2.1 Sentence Analysis
In order to apply an accurate sentence compression,
we need a syntactic analysis of the input sentence.

In a ﬁrst step, the input sentence gets tagged for
parts-of-speech. Before that, it needs to be trans-
formed into a valid input format for the part-of-
speech tagger. The tagger we use is TnT (Brants,
2000) , a hidden Markov trigram tagger, which was
trained on the Spoken Dutch Corpus (CGN), Inter-
nal Release 6. The accuracy of TnT trained on CGN
is reported to be 96.2% (Oostdijk et al., 2002).

In a second step,

the sentence is sent to the
This tool connects to a database
Abbreviator.
of common abbreviations, which are often pro-
nounced in full words (E.g. European Union be-

comes EU) and replaces the full form with its ab-
breviation. The database can also contain the tag
of the abbreviated part (E.g.
the tag for EU is
N(eigen,zijd,ev,basis,stan) [E: singular non-neuter
proper noun]).

In a third step, all numbers which are written in
words in the input are replaced by their form in dig-
its. This is done for all numbers which are smaller
than one million, both for cardinal and ordinal nu-
merals.

In a fourth step, the sentence is sent to ShaRPa,
which will result in a shallow parse-tree of the sen-
tence. The chunking accuracy for noun phrases
(NPs) has an F-value of 94.7%, while the chunk-
ing accuracy of prepositional phrases (PPs) has an
F-value of 95.1% (Vandeghinste, submitted).

A last step before the actual sentence compres-
sion consists of rule-based clause-detection: Rel-
ative phrases (RELP), subordinate clauses (SSUB)
and OTI-phrases (OTI is om ... te + inﬁnitive1) are
detected. The accuracy of these detections was eval-
uated on 30 ﬁles from the CGN component of read-
aloud books, which contained 7880 words. The
evaluation results are presented in table 1.

Type of S Precision
OTI
RELP
SSUB

F-value
Recall
71.43% 65.22% 68.18%
69.66% 68.89% 69.27%
56.83% 60.77% 58.74%

Table 1: Clause Detection Accuracy

The errors are mainly due to a wrong analysis
of coordinating conjunctions, which is not only the
weak point in the clause-detection module, but also
in ShaRPa. A full parse is needed to accurately
solve this problem.

2.2 Sentence Compression

For each chunk or clause detected in the previous
steps, the probabilities of removal, non-removal and
reduction are estimated. This is described in more
detail in 2.2.1.

Besides the statistical component in the compres-
sion, there are also a number of rules in the com-
pression program, which are described in more de-
tail in 2.2.2.

The way the statistical component and the rule-
based component are combined is described in
2.2.3.

1There is no equivalent construction in English. OTI is a

VP-selecting complementizer.


2.2.1 Use of Statistics
Chunk and clause removal, non-removal and reduc-
tion probabilities are estimated from the frequencies
of removal, non-removal and reduction of certain
types of chunks and clauses in the parallel corpus.

The parallel corpus consists of transcripts of tele-
vision programs on the one hand and the subti-
tles of these television programs on the other hand.
A detailed description of how the parallel corpus
was collected, and how the sentences and chunks
were aligned is given in (Vandeghinste and Tjong
Kim Sang, 2004).

All sentences in the source corpus (transcripts)
and the target corpus (subtitles) are analysed in the
same way as described in section 2.1, and are chunk
aligned. The chunk alignment accuracy is about
95% (F-value).

We estimated the removal, non-removal and re-
duction probabilities for the chunks of the types NP,
PP, adjectival phrase (AP), SSUB, RELP, and OTI,
based on their chunk removal, non-removal and re-
duction frequencies.

For the tokens not belonging to either of these
types, the removal and non-removal probabilities
were estimated based on the part-of-speech tag for
those words. A reduced tagset was used, as the orig-
inal CGN-tagset (Van Eynde, 2004) was too ﬁne-
grained and would lead to a multiplication of the
number of rules which are now used in ShaRPa. The
ﬁrst step in SharPa consists of this reduction.

For the PPs, the SSUBs and the RELPs, as well
as for the adverbs, the chunk/tag information was
considered as not ﬁne-grained enough, so the es-
timation of the removal, non-removal and reduc-
tion probabilities for these types are based on the
ﬁrst word of those phrases/clauses and the reduc-
tion, removal and non-removal probabilities of such
phrases in the parallel corpus, as the ﬁrst words of
these chunk-types are almost always the heads of
the chunk. This allows for instance to make the
distinction between several adverbs in one sentence,
so they do not all have the same removal and non-
removal probabilities. A disadvantage is that this
approach leads to sparse data concerning the less
frequent adverbs, for which a default value (average
over all adverbs) will be employed.

An example

: A noun phrase.

de grootste Belgische bank
[E: the largest Belgian bank]

After tagging and chunking the sentence and af-
ter detecting subordinate clauses, for every non-
terminal node in the shallow parse tree we retrieve
the measure of removal (X), of non-removal (=) and

of reduction2 ( (cid:0)
). For the terminal nodes, only the
measures of removal and of non-removal are used.

NP
= 0.54
X 0.27
(cid:1) 0.05

DET
= 0.68
X 0.28
de

ADJ
= 0.56
X 0.35
groot-
ste

ADJ
= 0.56
X 0.35
Bel-
gische

N
= 0.65
X 0.26
bank

For every combination the probability estimate
is calculated. So if we generate all possible com-
pressions (including no compression), the phrase
de grootste Belgische bank will get the
probability estimate (cid:2)(cid:4)(cid:3)(cid:6)(cid:5)(cid:8)(cid:7)(cid:10)(cid:9)(cid:12)(cid:11)(cid:13)(cid:2)(cid:4)(cid:3)(cid:15)(cid:14)(cid:17)(cid:16)(cid:18)(cid:9)(cid:19)(cid:2)(cid:20)(cid:3)(cid:21)(cid:5)(cid:22)(cid:14)(cid:18)(cid:9)(cid:23)(cid:2)(cid:20)(cid:3)(cid:21)(cid:5)(cid:22)(cid:14)(cid:18)(cid:9)
(cid:2)(cid:4)(cid:3)(cid:15)(cid:14)(cid:24)(cid:5)(cid:26)(cid:25)(cid:28)(cid:27)(cid:29)(cid:2)(cid:4)(cid:3)(cid:15)(cid:2)(cid:24)(cid:30)(cid:31)(cid:7) (cid:16)(cid:24)(cid:5) . For the phrase de Belgische
bank the probability estimate is (cid:2)(cid:20)(cid:3)(cid:6)(cid:2)(cid:17)(cid:5)!(cid:9)(cid:29)(cid:11)(cid:13)(cid:2)(cid:4)(cid:3)(cid:15)(cid:14)(cid:17)(cid:16)"(cid:9)
(cid:2)(cid:4)(cid:3)(cid:15)#(cid:24)(cid:5)$(cid:9)(cid:23)(cid:2)(cid:4)(cid:3)(cid:6)(cid:5)(cid:26)(cid:14)$(cid:9)(cid:23)(cid:2)(cid:4)(cid:3)(cid:6)(cid:14)(cid:17)(cid:5)(cid:17)(cid:25)%(cid:27)&(cid:2)(cid:20)(cid:3)(cid:6)(cid:2)(cid:26)(cid:2)(cid:22)(cid:7) #(cid:26)# , and so on for the
other alternatives.

In this way, the probability estimate of all possi-

ble alternatives is calculated.

2.2.2 Use of Rules
As the statistical information allows the generation
of ungrammatical sentences, a number of rules were
added to avoid generating such sentences. The pro-
cedure keeps the necessary tokens for each kind of
node. The rules were built in a bootstrapping man-
ner

In some of these rules, this procedure is applied
recursively. These are the rules implemented in our
system:

If a node is of type SSUB or RELP, keep the
ﬁrst word.

If a node is of type S, SSUB or RELP, keep

– the verbs. If there are prepositions which
are particles of the verb, keep the prepo-
sitions. If there is a prepositional phrase
which has a preposition which is in the
complements list of the verb, keep the
necessary tokens3 of that prepositional
phrase.

2These measures are estimated probabilities and do not need
to add up to 1, because in the parallel training corpus, some-
times a match was detected with a chunk which was not a re-
duction of the source chunk or which was not identical to the
source chunk: the chunk could be paraphrased, or even have
become longer.

3Recursive use of the rules

’
’

– each token which is in the list of nega-
tive words. These words are kept to avoid
altering the meaning of the sentence by
dropping words which negate the mean-
ing.

– the necessary tokens of the te + inﬁnitives

(TI).

– the conjunctions.

– the necessary tokens of each NP.

– the numerals.

– the adverbially used adjectives.

If a node is of type NP, keep

– each noun.

– each nominalised adjectival phrase.

– each token which is in the list of negative

words.

– the determiners.

– the numerals.

– the indeﬁnite prenominal pronouns.

If a node is of type PP, keep

– the preposition.

– the determiners.

– the necessary tokens of the NPs.

If the node is of type adjectival phrase, keep

– the head of the adjectival phrase.

– the prenominal numerals.

– each word which is in the list of negative

words.

If the node is of type OTI, keep

– the verbs.

– the te + inﬁnitives.

If the node is of type TI, keep the node.
If the node is a time phrase4, keep it.

These rules are chosen because in tests on earlier
versions of the system, using a different test set, un-
grammatical output was generated. By using these
rules the output should be grammatical, provided
that the input sentence was analysed correctly.

4A time phrase, as deﬁned in ShaRPa is used for special
phrases, like dates, times, etc. E.g. 27 september 1998, kwart
voor drie [E: quarter to three].

2.2.3 Combining Statistics and Rules
In the current version of the system, in a ﬁrst stage
all variations on a sentence are generated in the sta-
tistical part, and they are ranked according to their
In a second stage, all ungrammatical
probability.
sentences are (or should be) ﬁltered out, so the only
sentence alternatives which remain should be gram-
matical ones.

This is true, only if tagging as well as chunking
were correct. If errors are made on these levels, the
generation of an ungrammatical alternative is still
possible.

For efﬁciency reasons, a future version of the sys-
tem should combine the rules and statistics in one
stage, so that the statistical module only generates
grammatically valid sentence alternatives, although
there is no effect on correctness, as the resulting sen-
tence alternatives would be the same if statistics and
rules were better integrated.

2.3 Word Reduction

After the generation of several grammatical reduc-
tions, which are ordered according to their prob-
ability estimated by the product of the removal,
non-removal and reduction probabilities of all its
chunks, for every word in every compressed alterna-
tive of the sentence it is checked whether the word
can be reduced.

The words are sent to a WordSplitter-module,
which takes a word as its input and checks if it is
a compound by trying to split it up in two parts:
the modiﬁer and the head. This is done by lexicon
lookup of both parts. If this is possible, it is checked
whether the modiﬁer and the head can be recom-
pounded according to the word formation rules for
Dutch (Booij and van Santen, 1995), (Haeseryn et
al., 1997). This is done by sending the modiﬁer
and the head to a WordBuilding-module, which is
described in more detail in (Vandeghinste, 2002).
This is a hybrid module combining the compound-
ing rules with statistical information about the fre-
quency of compounds with the samen head, the fre-
quency of compounds with the same modiﬁer, and
the number of different compounds with the same
head.

Only if this module allows the recomposition of
the modiﬁer and the head, the word can be consid-
ered to be a compound, and it can potentially be re-
duced to its head, removing the modiﬁer.

If the words occur in a database which contains
a list of compounds which should not be split up,
the
the word cannot be reduced. For example,
word voetbal [E: football] can be split up and re-
compounded according to the word formation rules

’
’
’
’
’
’

for Dutch (voet [E: foot] and bal [E: ball]), but
we should not replace the word voetbal with the
word bal if we want an accurate compression, with
the same meaning as the original sentence, as this
would alter the meaning of the sentence too much.
The word voetbal has (at least) two different mean-
ings:
soccer and the ball with which soccer is
played. Reducing it to bal would only keep the sec-
ond meaning. The word gevangenisstraf [E: prison
sentence] can be split up and recompounded (gevan-
genis [E: prison] and straf [E: punishment]). We
can replace the word gevangenisstraf by the word
straf. This would still alter the meaning of the sen-
tence, but not to the same amount as it would have
been altered in the case of the word voetbal.

2.4 Selection of the Compressed Sentence
Applying all the steps described in the previous sec-
tions results in an ordered list of sentence alterna-
tives, which are supposedly grammatically correct.
When word reduction was possible, the word-
reduced alternative is inserted in this list, just after
its full-words equivalent.

The ﬁrst sentence in this list with a length smaller
than the maximal length (depending on the available
presentation time) is selected.

In a future version of the system, the word reduc-
tion information can be integrated in a better way
with the rest of the module, by combining the proba-
bility of reduction/non-reduction of a word with the
probability of the sentence alternative. The reduc-
tion probability of a word would then play its role
in the estimated probability of the compressed sen-
tence alternative containing this reduced word.

3 Evaluation
The evaluation of a sentence compression module is
not an easy task. The output of the system needs to
be judged manually for its accuracy. This is a very
time consuming task. Unlike (Jing, 2001), we do
not compare the system results with the human sen-
tence reductions. Jing reports a succes rate of 81.3%
for her program, but this measure is calculated as the
percentage of decisions on which the system agrees
with the decisions taken by the human summarizer.
This means that 81.3% of all system decisions are
correct, but does not say anything about how many
sentences are correctly reduced.

In our evaluation we do not expect the compres-
sor to simulate human summarizer behaviour. The
results presented here are calculated on the sentence
level:
the amount of valid reduced sentences, be-
ing those reductions which are judged by human
raters to be accurate reductions: grammatical sen-
tences with (more or less) the same meaning as the

input sentence, taking into account the meaning of
the previous sentences on the same topic.

3.1 Method

To estimate the available number of characters in a
subtitle, it is necessary to estimate the average pro-
nunciation time of the input sentence, provided that
it is unknown. We estimate sentence duration by
counting the number of syllables in a sentence and
multiplying this with the average duration per sylla-
ble (ASD).

The ASD for Dutch is reported to be about 177
ms (Koopmans-van Beinum and van Donzel, 1996),
which is the syllable speed without including pauses
between words or sentences.

We did some similar research on CGN using the
ASD as a unit of analysis, while we consider both
the situation without pauses and the situation with
pauses. Results of this research are presented in ta-
ble 2.

ASD
All ﬁles
One speaker
Read-aloud

no pauses
186
185
188

pauses included
237
239
256

Table 2: Average Syllable Duration (ms)

We extract the word duration from all the ﬁles
in each component of CGN. A description of the
components can be found in (Oostdijk et al., 2002).
We created a syllable counter for Dutch words,
which we evaluated on all words in the CGN lexi-
con. For 98.3% of all words in the lexicon, syllables
are counted correctly. Most errors occur in very low
frequency words or in foreign words.

By combining word duration information and the
number of syllables we can calculate the average
speaking speed.

We evaluated sentence compression in three dif-

ferent conditions:

The fastest ASD in our ASD-research was 185 ms
(one speaker, no pauses), which was used for Con-
dition A. We consider this ASD as the maximum
speed for Dutch.

The slowest ASD (256 ms) was used for Condi-
tion C. We consider this ASD to be the minimum
speed for Dutch.

We created a testset of 100 sentences mainly fo-
cused on news broadcasts in which we use the real
pronunciation time of each sentence in the testset
which results in an ASD of 192ms. This ASD was


used for Condition B, and is considered as the real
speed for news broadcasts.

We created a testset of 300 sentences, of which
200 were taken from transcripts of television news,
and 100 were taken from the ’broadcast news’ com-
ponent of CGN.

To evaluate the compressor, we estimate the du-
ration of each sentence, by counting the number of
syllables and multiplying that number with the ASD
for that condition. This leads to an estimated pro-
nunciation time. This is converted to the number of
characters, which is available for the subtitle.

We know the average time for subtitle presenta-
tion at the VRT (Flemish Broadcasting Coorpora-
tion) is 70 characters in 6 seconds, which gives us
an average of 11.67 characters per second.

So, for example, if we have a test sentence of
15 syllables, this gives us an estimated pronunci-
ation time of 2.775 seconds (15 syllables (cid:9)
185
ms/syllable) in condition A. When converting this to
the available characters, we multiply 2.775 seconds
by 11.67 characters/second, resulting in 32 (2.775s
(cid:9) 11.67 ch/s = 32.4 ch) available characters.

In condition B (considered to be real-time) for
the part of the test-sentences coming from CGN,
the pronunciation time was not estimated, as it was
available in CGN.

3.2 Results

The results of our experiments on the sentence com-
pression module are presented in table 3.

Condition
No output (0)
Avg Syllable speed
(msec/syllable)
Avg Reduction Rate
Interrater Agreement
Accurate Compr.
+/- Acc. Compr.
Reasonable Compr.

A

B
44.33% 41.67% 15.67%

C

256

185

192
39.93% 37.65% 16.93%
86.2% 86.9% 91.7%
4.8%
28.9%
8.0%
28.1% 26.3% 22.1%
51%
32.9% 34.3%

Table 3: Sentence Compression Evaluation on the
Sentence Level

The sentence compressor does not generate out-
put for all test sentences in all conditions: In those
cases where no output was generated, the sentence
compressor was not able to generate a sentence
alternative which was shorter than the maximum
number of characters available for that sentence.
The cases where no output is generated are not con-
sidered as errors because it is often impossible, even
for humans, to reduce a sentence by about 40%,

without changing the content too much. The amount
of test sentences where no output was generated
is presented in table 3. The high percentage of
sentences where no output was generated in condi-
tions A and B is most probably due to the fact that
the compression rates in these conditions are higher
than they would be in a real life application. Condi-
tion C seems to be closer to the real life compression
rate needed in subtitling.

Each condition has an average reduction rate over
the 300 test sentences. This reduction rate is based
on the available amount of characters in the subtitle
and the number of characters in the source sentence.
A rater scores a compressed sentence as + when
it is grammatically correct and semantically equiva-
lent to the input sentence. No essential information
should be missing. A sentence is scored as +/-
when it is grammatically correct, but some infor-
mation is missing, but is clear from the context in
which the sentence occurs. All other compressed
sentences get scored as -.

Each sentence is evaluated by two raters. The
lowest score of the two raters is the score which the
sentence gets. Interrater agreement is calculated on
a 2 point score: if both raters score a sentence as +
or +/- or both raters score a sentence as -, it is con-
sidered an agreed judgement. Interrater agreement
results are presented in table 3.

Sentence compression results are presented in ta-
ble 3. We consider both the + and +/- results as
reasonable compressions.

The resulting percentages of reasonable compres-
sions seem to be rather low, but one should keep
in mind that these results are based on the sentence
level. One little mistake in one sentence can lead
to an inaccurate compression, although the major
part of the decisions taken in the compression pro-
cess can still be correct. This makes it very hard
to compare our results to the results presented by
Jing (2001), but we presented our results on sen-
tence evaluations as it gives a clearer idea on how
well the system would actually perform in a real life
application.

As we do not try to immitate human subtitling be-
haviour, but try to develop an equivalent approach,
our system is not evaluated in the same way as the
system deviced by Jing.</corps>
		<conclusion>4 Conclusion

We have described a hybrid approach to sentence
compression which seems to work in general. The
combination of using statistics and ﬁltering out in-
valid results because they are ungrammatical by us-
ing a set of rules is a feasible way for automated


Guidance

1997.
subtitling.
Online

ITC.
for
ITC.
codes guidelines/broadcasting/tv/sub sign
audio/subtitling stnds/.

standards
on
Technical
report,
http://www.itc.org.uk/

at

H. Jing. 2001. Cut-and-Paste Text Summarization.

Ph.D. thesis, Columbia University.

F.J. Koopmans-van Beinum and M.E. van Donzel.
1996. Relationship Between Discourse Structure
and Dynamic Speech Rate. In Proceedings IC-
SLP 1996, Philadelphia, USA.

N. Oostdijk, W. Goedertier, F. Van Eynde, L. Boves,
J.P. Marters, M. Moortgat, and H. Baayen. 2002.
Experiences from the Spoken Dutch Corpus. In
Proceedings of LREC 2002, volume I, pages 340–
347, Paris. ELRA.

F. Van Eynde.

2004. Part-of-speech Tagging
en Lemmatisering.
Internal manual of Cor-
pus Gesproken Nederlands, published online at
http://www.ccl.kuleuven.ac.be/Papers/
POSmanual febr2004.pdf.

V. Vandeghinste and E. Tjong Kim Sang. 2004. Us-
ing a parallel transcript/subtitle corpus for sen-
tence compression.
In Proceedings of LREC
2004, Paris. ELRA.

V. Vandeghinste.

2002. Lexicon optimization:
Maximizing lexical coverage in speech recogni-
tion through automated compounding.
In Pro-
ceedings of LREC 2002, volume IV, pages 1270–
1276, Paris. ELRA.

V. Vandeghinste.

submitted. ShaRPa: Shallow
Rule-based Parsing, focused on Dutch. In Pro-
ceedings of CLIN 2003.

sentence compression.

The way of combining the probability-estimates
of chunk removal to get a ranking in the generated
sentence alternatives is working reasonably well,
but could be improved by using more ﬁne-grained
chunk types for data collection.

A full syntactic analysis of the input sentence
would lead to better results, as the current sentence
analysis tools have one very weak point: the han-
dling of coordinating conjunction, which leads to
chunking errors, both in the input sentence as in the
processing of the used parallel corpus. This leads to
misestimations of the compression probabilities and
creates noise in the behaviour of our system.

Making use of semantics would most probably
lead to better results, but a semantic lexicon and
semantic analysis tools are not available for Dutch,
and creating them would be out of the scope of the
current project.

In future research we will check the effects of
improved word-reduction modules, as word reduc-
tions often seem to lead to inaccurate compres-
sions. Leaving out the word-reduction module
would probably lead to an even bigger amount of
no output-cases. This will also be checked in future
research.

5 Acknowledgements
Research funded by IWT (Institute for Innovation
in Science and Technology) in the STWW pro-
gram, project ATraNoS (Automatic Transcription
and Normalisation of Speech). For more informa-
tion visit http://atranos.esat.kuleuven.ac.be/.

We would like to thank Ineke Schuurman for rat-

ing the reduced sentences.</conclusion>
		<discussion>Aucune discussion trouvée.</discussion>
		<biblio>G. Booij and A. van Santen. 1995. Morfologie. De
woordstructuur van het Nederlands. Amsterdam
University Press, Amsterdam, Netherlands.

T. Brants. 2000. TnT - A Statistical Part-of-Speech
Tagger. Published online at http://www.coli.uni-
sb.de/thorsten/tnt.

W. Daelemans and H. Strik. 2002. Het Neder-
lands in Taal- en Spraaktechnologie: Prioriteiten
voor Basisvoorzieningen. Technical report, Ne-
derlandse Taalunie.

B. Dewulf and G. Saerens.

Stijlboek
Teletekst Ondertiteling. Technical report, VRT,
Brussel. Internal Subtitling Guidelines.

2000.

W. Haeseryn, G. Geerts,

and
M. van den Toorn. 1997. Algemene Neder-
landse Spraakkunst. Martinus Nijhoff Uitgevers,
Groningen.

J de Rooij,</biblio>
	</article>
	<article>
		<preamble>marcu_statistics_sentence_pass_one.txt</preamble>
		<titre>Statistics-Based Summarization — Step One: Sentence Compression</titre>
		<auteur>Kevin Knight and Daniel Marcu</auteur>
		<abstract>When humans produce summaries of documents, they do not simply extract sentences and concatenate them. Rather, they create new sentences that are grammati- cal, that cohere with one another, and that capture the most salient pieces of information in the original doc- ument. Given that large collections of text/abstract pairs are available online, it is now possible to envision algorithms that are trained to mimic this process. In this paper, we focus on sentence compression, a sim- pler version of this larger challenge. We aim to achieve two goals simultaneously: our compressions should be grammatical, and they should retain the most impor- tant pieces of information. These two goals can con- ﬂict. We devise both noisy-channel and decision-tree approaches to the problem, and we evaluate results against manual compressions and a simple baseline.</abstract>
		<introduction>Most of the research in automatic summarization has focused on extraction, i.e., on identifying the most important clauses/sentences/paragraphs in texts (see (Mani & Maybury 1999) for a representative col- lection of papers). However, determining the most im- portant textual segments is only half of what a summa- rization system needs to do because, in most cases, the simple catenation of textual segments does not yield coherent outputs. Recently, a number of researchers have started to address the problem of generating co- herent summaries: McKeown et al. (1999), Barzilay et al. (1999), and Jing and McKeown (1999) in the context of multidocument summarization; Mani et al. (1999) in the context of revising single document extracts; and Witbrock and Mittal (1999) in the context of headline generation. The approach proposed by Witbrock and Mit- tal (1999) is the only one that applies a probabilistic model trained directly on (cid:3)Headline, Document(cid:4) pairs. However, this model has yet to scale up to generat- ing multiple-sentence abstracts as well as well-formed, grammatical sentences. All other approaches employ sets of manually written or semi-automatically derived Copyright c(cid:1) 2000, American Association for Artiﬁcial In- telligence (www.aaai.org). All rights reserved. rules for deleting information that is redundant, com- pressing long sentences into shorter ones, aggregating sentences, repairing reference links, etc. Our goal is also to generate coherent abstracts. How- ever, in contrast with the above work, we intend to eventually use (cid:3)Abstract, Text(cid:4) tuples, which are widely available, in order to automatically learn how to rewrite Texts as coherent Abstracts. In the spirit of the work in the statistical MT community, which is focused on sentence-to-sentence translations, we also decided to fo- cus ﬁrst on a simpler problem, that of sentence compres- sion. We chose this problem for two reasons: • First, the problem is complex enough to require the development of sophisticated compression models: Determining what is important in a sentence and determining how to convey the important informa- tion grammatically, using only a few words, is just a scaled down version of the text summarization prob- lem. Yet, the problem is simple enough, since we do not have to worry yet about discourse related issues, such as coherence, anaphors, etc. • Second, an adequate solution to this problem has an immediate impact on several applications. For example, due to time and space constraints, the generation of TV captions often requires only the most important parts of sentences to be shown on a screen (Linke-Ellis 1999; Robert-Ribes et al. 1999). A good sentence compression module would there- fore have an impact on the task of automatic cap- tion generation. A sentence compression module can also be used to provide audio scanning ser- vices for the blind (Grefenstette 1998). In gen- eral, since all systems aimed at producing coher- ent abstracts implement manually written sets of sentence compression rules (McKeown et al. 1999; Mani, Gates, & Bloedorn 1999; Barzilay, McKeown, & Elhadad 1999), it is likely that a good sentence compression module would impact the overall quality of these systems as well. This becomes particularly important for text genres that use long sentences. In this paper, we present two approaches to the sen- tence compression problem. Both take as input a se- quence of words W = w1, w2, . . . , wn (one sentence). An algorithm may drop any subset of these words. The words that remain (order unchanged) form a compres- sion. There are 2n compressions to choose from—some are reasonable, most are not. Our ﬁrst approach de- velops a probabilistic noisy-channel model for sentence compression. The second approach develops a decision- based, deterministic model.</introduction>
		<corps>A noisy-channel model for sentence
compression
This section describes a probabilistic approach to the
compression problem. In particular, we adopt the noisy
channel framework that has been relatively successful in
a number of other NLP applications, including speech
recognition (Jelinek 1997), machine translation (Brown
1993), part-of-speech tagging (Church 1988),
et al.
transliteration (Knight & Graehl 1998), and informa-
tion retrieval (Berger & Laﬀerty 1999).

In this framework, we look at a long string and imag-
ine that (1) it was originally a short string, and then
(2) someone added some additional, optional text to it.
Compression is a matter of identifying the original short
string. It is not critical whether or not the “original”
string is real or hypothetical. For example, in statistical
machine translation, we look at a French string and say,
“This was originally English, but someone added ‘noise’
to it.” The French may or may not have been translated
from English originally, but by removing the noise, we
can hypothesize an English source—and thereby trans-
late the string. In the case of compression, the noise
consists of optional text material that pads out the core
signal. For the larger case of text summarization, it may
be useful to imagine a scenario in which a news editor
composes a short document, hands it to a reporter, and
tells the reporter to “ﬂesh it out” . . . which results in
the article we read in the newspaper. As summarizers,
we may not have access to the editor’s original version
(which may or may not exist), but we can guess at it—
which is where probabilities come in.

As in any noisy channel application, we must solve

three problems:
• Source model. We must assign to every string s a
probability P(s), which gives the chance that s is gen-
erated as an “original short string” in the above hy-
pothetical process. For example, we may want P(s)
to be very low if s is ungrammatical.

• Channel model. We assign to every pair of strings
(cid:3)s, t(cid:4) a probability P(t | s), which gives the chance
that when the short string s is expanded, the result
is the long string t. For example, if t is the same
as s except for the extra word “not,” then we may
want P(t | s) to be very low. The word “not” is not
optional, additional material.

• Decoder. When we observe a long string t, we search
for the short string s that maximizes P(s | t). This
is equivalent to searching for the s that maximizes
P(s) · P (t | s).

It is advantageous to break the problem down this
way, as it decouples the somewhat independent goals
of creating a short text that (1) looks grammatical,
and (2) preserves important information. It is easier to
build a channel model that focuses exclusively on the
latter, without having to worry about the former. That
is, we can specify that a certain substring may represent
unimportant information, but we do not need to worry
that deleting it will result in an ungrammatical struc-
ture. We leave that to the source model, which worries
exclusively about well-formedness. In fact, we can make
use of extensive prior work in source language modeling
for speech recognition, machine translation, and natu-
ral language generation. The same goes for actual com-
pression (“decoding” in noisy-channel jargon)—we can
re-use generic software packages to solve problems in all
these application domains.

Statistical Models

In the experiments we report here, we build very sim-
In a departure from
ple source and channel models.
the above discussion and from previous work on statis-
tical channel models, we assign probabilities Ptree(s)
and Pexpand tree(t | s) to trees rather than strings. In
decoding a new string, we ﬁrst parse it into a large tree t
(using Collins’ parser (1997)), and we then hypothesize
and rank various small trees.

Good source strings are ones that have both (1) a
normal-looking parse tree, and (2) normal-looking word
pairs. Ptree(s) is a combination of a standard proba-
bilistic context-free grammar (PCFG) score, which is
computed over the grammar rules that yielded the tree
s, and a standard word-bigram score, which is com-
puted over the leaves of the tree. For example, the
tree s =(S (NP John) (VP (VB saw) (NP Mary))) is
assigned a score based on these factors:
Ptree(s) = P(TOP → S | TOP) ·
P(S → NP VP | S) · P(NP → John | NP) ·
P(VP → VB NP | VP) · P(VP → saw | VB) ·
P(NP → Mary | NP) ·
P(John | EOS) · P(saw | John) ·
P(Mary | saw) · P(EOS | Mary)

Our stochastic channel model performs minimal op-
erations on a small tree s to create a larger tree t. For
each internal node in s, we probabilistically choose an
expansion template based on the labels of the node and
its children. For example, when processing the S node
in the tree above, we may wish to add a prepositional
phrase as a third child. We do this with probability
P(S → NP VP PP | S → NP VP). Or we may choose
to leave it alone, with probability P(S → NP VP | S →
NP VP). After we choose an expansion template, then
for each new child node introduced (if any), we grow a
new subtree rooted at that node—for example (PP (P
in) (NP Pittsburgh)). Any particular subtree is grown
with probability given by its PCFG factorization, as
above (no bigrams).


H

a

G

C

b

A

B

R

d

D

e

Q

Z

c

(t)

H

a

G

A

C

b

D

e

G

K

b

F

H

a

D

e

(s1)

(s2)

Figure 1: Examples of parse trees.

Example

In this section, we show how to tell whether one poten-
tial compression is more likely than another, according
to the statistical models described above. Suppose we
observe the tree t in Figure 1, which spans the string
abcde. Consider the compression s1, which is shown in
the same ﬁgure.

We

the

compute

and
| s1). Breaking this down further,
Pexpand tree(t
the source PCFG and word-bigram factors, which
describe Ptree(s1), are:

Ptree(s1)

factors

P(TOP → G | TOP) P(H → a | H)
P(C → b | C)
P(G → H A | G)
P(D → e | D)
P(A → C D | A)

P(a | EOS)
P(b | a)

P(e | b)
P(EOS | e)

The channel expansion-template factors and the chan-
nel PCFG (new tree growth) factors, which describe
Pexpand tree(t | s1), are:

P(G → H A | G → H A)
P(A → C B D | A → C D)
P(B → Q R | B)
P(Q → Z | Q)

P(Z → c | Z)
P(R → d | R)

A diﬀerent compression will be scored with a diﬀerent
set of factors. For example, consider a compression of
t that leaves t completely untouched. In that case, the
source costs Ptree(t) are:

P(TOP → G | TOP) P(H → a | H) P(a | EOS)
P(G → H A | G)
P(A → C D | A)
P(B → Q R | B)
P(Q → Z | Q)

P(C → b | C)
P(b | a)
P(c | b)
P(Z → c | Z)
P(R → d | R)
P(d | c)
P(D → e | D) P(e | d)

P(EOS | e)

The channel costs Pexpand tree(t | t) are:

The documentation is typical of Epson quality: excellent.
Documentation is excellent.

All of our design goals were achieved and the delivered
performance matches the speed of the underlying device.
All design goals were achieved.

Reach’s E-mail product, MailMan, is a message- manage-
ment system designed initially for VINES LANs that will
eventually be operating system-independent.
MailMan will eventually be operating system-independent.

Although the modules themselves may be physically and/or
electrically incompatible, the cable-speciﬁc jacks on them
provide industry-standard connections.
Cable-speciﬁc jacks provide industry-standard connections.

Ingres/Star prices start at $2,100.
Ingres/Star prices start at $2,100.

Figure 2: Examples from our parallel corpus.

P(G → H A | G → H A)
P(A → C B D | A → C B D)
P(B → Q R | B → Q R)
P(Q → Z | Q → Z)

Now we can simply compare Pexpand tree(s1 |
t) = Ptree(s1) · Pexpand tree(t | s1))/Ptree(t) ver-
sus Pexpand tree(t | t) = Ptree(t) · Pexpand tree(t |
t))/Ptree(t) and select the more likely one. Note that
Ptree(t) and all the PCFG factors can be canceled out,
as they appear in any potential compression. Therefore,
we need only compare compressions of the basis of the
expansion-template probabilities and the word-bigram
probabilities. The quantities that diﬀer between the
two proposed compressions are boxed above. There-
fore, s1 will be preferred over t if and only if:

P(e | b) · P(A → C B D | A → C D) >
P(b | a) · P(c | b) · P(d | c) ·
P(A → C B D | A → C B D) ·
P(B → Q R | B → Q R) · P(Q → Z | Q → Z)

Training Corpus
In order to train our system, we used the Ziﬀ-Davis
corpus, a collection of newspaper articles announcing
computer products. Many of the articles in the corpus
are paired with human written abstracts. We automat-
ically extracted from the corpus a set of 1067 sentence
pairs. Each pair consisted of a sentence t = t1, t2, . . . , tn
that occurred in the article and a possibly compressed
version of it s = s1, s2, . . . , sm, which occurred in the
human written abstract. Figure 2 shows a few sentence
pairs extracted from the corpus.

We decided to use such a corpus because it is con-
sistent with two desiderata speciﬁc to summarization
(i) the human-written Abstract sentences are
work:


grammatical; (ii) the Abstract sentences represent in a
compressed form the salient points of the original news-
paper Sentences. We decided to keep in the corpus un-
compressed sentences as well, since we want to learn
not only how to compress a sentence, but also when to
do it.

(VP . . . )

Learning Model Parameters
We collect expansion-template probabilities from our
parallel corpus. We ﬁrst parse both sides of the parallel
corpus, and then we identify corresponding syntactic
nodes. For example, the parse tree for one sentence
may begin (S (NP . . . )
(PP . . . )) while
the parse tree for its compressed version may begin (S
(NP . . . ) (VP . . . )). If these two S nodes are deemed
to correspond, then we chalk up one joint event (S →
NP VP, S → NP VP PP); afterwards we normalize.
Not all nodes have corresponding partners; some non-
correspondences are due to incorrect parses, while oth-
ers are due to legitimate reformulations that are beyond
the scope of our simple channel model. We use standard
methods to estimate word-bigram probabilities.

Decoding
There is a vast number of potential compressions of a
large tree t, but we can pack them all eﬃciently into a
shared-forest structure. For each node of t that has n
children, we
• generate 2n − 1 new nodes, one for each non-empty

subset of the children, and

• pack those nodes so that they are referred to as a

whole.

For example, consider the large tree t above. All com-
pressions can be represented with the following forest:

A → B C H → a
G → H A B → R
C → b
Q → Z
G → H
A → C
Z → c
A → C B D A → B
G → A
R → d
A → D
B → Q R A → C B
D → e
A → C D
B → Q

We can also assign an expansion-template probability
to each node in the forest. For example, to the B →
Q node, we can assign P(B → Q R | B → Q). If the
observed probability from the parallel corpus is zero,
then we assign a small ﬂoor value of 10−6. In reality,
we produce forests that are much slimmer, as we only
consider compressing a node in ways that are locally
grammatical according to the Penn Treebank—if a rule
of the type A → C B has never been observed, then it
will not appear in the forest.

At this point, we want to extract a set of high-
scoring trees from the forest, taking into account
both expansion-template probabilities and word-bigram
probabilities. Fortunately, we have such a generic ex-
tractor on hand (Langkilde 2000). This extractor was
designed for a hybrid symbolic-statistical natural lan-
guage generation system called Nitrogen. In that ap-
plication, a rule-based component converts an abstract

semantic representation into a vast number of potential
English renderings. These renderings are packed into
a forest, from which the most promising sentences are
extracted using statistical scoring.

For our purposes, the extractor selects the trees with
the best combination of word-bigram and expansion-
template scores. It returns a list of such trees, one for
each possible compression length. For example, for
the sentence Beyond that basic level, the operations of
the three products vary, we obtain the following “best”
compressions, with negative log-probabilities shown in
parentheses (smaller = more likely):

Beyond that basic level, the operations of the three products vary

widely (1514588)

Beyond that level, the operations of the three products vary widely

(1430374)

Beyond that basic level, the operations of the three products vary

(1333437)

Beyond that

level,

the operations of

the three products vary

(1249223)

Beyond that basic level,

the operations of

the products vary

(1181377)

The operations of the three products vary widely (939912)

The operations of the products vary widely (872066)

The operations of the products vary (748761)

The operations of products vary (690915)

Operations of products vary (809158)

The operations vary (522402)

Operations vary (662642)

Length Selection
It is useful to have multiple answers to choose from, as
one user may seek a 20% compression, while another
seeks a 60% compression. However, for purposes of
evaluation, we want our system to be able to select a
single compression. If we rely on the log-probabilities
as shown above, we will almost always choose the short-
est compression. (Note above, however, how the three-
word compression scores better than the two-word com-
pression, as the models are not entirely happy removing
the article “the”). To create a more fair competition,
we divide the log-probability by the length of the com-
pression, rewarding longer strings. This is commonly
done in speech recognition.

If we plot this normalized score against compression
length, we usually observe a (bumpy) U-shaped curve,
In a typical more diﬃcult
as illustrated in Figure 3.
case, a 25-word sentence may be optimally compressed
by a 17-word version. Of course, if a user requires a
shorter compression than that, she may select another
region of the curve and look for a local minimum.

A decision-based model for sentence
compression
In this section, we describe a decision-based, history
model of sentence compression. As in the noisy-channel
approach, we again assume that we are given as input


t
s
e
b
f
o
y
t
i
l
i
b
a
b
o
r
p
-
g
o
l

e
v
i
t
a
g
e
n
d
e
t
s
u
j
d
A

n
h
t
g
n
e
l

r
a
l
u
c
i
t
r
a
p

a

t
a

s
n
o
i
s
s
e
r
p
m
o
c

n

/

)
s

|

t

(
P
)
s
(
P
g
o
l
-

0.20

0.15

0.10

.

e
c
n
a
t
s
i
d
s
i
d
n
a
b
d
a
o
r
b
f
o

e
g
a
t
n
a
v
d
a

r
e
h
t
o
n
a
y
l
l
a
n
i
F

.

e
c
n
a
t
s
i
d

s
i

d
n
a
b
d
a
o
r
b

f
o
e
g
a
t
n
a
v
d
a

r
e
h
t
o
n
a

,

y
l
l
a
n
i
F

.

e
c
n
a
t
s
i
d
s
i

e
g
a
t
n
a
v
d
A

.

e
c
n
a
t
s
i
d
s
i

e
g
a
t
n
a
v
d
a

r
e
h
t
o
n
A

.

e
c
n
a
t
s
i
d

s
i

d
n
a
b
d
a
o
r
b

f
o

e
g
a
t
n
a
v
d
A

.

e
c
n
a
t
s
i
d

s
i
d
n
a
b
d
a
o
r
b

f
o

e
g
a
t
n
a
v
d
a

r
e
h
t
o
n
A

4

5

6

7

8

9

Compression length n

Figure 3: Adjusted log-probabilities for top-scoring
compressions at various lengths (lower is better).

a parse tree t. Our goal is to “rewrite” t into a smaller
tree s, which corresponds to a compressed version of the
original sentence subsumed by t. Suppose we observe in
our corpus the trees t and s2 in Figure 1. In this model,
we ask ourselves how we may go about rewriting t into
s2. One possible solution is to decompose the rewriting
operation into a sequence of shift-reduce-drop actions
that are speciﬁc to an extended shift-reduce parsing
paradigm.

In the model we propose, the rewriting process starts
with an empty Stack and an Input List that contains the
sequence of words subsumed by the large tree t. Each
word in the input list is labeled with the name of all syn-
tactic constituents in t that start with it (see Figure 4).
At each step, the rewriting module applies an opera-
tion that is aimed at reconstructing the smaller tree s2.
In the context of our sentence-compression module, we
need four types of operations:
• shift operations transfer the ﬁrst word from the in-

put list into the stack;

• reduce operations pop the k syntactic trees located
at the top of the stack; combine them into a new
tree; and push the new tree on the top of the stack.
Reduce operations are used to derive the structure of
the syntactic tree of the short sentence.

• drop operations are used to delete from the input list
subsequences of words that correspond to syntactic
constituents. A drop x operations deletes from the

Stack

Input List

Stack

Input List

G
H
a

A

C
b

B
Q
Z
c

R
d

D
e

A

C
b

B
Q
Z
c

R
d

B
Q
Z
c

R
d

D
e

H

a

K

b

H

a

D
e

SHIFT;

ASSIGNTYPE H

STEPS 1-2

SHIFT;

ASSIGNTYPE K

STEPS 3-4

REDUCE 2 F

STEP 5

D
e

R
d

B
Q
Z
c

D
e

F

K

b

F

K

b

D

e

D

e

G

H

a

F

F

H

a

K

b

K

b

H

a

H

a

DROP B

STEP 6

SHIFT;

ASSIGNTYPE D

STEPS 7-8

REDUCE 2 G

STEP 9

Figure 4: Example of incremental tree compression.

input list all words that are spanned by constituent
x in t.

• assignType operations are used to change the label
of trees at the top of the stack. These actions assign
POS tags to the words in the compressed sentence,
which may be diﬀerent from the POS tags in the
original sentence.

The decision-based model
is more ﬂexible than the
channel model because it enables the derivation of trees
whose skeleton can diﬀer quite drastically from that of
the tree given as input. For example, using the channel
model, we are unable to obtain tree s2 from t. However,
the four operations listed above enable us to rewrite a
tree t into any tree s, as long as an in-order traversal of
the leaves of s produces a sequence of words that occur
in the same order as the words in the tree t. For exam-
ple, the tree s2 can be obtained from tree t by following
this sequence of actions, whose eﬀects are shown in Fig-
ure 4: shift; assignType H; shift; assignType K;
reduce 2 F; drop B; shift; assignType D; reduce
2 G.

To save space, we show shift and assignType op-
erations on the same line; however, the reader should
understand that they correspond to two distinct ac-
tions. As one can see, the assignType K operation
rewrites the POS tag of the word b; the reduce op-
erations modify the skeleton of the tree given as input.
To increase readability, the input list is shown in a for-
mat that resembles as closely as possible the graphical
representation of the trees in ﬁgure 1.

Learning the parameters of the
decision-based model

We associate with each conﬁguration of our shift-
reduce-drop, rewriting model a learning case. The cases
are generated automatically by a program that derives
sequences of actions that map each of the large trees in
our corpus into smaller trees. The rewriting procedure
simulates a bottom-up reconstruction of the smaller
trees.

Overall, the 1067 pairs of long and short sentences
yielded 46383 learning cases. Each case was labeled

































































with one action name from a set of 210 possible ac-
tions: There are 37 distinct assignType actions, one
for each POS tag. There are 63 distinct drop actions,
one for each type of syntactic constituent that can be
deleted during compression. There are 109 distinct re-
duce actions, one for each type of reduce operation that
is applied during the reconstruction of the compressed
sentence. And there is one shift operation. Given a
tree t and an arbitrary conﬁguration of the stack and
input list, the purpose of the decision-based classiﬁer
is to learn what action to choose from the set of 210
possible actions.

To each learning example, we associated a set of 99

features from the following two classes:

Operational features reﬂect the number of trees
the input list, and the types of
in the stack,
the last ﬁve operations. They also encode infor-
mation that denote the syntactic category of the
root nodes of the partial trees built up to a cer-
tain time. Examples of such features are: num-
berTreesInStack, wasPreviousOperationShift,
syn-
tacticLabelOfTreeAtTheTopOfStack, etc.

Original-tree-speciﬁc features denote the

syntac-
tic constituents that start with the ﬁrst unit in the
input list. Examples of such features are: inputList-
StartsWithA CC, inputListStartsWithA PP, etc.

The decision-based compression module uses the
C4.5 program (Quinlan 1993) in order to learn deci-
sion trees that specify how large syntactic trees can
be compressed into shorter trees. A ten-fold cross-
validation evaluation of the classiﬁer yielded an accu-
racy of 87.16% (± 0.14). A majority baseline classi-
ﬁer that chooses the action shift has an accuracy of
28.72%.

Employing the decision-based model

To compress sentences, we apply the shift-reduce-drop
model in a deterministic fashion. We parse the sentence
to be compressed (Collins 1997) and we initialize the
input list with the words in the sentence and the syn-
tactic constituents that “begin” at each word, as shown
in Figure 4. We then incrementally inquire the learned
classiﬁer what action to perform, and we simulate the
execution of that action. The procedure ends when the
input list is empty and when the stack contains only
one tree. An inorder traversal of the leaves of this tree
produces the compressed version of the sentence given
as input.

Since the model is deterministic, it produces only one
output. The advantage is that the compression is very
fast: it takes only a few milliseconds per sentence. The
disadvantage is that it does not produce a range of
compressions, from which another system may subse-
quently choose. It is straightforward though to extend
the model within a probabilistic framework by applying,
for example, the techniques used by Magerman (1995).

Evaluation
To evaluate our compression algorithms, we randomly
selected 32 sentence pairs from our parallel corpus,
which we will refer to as the Test Corpus. We used the
other 1035 sentence pairs for training. Figure 5 shows
three sentences from the Test Corpus, together with the
compressions produced by humans, our compression al-
gorithms, and a baseline algorithm that produces com-
pressions with highest word-bigram scores. The exam-
ples are chosen so as to reﬂect good, average, and bad
performance cases. The ﬁrst sentence is compressed in
the same manner by humans and our algorithms (the
baseline algorithm chooses though not to compress this
sentence). For the second example, the output of the
Decision-based algorithm is grammatical, but the se-
mantics is negatively aﬀected. The noisy-channel al-
gorithm deletes only the word “break”, which aﬀects
the correctness of the output less. In the last example,
the noisy-channel model is again more conservative and
decides not to drop any constituents. In constrast, the
decision-based algorithm compresses the input substan-
tially, but it fails to produce a grammatical output.

We presented each original sentence in the Test Cor-
pus to four judges, together with four compressions of it:
the human generated compression, the outputs of the
noisy-channel and decision-based algorithms, and the
output of the baseline algorithm. The judges were told
that all outputs were generated automatically. The or-
der of the outputs was scrambled randomly across test
cases.

To avoid confounding, the judges participated in two</corps>
		<conclusion>Aucune conclusion trouvée.</conclusion>
		<discussion>Aucune discussion trouvée.</discussion>
		<biblio>Barzilay, R.; McKeown, K.; and Elhadad, M. 1999.
Information fusion in the context of multi-document
summarization.
In Proceedings of the 37th Annual
Meeting of the Association for Computational Linguis-
tics (ACL–99), 550–557.

Berger, A., and Laﬀerty, J. 1999. Information retrieval
as statistical translation. In Proceedings of the 22nd
Conference on Research and Development in Informa-
tion Retrieval (SIGIR–99), 222–229.

Brown, P.; Della Pietra, S.; Della Pietra, V.; and Mer-
cer, R. 1993. The mathematics of statistical ma-
chine translation: Parameter estimation. Computa-
tional Linguistics 19(2):263–311.

Church, K. 1988. A stochastic parts program and noun
phrase parser for unrestricted text. In Proceedings of
the Second Conference on Applied Natural Language
Processing, 136–143.

Collins, M. 1997. Three generative, lexicalized mod-
els for statistical parsing. In Proceedings of the 35th
Annual Meeting of the Association for Computational
Linguistics (ACL–97), 16–23.

1998. Producing intelligent tele-
Grefenstette, G.
graphic text reduction to provide an audio scanning
service for the blind. In Working Notes of the AAAI


Spring Symposium on Intelligent Text Summarization,
111–118.
Jelinek, F. 1997. Statistical Methods for Speech Recog-
nition. The MIT Press.
Jing, H., and McKeown, K. 1999. The decomposition
of human-written summary sentences. In Proceedings
of the 22nd Conference on Research and Development
in Information Retrieval (SIGIR–99).
Knight, K., and Graehl, J. 1998. Machine transliter-
ation. Computational Linguistics 24(4):599–612.
Langkilde, I. 2000. Forest-based statistical sentence
generation. In Proceedings of the 1st Annual Meeting
of the North American Chapter of the Association for
Computational Linguistics.
1999. Closed captioning in Amer-
Linke-Ellis, N.
ica: Looking beyond compliance.
In Proceedings of
the TAO Workshop on TV Closed Captions for the
hearing impaired people, 43–59.
Magerman, D. 1995. Statistical decision-tree models
for parsing. In Proceedings of the 33rd Annual Meeting
of the Association for Computational Linguistics, 276–
283.
Mani, I., and Maybury, M., eds. 1999. Advances in
Automatic Text Summarization. The MIT Press.
Mani, I.; Gates, B.; and Bloedorn, E. 1999. Improving
summaries by revising them. In Proceedings of the 37th
Annual Meeting of the Association for Computational
Linguistics, 558–565.
McKeown, K.; Klavans, J.; Hatzivassiloglou, V.;
Barzilay, R.; and Eskin, E. 1999. Towards multidoc-
ument summarization by reformulation: Progress and
prospects.
In Proceedings of the Sixteenth National
Conference on Artiﬁcial Intelligence (AAAI–99).
Quinlan, J. 1993. C4.5: Programs for Machine Learn-
ing. San Mateo, CA: Morgan Kaufmann Publishers.
Robert-Ribes, J.; Pfeiﬀer, S.; Ellison, R.; and Burn-
ham, D. 1999. Semi-automatic captioning of TV pro-
grams, an Australian perspective.
In Proceedings of
the TAO Workshop on TV Closed Captions for the
hearing impaired people, 87–100.
Ultra-
Witbrock, M., and Mittal, V.
summarization: A statistical approach to generating
highly condensed non-extractive summaries. In Pro-
ceedings of the 22nd International Conference on Re-
search and Development in Information Retrieval (SI-
GIR’99), Poster Session, 315–316.

1999.</biblio>
	</article>
	<article>
		<preamble>mikheev.txt</preamble>
		<titre>Periods, Capitalized Words, etc.</titre>
		<auteur>Andrei Mikheev∗</auteur>
		<abstract>∗ Institute for Communicating and Collaborative Systems, Division of Informatics, 2 Buccleuch Place,</abstract>
		<introduction>problem can be found in Palmer and Hearst (1997). The disambiguation of capitalized words and sentence boundaries presents a chicken-and-egg problem. If we know that a capitalized word that follows a period is a common word, we can safely assign such period as sentence terminal. On the other hand, if we know that a period is not sentence terminal, then we can conclude that the following capitalized word is a proper name. Another frequent source of ambiguity in end-of-sentence marking is introduced by abbreviations: if we know that the word that precedes a period is not an abbreviation, then almost certainly this period denotes a sentence boundary. If, however, this word is an abbreviation, then it is not that easy to make a clear decision. This problem is exacerbated by the fact that abbreviations do not form a closed set; that is, one can- not list all possible abbreviations. Moreover, abbreviations can coincide with regular words; for example, “in” can denote an abbreviation for “inches,” “no” can denote an abbreviation for “number,” and “bus” can denote an abbreviation for “business.” In this article we present a method that tackles sentence boundaries, capitalized words, and abbreviations in a uniform way through a document-centered approach. As opposed to the two dominant techniques of computing statistics about the words that surround potential sentence boundaries or writing specialized grammars, our ap- 1 In this article we are concerned only with the identiﬁcation of proper names. 290 Mikheev Periods, Capitalized Words, etc. proach disambiguates capitalized words and abbreviations by considering suggestive local contexts and repetitions of individual words within a document. It then applies this information to identify sentence boundaries using a small set of rules.</introduction>
		<corps>2. Performance Measure, Corpora for Evaluation, and Intended Markup

A standard practice for measuring the performance of a system for the class of tasks
with which we are concerned in this article is to calculate its error rate:

error rate =

incorrectly assigned
all assigned by system

This single measure gives enough information, provided that the system does not
leave unassigned word tokens that it is intended to handle. Obviously, we want the
system to handle all cases as accurately as possible. Sometimes, however, it is beneﬁcial
to assign only cases in which the system is conﬁdent enough, leaving the rest to be
handled by other methods. In this case apart from the error rate (which corresponds
to precision or accuracy as 1−error rate) we also measure the system’s coverage or
recall

coverage =

correctly assigned
all to be assigned

2.1 Corpora for Evaluation
There are two corpora normally used for evaluation in a number of text-processing
tasks: the Brown corpus (Francis and Kucera 1982) and the Wall Street Journal (WSJ)
corpus, both part of the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993).
The Brown corpus represents general English. It contains over one million word tokens
and is composed from 15 subcorpora that belong to different genres and domains,
ranging from news wire texts and scientiﬁc papers to ﬁction and transcribed speech.
The Brown corpus is rich in out-of-vocabulary (unknown) words, spelling errors, and
ungrammatical sentences with complex internal structure. Altogether there are about
500 documents in the Brown corpus, with an average length of 2,300 word tokens.

The WSJ corpus represents journalistic news wire style. Its size is also over a
million word tokens, and the documents it contains are rich in abbreviations and
proper names, but they are much shorter than those in the Brown corpus. Altogether
there are about 2,500 documents in the WSJ corpus, with an average length of about
500 word tokens.

Documents in the Penn Treebank are segmented into paragraphs and sentences.
Sentences are further segmented into word tokens annotated with part-of-speech (POS)
information. POS information can be used to distinguish between proper names and
common words. We considered proper nouns (NNP), plural proper nouns (NNPS), and
proper adjectives2 (JJP) to signal proper names, and all other categories were consid-
ered to signal common words or punctuation. Since proper adjectives are not included
in the Penn Treebank tag set, we had to identify and retag them ourselves with the
help of a gazetteer.

Abbreviations in the Penn Treebank are tokenized together with their trailing pe-
riods, whereas fullstops and other sentence boundary punctuation are tokenized as
separate tokens. This gives all necessary information for the evaluation in all our three

2 These are adjectives derived from proper nouns (e.g. “American”).

291


Computational Linguistics

Volume 28, Number 3

tasks: the sentence boundary disambiguation task, the capitalized word disambigua-
tion task, and the abbreviation identiﬁcation task.

2.2 Tokenization Convention and Corpora Markup
For easier handling of potential sentence boundary punctuation, we developed a new
tokenization convention for periods. In the traditional Penn Treebank schema, abbrevi-
ations are tokenized together with their trailing periods, and thus stand-alone periods
unambiguously signal the end of a sentence. We decided to treat periods and all other
potential sentence termination punctuation as “ﬁrst-class citizens” and adopted a con-
vention always to tokenize a period (and other punctuation) as a separate token when
it is followed by a white space, line break, or punctuation. In the original Penn Tree-
bank format, periods are unambiguous, whereas in our new convention they can take
on one of the three tags: fullstop (.), part of abbreviation (A) or both (*).

To generate the new format from the Penn Treebank, we had to split ﬁnal periods
from abbreviations, mark them as separate tokens and assign them with A or * tags
according to whether or not the abbreviation was the last token in a sentence. We
applied a similar tokenization convention to the case in which several (usually three)
periods signal ellipsis in a sentence. Again, sometimes such constructions occur within
a sentence and sometimes at a sentence break. We decided to treat such constructions
similarly to abbreviations, tokenize all periods but the last together in a single token,
and tokenize the last period separately and tag it with A or * according to whether
or not the ellipsis was the last token in a sentence. We treated periods in numbers
(e.g., 14.534) or inside acronyms (e.g., Y.M.C.A.) as part of tokens rather than separate
periods.

In all our experiments we treated embedded sentence boundaries in the same way
as normal sentence boundaries. An embedded sentence boundary occurs when there
is a sentence inside a sentence. This can be a quoted direct-speech subsentence inside a
sentence, a subsentence embedded in brackets, etc. We considered closing punctuation
of such sentences equal to closing punctuation of normal sentences.

We also specially marked word tokens in positions where they were ambiguously

capitalized if such word tokens occurred in one of the following contexts:

•

•

•

the ﬁrst token in a sentence

following a separately tokenized period, question mark, exclamation
mark, semicolon, colon, opening quote, closing quote, opening bracket,
or closed bracket

occurring in a sentence with all words capitalized

All described transformations were performed automatically by applying a simple
Perl script. We found quite a few infelicities in the original tokenization and tagging,
however, which we had to correct by hand. We also converted both our corpora from
their original Penn Treebank format into an XML format where each word token is
represented as an XML element (W) with the attribute C holding its POS information
and attribute A set to Y for ambiguously capitalized words. An example of such a
markup is displayed in Figure 1.

3. Our Approach to Sentence Boundary Disambiguation

If we had at our disposal entirely correct information on whether or not each word
preceding a period was an abbreviation and whether or not each capitalized word

292


Mikheev

Periods, Capitalized Words, etc.

...<W C=RB>soon</W><W C=’.’>.</W> <W A=Y C=NNP>Mr</W><W C=A>.</W>...

...<W C=VBN>said</W> <W C=NNP>Mr</W><W C=A>.</W>

<W A=Y C=NNP>Brown</W>...

...<W C=’,’>,</W> <W C=NNP>Tex</W><W C=’*’>.</W>

<W A=Y C=JJP>American</W>...

Figure 1
Example of the new tokenization and markup generated from the Penn Treebank format.
Tokens are represented as XML elements W, where the attribute C holds POS information.
Proper names are tagged as NNP, NNPS and JJP. Periods are tagged as . (fullstop), A (part of
abbreviation), * (a fullstop and part of abbreviation at the same time). Ambiguously
capitalized words are marked with A = Y.

that follows a period was a proper name, we could apply a very simple set of rules
to disambiguate sentence boundaries:

•

•

•

•

If a period follows a nonabbreviation, it is sentence terminal (.).

If a period follows an abbreviation and is the last token in a text passage
(paragraph, document, etc.), it is sentence terminal and part of the
abbreviation (*).

If a period follows an abbreviation and is not followed by a capitalized
word, it is part of the abbreviation and is not sentence terminal (A).

If a period follows an abbreviation and is followed by a capitalized word
that is not a proper name, it is sentence terminal and part of the
abbreviation (*).

It is a trivial matter to extend these rules to allow for brackets and quotation marks
between the period and the following word. To handle other sentence termination
punctuation such as question and exclamation marks and semicolons, this rule set
also needs to include corresponding rules. The entire rule set for sentence boundary
disambiguation that was used in our experiments is listed in Appendix A.

3.1 Ideal Case: Upper Bound for Our SBD Approach
The estimates from the Brown corpus and the WSJ corpus (section 3) show that the
application of the SBD rule set described above together with the information on
abbreviations and proper names marked up in the corpora produces very accurate
results (error rate less than 0.0001%), but it leaves unassigned the outcome of the case
in which an abbreviation is followed by a proper name. This is a truly ambiguous case,
and to deal with this situation in general, one should encode detailed information
about the words participating in such contexts. For instance, honoriﬁc abbreviations
such as Mr. or Dr. when followed by a proper name almost certainly do not end a
sentence, whereas the abbreviations of U.S. states such as Mo., Cal., and Ore., when
followed directly by a proper name, most likely end a sentence. Obviously encoding
this kind of information into the system requires detailed analysis of the domain lexica,
is not robust to unseen abbreviations, and is labor intensive.

To make our method robust to unseen words, we opted for a crude but simple
solution. If such ambiguous cases are always resolved as “not sentence boundary” (A),
this produces, by our measure, an error rate of less than 3%. Estimates from the Brown

293


Computational Linguistics

Volume 28, Number 3

Table 1
Estimates of the upper and lower bound error rates on the SBD task for our method. Three
estimated categories are sentence boundaries, ambiguously capitalized words, and
abbreviations.

Brown Corpus

WSJ Corpus

SBD

Amb. Cap. Abbreviations SBD

Amb. Cap. Abbreviations

Number of
resolved instances

59,539 58,957

4,657

53,043 54,537

16,317

A Upper Bound:

0.01% 0.0%

0.0%

0.13% 0.0%

0.0%

All correct proper
names
All correct abbrs.

B Lower Bound:
Lookup proper
names
Guessed abbrs.
C Lookup proper

names
All correct abbrs.

2.00% 7.4%

10.8%

4.10% 15.0%

9.6%

1.20% 7.4%

0.0%

2.34% 15.0%

0.0%

D All correct proper 0.45% 0.0%

10.8%

1.96% 0.0%

9.6%

names
Guessed abbrs.

corpus and the WSJ corpus showed that such ambiguous cases constitute only 5–7%
of all potential sentence boundaries. This translates into a relatively small impact of
the crude strategy on the overall error rate on sentence boundaries. This impact was
measured at 0.01% on the Brown corpus and at 0.13% on the WSJ corpus, as presented
in row A of Table 1. Although this overly simplistic strategy extracts a small penalty
from the performance, we decided to use it because it is very general and independent
of domain-speciﬁc knowledge.

The SBD handling strategy described above is simple, robust, and well perform-
ing, but it relies on the assumption that we have entirely correct information about
abbreviations and proper names, as can be seen in row A of the table. The main dif-
ﬁculty is that when dealing with real-world texts, we have to identify abbreviations
and proper names ourselves. Thus estimates based on the application of our method
when using 100% correctly disambiguated capitalized words and abbreviations can be
considered as the upper bound for the SBD approach, that is, the top performance we
can achieve.

3.2 Worst Case: Lower Bound for Our SBD Approach
We can also estimate the lower bound for this approach applying very simple strategies
to the identiﬁcation of proper names and abbreviations.

The simplest strategy for deciding whether or not a capitalized word in an ambigu-
ous position is a proper name is to apply a lexical-lookup strategy (possibly enhanced
with a morphological word guesser, e.g., Mikheev [1997]). Using this strategy, words
not listed as known common words for a language are usually marked as proper
names. The application of this strategy produced a 7.4% error rate on the Brown
corpus and a 15% error rate on the WSJ corpus. The difference in error rates can be
explained by the observation that the WSJ corpus contains a higher percentage of orga-
nization names and person names, which often coincide with common English words,

294


Mikheev

Periods, Capitalized Words, etc.

and it contains more words in titles with all important words capitalized, which we
also consider as ambiguously capitalized.

The simplest strategy for deciding whether a word that is followed by a period
is an abbreviation or a regular word is to apply well-known heuristics based on the
observation that single-word abbreviations are short and normally do not include
vowels (Mr., Dr., kg.). Thus a word without vowels can be guessed to be an abbreviation
unless it is written in all capital letters and can stand for an acronym or a proper name
(e.g., BBC). A span of single letters separated by periods forms an abbreviation too
(e.g., Y.M.C.A.). A single letter followed by a period is also a very likely abbreviation.
There is also an additional heuristic that classiﬁes as abbreviations short words (with
length less than ﬁve characters) that are followed by a period and then by a comma, a
lower-cased word, or a number. All other words are considered to be nonabbreviations.
These heuristics are reasonably accurate. On the WSJ corpus they misrecognized
as abbreviations only 0.2% of tokens. On the Brown corpus the misrecognition rate was
signiﬁcantly higher: 1.6%. The major source for these errors were single letters that
stand for mathematical symbols in the scientiﬁc subcorpora of the Brown Corpus (e.g.,
point T or triangle F). The major shortcoming of these abbreviation-guessing heuristics,
however, comes from the fact that they failed to identify about 9.5% of abbreviations.
This brings the overall error rate of the abbreviation-guessing heuristics to about 10%.
Combining the information produced by the lexical-lookup approach to proper
name identiﬁcation with the abbreviation-guessing heuristics feeding the SBD rule set
gave us a 2.0% error rate on the Brown corpus and 4.1% on the WSJ corpus on the
SBD task. This can be interpreted as the lower bound to our SBD approach. Here we
see how errors in the identiﬁcation of proper names and abbreviations propagated
themselves into errors on sentence boundaries. Row B of Table 1 displays a summary
for the lower-bound results.

3.3 Major Findings
We also measured the importance of each of the two knowledge sources (abbreviations
and proper names) separately. First, we applied the SBD rule set when all abbreviations
were correctly identiﬁed (using the information presented in the corpus) but applying
the lexical lookup strategy to proper-name identiﬁcation (row C of Table 1). Then, we
applied the SBD rule set when all proper names were correctly identiﬁed (using the
information presented in the corpus) but applying the guessing heuristics to handle
abbreviations (row D of the table). In general, when a knowledge source returned
100% accurate information this signiﬁcantly improved performance on the SBD task
measured against the lower-bound error rate. We also see that proper names have a
higher impact on the SBD task than abbreviations.

Since the upper bound of our SBD approach is high and the lower bound is far
from being acceptable, our main strategy for sentence boundary disambiguation will be to
invest in the disambiguation of capitalized words and abbreviations that then feed our SBD
rule set.

4. Document-Centered Approach to Proper Name and Abbreviation Handling

As we discussed above, virtually any common word can potentially act as a proper
name or part of a multiword proper name. The same applies to abbreviations: there is
no ﬁxed list of abbreviations, and almost any short word can be used as an abbrevia-
tion. Fortunately, there is a mitigating factor too: important words are typically used
in a document more than once and in different contexts. Some of these contexts create

295


Computational Linguistics

Volume 28, Number 3

ambiguity, but some do not. Furthermore, ambiguous words and phrases are usually
unambiguously introduced at least once in the text unless they are part of common
knowledge presupposed to be possessed by the readers.

This observation can be applied to a broader class of tasks. For example, people
are often referred to by their surnames (e.g., Black) but are usually introduced at least
once in the text either with their ﬁrst name (John Black) or with their title/profession
afﬁliation (Mr. Black, President Bush), and it is only when their names are common
knowledge that they do not need an introduction (e.g., Castro, Gorbachev). Thus our
suggestion is to look at the unambiguous usages of the words in question in the entire document.
In the case of proper name identiﬁcation, we are not concerned with the semantic
class of a name (e.g., whether it is a person’s name or a location), but rather we simply
want to distinguish whether a capitalized word in a particular occurrence acts as a
proper name (or part of a multiword proper name). If we restrict our scope to a single
sentence, we might ﬁnd that there is just not enough information to make a reliable
decision. For instance, Riders in the sentence Riders rode all over the green is equally likely
to be a proper noun, a plural proper noun, or a plural common noun. But if in the
same text we ﬁnd John Riders, this sharply increases the likelihood that the proper noun
interpretation is the correct one, and conversely if we ﬁnd many riders, this suggests
the plural-noun interpretation.

The above reasoning can be summarized as follows: if we detect that a word is
used capitalized in an unambiguous context, this increases the chances that this word
acts as a proper name in ambiguous positions in the same document. And conversely
if a word is seen only lower-cased, this increases the chances that it should be treated
as a common word even when used capitalized in ambiguous positions in the same
document. (This, of course, is only a general principle and will be further elaborated
elsewhere in the article.)

The same logic applies to abbreviations. Although a short word followed by a
period is a potential abbreviation, the same word occurring in the same document
in a different context can be unambiguously classiﬁed as a regular word if it is used
without a trailing period, or it can be unambiguously classiﬁed as an abbreviation if
it is used with a trailing period and is followed by a lower-cased word or a comma.
This information gives us a better chance of assigning these potential abbreviations
correctly in nonobvious contexts.

We call such style of processing a document-centered approach (DCA), since in-
formation for the disambiguation of an individual word token is derived from the
entire document rather than from its immediate local context. Essentially the system
collects suggestive instances of usage for target words from each document under
processing and applies this information on the ﬂy to the processing of the document,
in a manner similar to instance-based learning. This differentiates DCA from the tradi-
tional corpus-based approach, in which learning is applied prior to processing, which
is usually performed with supervision over multiple documents of the training corpus.

5. Building Support Resources

Our method requires only four word lists. Each list is a collection of words that belong
to a single type, but at the same time, a word can belong to multiple lists. Since we
have four lists, we have four types:

common word (as opposed to proper name)

common word that is a frequent sentence starter

•

•

296


Mikheev

•

•

frequent proper name

abbreviation (as opposed to regular word)

Periods, Capitalized Words, etc.

These four lists can be acquired completely automatically from raw (unlabeled) texts.
For the development of these lists we used a collection of texts of about 300,000 words
derived from the New York Times (NYT) corpus that was supplied as training data for
the 7th Message Understanding Conference (MUC-7) (Chinchor 1998). We used these
texts because the approach described in this article was initially designed to be part
of a named-entity recognition system (Mikheev, Grover, and Moens 1998) developed
for MUC-7. Although the corpus size of 300,000 words can be seen as large, the fact
that this corpus does not have to be annotated in any way and that a corpus of similar
size can be easily collected from on-line sources (including the Internet) makes this
resource cheap to obtain.

The ﬁrst list on which our method relies is a list of common words. This list
includes common words for a given language, but no supplementary information such
as POS or morphological information is required to be present in this list. A variety
of such lists for many languages are already available (e.g., Burnage 1990). Words in
such lists are usually supplemented with morphological and POS information (which
is not required by our method). We do not have to rely on pre-existing resources,
however. A list of common words can be easily obtained automatically from a raw
(unannotated in any way) text collection by simply collecting and counting lower-
cased words in it. We generated such list from the NYT collection. To account for
potential spelling and capitalization errors, we included in the list of common words
only words that occurred lower-cased at least three times in the NYT texts. The list
of common words that we developed from the NYT collection contained about 15,000
English words.

The second list on which our method relies is a frequent-starters list, a list of
common words most frequently used in sentence-starting positions. This list can also
be obtained completely automatically from an unannotated corpus by applying the
lexical-lookup strategy. As discussed in Section 3.2, this strategy performs with a
7–15% error rate. We applied the list of common words over the NYT text collec-
tion to tag capitalized words in sentence-starting positions as common words and as
proper names: if a capitalized word was found in the list of common words, it was
tagged as a common word: otherwise it was tagged as a proper name. Of course, such
tagging was far from perfect, but it was good enough for our purposes. We included in
the frequent-starters list only the 200 most frequent sentence-starting common words.
This was more than a safe threshold to ensure that no wrongly tagged words were
added to this list. As one might predict, the most frequent sentence-starting common
word was The. This list also included some adverbs, such as However, Suddenly, and
Once; some prepositions, such as In, To, and By; and even a few verbs: Let, Have, Do, etc.
The third list on which our method relies is a list of single-word proper names that
coincide with common words. For instance, the word Japan is much more likely to be
used as a proper name (name of a country) rather than a verb, and therefore it needs to
be included in this list. We included in the proper name list 200 words that were most
frequently seen in the NYT text collection as single capitalized words in unambiguous
positions and that at the same time were present in the list of common words. For
instance, the word The can frequently be seen capitalized in unambiguous positions,
but it is always followed by another capitalized word, so we do not count it as a
candidate. On the other hand the word China is often seen capitalized in unambiguous
positions where it is not preceded or followed by other capitalized words. Since china

297


Computational Linguistics

Volume 28, Number 3

Table 2
Error rates for different combinations of the abbreviation identiﬁcation methods, including
combinations of guessing heuristics (GH), lexical lookup (LL), and the document-centered
approach (DCA).

Abbreviation Identiﬁcation Method WSJ

Brown

A GH
LL
B
C GH + LL
D GH + DCA
E GH + DCA + LL

9.6% 10.8%
12.6% 11.9%
2.1%
8.9%
1.2%

1.2%
6.6%
0.8%

is also listed among common words and is much less frequently used in this way, we
include it in the proper name list.

The fourth list on which our method relies is a list of known abbreviations.
Again, we induced this list completely automatically from an unannotated corpus.
We applied the abbreviation-guessing heuristics described in Section 6 to our NYT
text collection and then extracted the 270 most frequent abbreviations: all abbrevi-
ations that appeared ﬁve times or more. This list included honoriﬁc abbreviations
(Mr, Dr), corporate designators (Ltd, Co), month name abbreviations (Jan, Feb), ab-
breviations of names of U.S. states (Ala, Cal), measure unit abbreviations (ft, kg), etc.
Although we described these abbreviations in groups, this information was not en-
coded in the list; the only information this list provides is that a word is a known
abbreviation.

Among these four lists the ﬁrst three reﬂect general language regularities and
usually do not require modiﬁcation for handling texts from a new domain. The ab-
breviation list, however, is much more domain dependent and for better performance
needs to be reinduced for a new domain. Since the compilation of all four lists does not
require data preannotated in any way, it is very easy to specialize the above-described
lists to a particular domain: we can simply rebuild the lists using a domain-speciﬁc
corpus. This process is completely automatic and does not require any human labor
apart from collecting a raw domain-speciﬁc corpus. Since all cutoff thresholds that
we applied here were chosen by intuition, however, different domains might require
some new settings.

6. Recognizing Abbreviations

The answer to the question of whether or not a particular word token is an abbreviation
or a regular word largely solves the sentence boundary problem. In the Brown corpus
92% of potential sentence boundaries come after regular words. The WSJ corpus is
richer with abbreviations, and only 83% of sentences in that corpus end with a regular
word followed by a period. In Section 3 we described the heuristics for abbreviation
guessing and pointed out that although these heuristics are reasonably accurate, they
fail to identify about 9.5% of abbreviations. Since unidentiﬁed abbreviations are then
treated as regular words, the overall error rate of the guessing heuristics was measured
at about 10% (row A of Table 2). Thus, to improve this error rate, we need ﬁrst of all
to improve the coverage of the abbreviation-handling strategy.

A standard way to do this is to use the guessing heuristics in conjunction with a
list of known abbreviations. We decided to use the list of 270 abbreviations described
in Section 5. First we applied only the lexical-lookup strategy to our two corpora (i.e.,

298


Mikheev

Periods, Capitalized Words, etc.

only when a token was found in the list of 270 known abbreviations was it marked
as an abbreviation). This gave us an unexpectedly high error rate of about 12%, as
displayed in row B of Table 2. When we investigated the reason for the high error
rate, we found that the majority of single letters and spans of single letters sepa-
rated by periods (e.g. Y.M.C.A.) found in the Brown corpus and the WSJ corpus were
not present in our abbreviation list and therefore were not recognized as abbrevia-
tions.

Such cases, however, are handled well by the abbreviation-guessing heuristics.
When we applied the abbreviation list together with the abbreviation-guessing heuris-
tics (row C of Table 2), this gave us a very strong performance on the WSJ corpus:
an error rate of 1.2%. On the Brown corpus, the error rate was higher: 2.1%. This can
be explained by the fact that we collected our abbreviation list from a corpus of news
articles that is not too dissimilar to the texts in the WSJ corpus and thus, this list con-
tained many abbreviations found in that corpus. The Brown corpus, in contrast, ranges
across several different domains and sublanguages, which makes it more difﬁcult to
compile a list from a single corpus to cover it.

6.1 Unigram DCA
The abbreviation-guessing heuristics supplemented with a list of abbreviations are
accurate, but they still can miss some abbreviations. For instance, if an abbreviation
like sec or Okla. is followed by a capitalized word and is not listed in the list of
abbreviations, the guessing heuristics will not uncover them. We also would like to
boost the abbreviation handling with a domain-independent method that enables the
system to function even when the abbreviation list is not much of a help. Thus, in
addition to the list of known abbreviations and the guessing heuristics, we decided to
apply the DCA as described below.

Each word of length four characters or less that is followed by a period is treated as
a potential abbreviation. First, the system collects unigrams of potential abbreviations
in unambiguous contexts from the document under processing. If a potential abbre-
viation is used elsewhere in the document without a trailing period, we can conclude
that it in fact is not an abbreviation but rather a regular word (nonabbreviation). To
decide whether a potential abbreviation is really an abbreviation, we look for contexts
in which it is followed by a period and then by a lower-cased word, a number, or a
comma.

For instance, the word Kong followed by a period and then by a capitalized word
cannot be safely classiﬁed as a regular word (nonabbreviation), and therefore it is a
potential abbreviation. But if in the same document we detect a context lived in Hong
Kong in 1993, this indicates that Kong in this document is normally written without
a trailing period and hence is not an abbreviation. Having established that, we can
apply this information to nonevident contexts and classify Kong as a regular word
throughout the document. However, if we detect a context such as Kong., said, this
indicates that in this document, Kong is normally written with a trailing period and
hence is an abbreviation. This gives us grounds for classifying Kong as an abbreviation
in all its occurrences within the same document.

6.2 Bigram DCA
The DCA relies on the assumption that there is a consistency in writing within the
same document. Different authors can write Mr or Dr with or without a trailing period,
but we assume that the same author (the author of a particular document) writes
it in the same way consistently. A situation can arise, however, in which the same
potential abbreviation is used as a regular word and as an abbreviation within the same

299


Computational Linguistics

Volume 28, Number 3

document. This is usually the case when an abbreviation coincides with a regular word,
for example Sun. (meaning Sunday) and Sun (the name of a newspaper). To tackle
this problem, the system can collect from a document not only unigrams of potential
abbreviations in unambiguous contexts but also their bigrams with the preceding
word. Of course, as in the case with unigrams, the bigrams are collected on the ﬂy
and completely automatically.

For instance, if the system ﬁnds a context vitamin C is, it stores the bigram vita-
min C and the unigram C with the information that C is a regular word. If in the
same document the system also detects a context John C. later said, it stores the bi-
gram John C and the unigram C with the information that C is an abbreviation. Here
we have conﬂicting information for the word C: it was detected to act as a regu-
lar word and as an abbreviation within the same document, so there is not enough
information to resolve ambiguous cases purely using the unigram. Some cases, how-
ever, can still be resolved on the basis of the bigrams. The system will assign C as a
regular word (nonabbreviation) in an ambiguous context such as vitamin C. Research
because of the stored vitamin C bigram. Obviously from such a short context, it is
difﬁcult even for a human to make a conﬁdent decision, but the evidence gathered
from the entire document can inﬂuence this decision with a high degree of conﬁ-
dence.

6.3 Resulting Approach
When neither unigrams nor bigrams can help to resolve an ambiguous context for a
potential abbreviation, the system decides in favor of the more frequent category for
that abbreviation. If the word In was detected to act as a regular word (preposition) ﬁve
times in the current document and two times as abbreviation (for the state Indiana), in
a context in which neither of the bigrams collected from the document can be applied,
In is assigned as a regular word (nonabbreviation). The last-resort strategy is to assign
all nonresolved cases as nonabbreviations.

Row D of Table 2 shows the results when we applied the abbreviation-guessing
heuristics together with the DCA. On the WSJ corpus, the DCA reduced the error rate
of the guessing heuristics alone (row A) by about 30%; on the Brown corpus its impact
was somewhat smaller, about 18%. This can be explained by the fact that abbreviations
in the WSJ corpus have a much higher repetition rate, which is very important for
the DCA.

We also applied the DCA together with the lexical lookup and the guessing heuris-
tics. This reduced the error rate on abbreviation identiﬁcation by about 30% in com-
parison with the list and guessing heuristics conﬁguration, as can be seen in row E of
Table 2.

7. Disambiguating Capitalized Words

The second key task of our approach is the disambiguation of capitalized words that
follow a potential sentence boundary punctuation sign. Apart from being an important
component in the task of text normalization, information about whether or not a
capitalized word that follows a period is a common word is crucial for the SBD task,
as we showed in Section 3. We tackle capitalized words in a similar fashion as we
tackled the abbreviations: through a document-centered approach that analyzes on
the ﬂy the distribution of ambiguously capitalized words in the entire document. This
is implemented as a cascade of simple strategies, which were brieﬂy described in
Mikheev (1999).

300


Mikheev

Periods, Capitalized Words, etc.

7.1 The Sequence Strategy
The ﬁrst DCA strategy for the disambiguation of ambiguous capitalized words is to
explore sequences of words extracted from contexts in which the same words are used
unambiguously with respect to their capitalization. We call this the sequence strategy.
The rationale behind this strategy is that if there is a phrase of two or more capitalized
words starting from an unambiguous position (e.g., following a lower-cased word),
the system can be reasonably conﬁdent that even when the same phrase starts from an
unreliable position (e.g., after a period), all its words still have to be grouped together
and hence are proper nouns. Moreover, this applies not just to the exact replication of
the capitalized phrase, but to any partial ordering of its words of size two characters
or more preserving their sequence.

For instance, if a phrase Rocket Systems Development Co. is found in a document
starting from an unambiguous position (e.g., after a lower-cased word, a number, or a
comma), the system collects it and also generates its partial-order subphrases: Rocket
Systems, Rocket Systems Co., Rocket Co., Systems Development, etc. If then in the same
document Rocket Systems is found in an ambiguous position (e.g., after a period), the
system will assign the word Rocket as a proper noun because it is part of a multiword
proper name that was seen in the unambiguous context.

A span of capitalized words can also internally include alpha-numerals, abbrevia-
tions with internal periods, symbols, and lower-cased words of length three characters
or shorter. This enables the system to capture phrases like A & M and The Phantom of
the Opera. Partial orders from such phrases are generated in a similar way, but with
the restriction that every generated subphrase should start and end with a capitalized
word.

The sequence strategy can also be applied to disambiguate common words. Since
in the case of common words the system cannot determine boundaries of a phrase,
only bigrams of the lower-cased words with their following words are collected from
the document. For instance, from a context continental suppliers of Mercury, the sys-
tem collects three bigrams: continental suppliers, suppliers of, and of Mercury. When the
system encounters the phrase Continental suppliers after a period, it can now use the
information that in the previously stored bigram continental suppliers, the word token
continental was written lower-cased and therefore was unambiguously used as a com-
mon word. On this basis the system can assign the ambiguous capitalized word token
Continental as a common word.

Row A of Table 3 displays the results obtained in the application of the sequence
strategy to the Brown corpus and the WSJ corpus. The sequence strategy is extremely
useful when we are dealing with names of organizations, since many of them are
multiword phrases composed of common words. For instance, the words Rocket and
Insurance can be used both as proper names and common words within the same
document. The sequence strategy maintains contexts of the usages of such words
within the same document, and thus it can disambiguate such usages in the ambiguous
positions matching surrounding words. And indeed, the error rate of this strategy
when applied to proper names was below 1%, with coverage of about 9–12%.

For tagging common words the sequence strategy was also very accurate (error
rate less than 0.3%), covering 17% of ambiguous capitalized common words on the
WSJ corpus and 25% on the Brown corpus. The higher coverage on the Brown corpus
can be explained by the fact that the documents in that corpus are in general longer
than those in the WSJ corpus, which enables more word bigrams to be collected from
a document.

Dual application of the sequence strategy contributes to its robustness against po-
tential capitalization errors in the document. The negative evidence (not proper name)

301


Computational Linguistics

Volume 28, Number 3

Table 3
First part: Error rates of different individual strategies for capitalized-word disambiguation.
Second part: Error rates of the overall cascading application of the individual strategies.

Strategy

Word Class

Error Rate

Coverage

WSJ

Brown WSJ

Brown

A Sequence strategy
Sequence strategy
Frequent-list lookup strategy
Frequent-list lookup strategy

B

C Single-word assignment strategy Proper Names

Single-word assignment strategy Common Words

Proper Names
Common Words
Proper Names
Common Words

0.12% 0.97%
0.28% 0.21%
0.49% 0.16%
0.21% 0.14%
3.18% 1.96%
6.51% 2.87%
Proper/Common 1.10% 0.76%
Proper/Common 4.88% 2.83% 100.0% 100.0%

12.6%
8.82%
17.68% 26.5%
2.62%
6.54%
64.62% 61.20%
18.77% 34.13%
4.78%
84.12% 91.76%

3.07%

D Cascading DCA
E Cascading DCA

and lexical lookup

is used together with the positive evidence (proper name) and blocks assignment when
conﬂicts are found. For instance, if the system detects a capitalized phrase The President
in an unambiguous position, then the sequence strategy will treat the word the as part
of the proper name The President even when this phrase follows a period. If in the
same document, however, the system detects alternative evidence (e.g., the President,
where the is not part of the proper name), it then will block as unsafe the assignment
of The as a proper name in ambiguous usages of The President.

7.2 Frequent-List Lookup Strategy
The frequent-list lookup strategy applies lookup of ambiguously capitalized words
in two word lists. The ﬁrst list contains common words that are frequently found
in sentence-starting positions, and the other list contains the most frequent proper
names. Both these lists can be compiled completely automatically, as explained in
section 5. Thus, if an ambiguous capitalized word is found in the list of frequent
sentence-starting common words, it is assigned as a common word, and if it is found
in the list of frequent proper names, it is assigned as a proper name. For instance,
the word token The when used after a period will be recognized as a common word,
because The is a frequent sentence-starting common word. The Word token Japan in a
similar context will be recognized as a proper name, because Japan is a member of the
frequent-proper-name list.

Note, however, that this strategy is applied after the sequence strategy and thus, a
word listed in one of the lists will not necessarily be marked according to its list class.
The list lookup assignment is applied only to the ambiguously capitalized words that
have not been handled by the sequence strategy.

Row B of Table 3 displays the results of the application of the frequent-list lookup
strategy to the Brown corpus and the WSJ corpus. The frequent-list lookup strategy
produced an error rate of less than 0.5%. A few wrong assignments came from phrases
like Mr. A and Mrs. Someone and words in titles like I’ve Got a Dog, where A, Someone,
and I were recognized as common words although they were tagged as proper nouns
in the text. The frequent-list lookup strategy is not very effective for proper names,
where it covered under 7% of candidates in the Brown corpus and under 3% in the
WSJ corpus, but it is extremely effective for common words: it covered over 60% of
ambiguous capitalized common words.

302


Mikheev

Periods, Capitalized Words, etc.

7.3 Single-Word Assignment
The sequence strategy is accurate, but it covers only 9–12% of proper names in ambigu-
ous positions. The frequent-list lookup strategy is mostly effective for common words.
To boost the coverage on the proper name category, we introduced another DCA
strategy. We call this strategy single-word assignment, and it can be summarized as
follows: if a word in the current document is seen capitalized in an unambiguous po-
sition and at the same time it is not used lower-cased anywhere in the document, this
word in this particular document is very likely to stand for a proper name even when
used capitalized in ambiguous positions. And conversely, if a word in the current
document is used only lower-cased (except in ambiguous positions), it is extremely
unlikely that this word will act as a proper name in an ambiguous position and thus,
such a word can be marked as a common word.

Note that by the time single-word assignment is implemented, the sequence strat-
egy and the frequent-list lookup strategy have been already applied and all high-
frequency sentence-initial words have been assigned. This ordering is important, be-
cause even if a high-frequency common word is observed in a document only as a
proper name (usually as part of a multiword proper name), it is still not safe to mark
it as a proper name in ambiguous positions.

Row C of Table 3 displays the results of the application of the single-word assign-
ment strategy to the Brown corpus and the WSJ corpus. The single-word assignment
strategy is useful for proper-name identiﬁcation: although it is not as accurate as the
sequence strategy, it still produces a reasonable error rate at the same time boosting the
coverage considerably (19–34%). On common words this method is not as effective,
with an error rate as high as 6.61% on the WSJ corpus and a coverage below 5%.

The single-word-assignment strategy handles well the so-called unknown-word
problem, which arises when domain-speciﬁc lexica are missing from a general vocab-
ulary. Since our system is not equipped with a general vocabulary but rather builds a
document-speciﬁc vocabulary on the ﬂy,” important domain-speciﬁc words are iden-
tiﬁed and treated similarly to all other words.

A generally difﬁcult case for the single-word assignment strategy arises when a
word is used both as a proper name and as a common word in the same document, es-
pecially when one of these usages occurs only in an ambiguous position. For instance,
in a document about steel, the only occurrence of Steel Company happened to start
a sentence. This produced an erroneous assignment of the word Steel as a common
word. Another example: in a document about the Acting Judge, the word acting in a
sentence Acting on behalf. . . was wrongly classiﬁed as a proper name. These difﬁculties,
however, often are compensated for by the sequence strategy, which is applied prior
to the single-word assignment strategy and tackles such cases using n-grams of words.

7.4 Quotes, Brackets, and “After Abbr.” Heuristic
Capitalized words in quotes and brackets do not directly contribute to our primary
task of sentence boundary disambiguation, but they still present a case of ambiguity
for the task of capitalized-word disambiguation. To tackle them we applied two simple
heuristics:

•

•

If a single capitalized word is used in quotes or brackets it is a proper
noun (e.g., John (Cool) Lee).

If there is a lowercased word, a number, or a comma that is followed by
an opening bracket and then by a capitalized word, this capitalized word
is a proper noun (e.g., . . . happened (Moscow News reported yesterday) but. . . ).

303


Computational Linguistics

Volume 28, Number 3

These heuristics are reasonably accurate: they achieved under 2% error rate on our
two test corpora, but they covered only about 6–7% of proper names.

When we studied the distribution of capitalized words after capitalized abbrevi-
ations, we uncovered an interesting empirical fact. A capitalized word that follows
a capitalized abbreviation is almost certainly a proper name unless it is listed in the
list of frequent sentence-starting common words (i.e., it is not The, However, etc.). The
error rate of this heuristic is about 0.8% and, not surprisingly, in 99.5% of cases the
abbreviation and the following proper name belonged to the same sentence. Naturally,
the coverage of this “after abbr.” heuristic depends on the proportion of capitalized
abbreviations in the text. In our two corpora this heuristic disambiguated about 20%
of ambiguous capitalized proper names.

7.5 Tagging Proper Names: The Overall Performance
In general, the cascading application of the above-described strategies achieved an
error rate of about 1%, but it left unclassiﬁed about 9% of ambiguous capitalized
words in the Brown corpus and 15% of such words in the WSJ corpus. Row D of
Table 3 displays the results of the application of the cascading application of the
capitalized-word disambiguation strategies to the Brown corpus and the WSJ corpus.
For the proper-name category, the most productive strategy was single-word as-
signment, followed by the “after abbr.” strategy, and then the sequence strategy. For
common words, the most productive was the frequent-list lookup strategy, followed
by the sequence strategy.

Since our system left unassigned 10–15% of ambiguous capitalized words, we have
to decide what to do with them. To keep our system simple and domain independent,
we opted for the lexical-lookup strategy that we evaluated in Section 3. This strategy,
of course, is not very accurate, but it is applied only to the unassigned words. Row E
of Table 3 displays the results of applying the lexical-lookup strategy after the DCA
methods. We see that the error rate went up in comparison to the DCA-only method
by more than three times (2.9% on the Brown corpus and 4.9% on the WSJ corpus),
but no unassigned ambiguous capitalized words are left in the text.

8. Putting It All Together: Assigning Sentence Boundaries

After abbreviations have been identiﬁed and capitalized words have been classiﬁed
into proper names and common words, the system can carry out the assignments
of sentence boundaries using the SBD rule set described in Section 3 and listed in
Appendix A. This rule set makes use of the observation that if we have at our dis-
posal unambiguous (but not necessarily correct) information as to whether a particular
word that precedes a period is an abbreviation and whether the word that follows
this period is a proper name, then in mixed-case texts we can easily assign a pe-
riod (and other potential sentence termination punctuation) as a sentence break or
not.

The only ambiguous outcome is generated by the conﬁguration in which an ab-
breviation is followed by a proper name. We decided to handle this case by applying
a crude and simple strategy of always resolving it as “not sentence boundary.” On one
hand, this makes our method simple and robust, but on the other hand, it imposes
some penalty on its performance.

Row A of Table 4 summarizes the upper bound for our SBD approach: when we
have entirely correct information on the abbreviations and proper names, as explained
in Section 3.1. There the erroneous assignments come only from the crude treatment
of abbreviations that are followed by proper names.

304


Mikheev

Periods, Capitalized Words, etc.

Table 4
Error rates measured on the SBD, capitalized-word disambiguation, and abbreviation
identiﬁcation tasks achieved by different methods described in this article.

METHOD

Brown Corpus

WSJ Corpus

SBD Capitalized Abbreviations SBD Capitalized Abbreviations

words

0.01% 0.0%
A Upper bound
2.00% 7.40%
B Lower bound
0.20% 3.15%
C Best quoted
D DCA
0.28% 2.83%
E DCA (no abbreviations 0.65% 2.89%

lexicon)
F POS tagger
G POS tagger + DCA

0.25% 3.15%
0.20% 1.87%

0.0%
10.8%
—
0.8%
8.9%

1.2%
0.8%

words

0.13% 0.0%
4.10% 15.0%
0.50% 4.72%
0.45% 4.88%
1.41% 4.92%

0.39% 4.72%
0.31% 3.22%

0.0%
9.6%
—
1.2%
6.6%

2.1%
1.2%

Row B of Table 4 summarizes the lower-bound results. The lower bound for our
approach was estimated by applying the lexical-lookup strategy for capitalized-word
disambiguation together with the abbreviation-guessing heuristics to feed the SBD
rule set, as described in Section 3.2. Here we see a signiﬁcant impact of the infelicities
in the disambiguation of capitalized words and abbreviations on the performance of
the SBD rule set.

Row C of Table 4 summarizes the highest results known to us (for all three tasks)
produced by automatic systems on the Brown corpus and the WSJ corpus. State-of-the-
art machine learning and rule-based SBD systems achieve an error rate of 0.8–1.5%
measured on the Brown corpus and the WSJ corpus. The best performance on the
WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst
1997) with the Alembic system (Aberdeen et al. 1995): a 0.5% error rate. The best
performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who
trained a decision tree classiﬁer on a 25-million-word corpus. In the disambiguation of
capitalized words, the most widespread method is POS tagging, which achieves about
a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported
in Mikheev (2000). We are not aware of any studies devoted to the identiﬁcation of
abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ
corpus.

In row D of Table 4, we summarized our main results: the results obtained by the
application of our SBD rule set, which uses the information provided by the DCA to
capitalized word disambiguation applied together with lexical lookup (as described
in Section 7.5), and the abbreviation-handling strategy, which included the guessing
heuristics, the DCA, and the list of 270 abbreviations (as described in Section 6). As
can be seen in the table, the performance of this system is almost indistinguishable
from the best previously quoted results. On proper-name disambiguation, it achieved
a 2.83% error rate on the Brown corpus and a 4.88% error rate on the WSJ corpus.
On the SBD task, it achieved a 0.28% error rate on the Brown corpus and a 0.45%
error rate on the WSJ corpus. If we compare these results with the upper bound
for our SBD approach, we can see that the infelicities in proper-name and abbrevia-
tion identiﬁcation introduced an increase of about 0.3% in the error rate on the SBD
task.

To test the adaptability of our approach to a completely new domain, we applied
our system in a conﬁguration in which it was not equipped with the list of 270 abbre-

305


Computational Linguistics

Volume 28, Number 3

viations, since this list is the only domain-sensitive resource in our system. The results
for this conﬁguration are summarized in row E of Table 4. The error rate increase of
5–7% on the abbreviation handling introduced about a twofold increase in the SBD
error rate on the Brown corpus (a 0.65% error rate) and about a threefold increase on
the WSJ corpus (1.41%). But these results are still comparable to those of the majority
of currently used sentence splitters.

9. Detecting Limits for the DCA

Since our DCA method relies on the assumption that the words it tries to disam-
biguate occur multiple times in a document, its performance clearly should depend
on the length of the document: very short documents possibly do not provide enough
disambiguation clues, whereas very long documents possibly contain too many clues
that cancel each other.

As noted in Section 2.1, the average length of the documents in the Brown corpus
is about 2,300 words. Also, the documents in that corpus are distributed very densely
around their mean. Thus not much can be inferred about the dependency of the perfor-
mance of the method on document length apart from the observation that documents
2,000–3,000 words long are handled well by our approach. In the WSJ corpus, the aver-
age length of the document is about 500 words, and therefore we could investigate the
effect of short documents on the performance. We divided documents into six groups
according to their length and plotted the error rate for the SBD and capitalized-word
disambiguation tasks as well as the number of documents in a group, as shown in
Figure 2. As can be seen in the ﬁgure, short documents (50 words or less) have the
highest average error rate both for the SBD task (1.63) and for the capitalized-word
disambiguation task (5.25). For documents 50 to 100 words long, the error rate is still
a bit higher than normal, and for longer documents the error rate stabilizes around
1.5 for the capitalized-word disambiguation task and 0.3 for the SBD task. The error
rate on documents 2,000 words long and higher is almost identical to that registered
on the Brown corpus on documents of the same length.

Thus here we can conclude that the proposed approach tends not to be very
effective for documents shorter than 50 words (one to three sentences), but it handles
well documents up to 4,000 words long. Since our corpora did not contain documents
signiﬁcantly longer than that, we could not estimate whether or when the performance
of our method signiﬁcantly deteriorates on longer documents. We also evaluated the
performance of the method on different subcorpora of the Brown corpus: the most
difﬁcult subdomains proved to be scientiﬁc texts, spoken-language transcripts, and
journalistic texts, whereas ﬁction was the easiest genre for the system.

10. Incorporating DCA into a POS Tagger

To test our hypothesis that DCA can be used as a complement to a local-context
approach, we combined our main conﬁguration (evaluated in row D of Table 4) with
a POS tagger. Unlike other POS taggers, this POS tagger (Mikheev 2000) was also
trained to disambiguate sentence boundaries.

10.1 Training a POS Tagger
In our markup convention (Section 2), periods are tokenized as separate tokens re-
gardless of whether they stand for fullstops or belong to abbreviations. Consequently
a POS tagger can naturally treat them similarly to any other ambiguous words. There
is, however, one difference in the implementation of such a tagger. Normally, a POS

306


Mikheev

Periods, Capitalized Words, etc.

Number of Docs

600

400

200

Error Rate

3

2

1

c

x

o

c

x

o

c

c

x

o

x

o

c

x

o

c

x

o

c

Number of Documents

Cap.Word error rate

x

SBD error rate
o

Doc. Length

50

100

200

500

1000

2000

3000

Figure 2
Distribution of the error rate and the number of documents across the document length
(measured in word tokens) in the WSJ corpus.

tagger operates on text spans that form a sentence. This requires resolving sentence
boundaries before tagging. We see no good reason, however, why such text spans
should necessarily be sentences, since the majority of tagging paradigms (e.g., Hidden
Markov Model [HMM] [Kupiec 1992], Brill’s [Brill 1995a], and MaxEnt [Ratnaparkhi
1996]) do not attempt to parse an entire sentence and operate only in the local win-
dow of two to three tokens. The only reason why taggers traditionally operate on
the sentence level is that a sentence naturally represents a text span in which POS
information does not depend on the previous and following history.

This issue can be also addressed by breaking the text into short text spans at
positions where the previous tagging history does not affect current decisions. For
instance, a bigram tagger operates within a window of two tokens, and thus a se-
quence of word tokens can be terminated at an unambiguous word token, since this
unambiguous word token will be the only history used in tagging of the next token.
At the same time since this token is unambiguous, it is not affected by the history.
A trigram tagger operates within a window of three tokens, and thus a sequence of
word tokens can be terminated when two unambiguous words follow each other.

Using Penn Treebank with our tokenization convention (Section 2), we trained a
trigram HMM POS tagger. Words were clustered into ambiguity classes (Kupiec 1992)
according to the sets of POS tags they can take on. The tagger predictions were based
on the ambiguity class of the current word, abbreviation/capitalization information,

307


Computational Linguistics

Volume 28, Number 3

and trigrams of POS tags:

P(t1 . . . tnO1 . . . On) = argmax

i=n(cid:1)

i=1

P(Oi | ti) ∗ P(ti | ti−1ti−2ai−1)

where ti is a disambiguated POS tag of the ith word, ai is the abbreviation ﬂag of
the ith word, and Oi is the observation at the ith position, which in our case is
the ambiguity class the word belongs to, its capitalization, and its abbreviation ﬂag
(AmbClassi, ai, Capi). Since the abbreviation ﬂag of the previous word strongly inﬂu-
ences period disambiguation, it was included in the standard trigram model.

We decided to train the tagger with the minimum of preannotated resources. First,
we used 20,000 tagged words to “bootstrap” the training process, because purely un-
supervised techniques, at least for the HMM class of taggers, yield lower precision.
We also used our DCA system to assign capitalized words, abbreviations, and sen-
tence breaks, retaining only cases assigned by the strategies with an accuracy not less
than 99.8%. This was done because purely unsupervised techniques (e.g., Baum-Welch
[Baum and Petrie 1966] or Brill’s [Brill 1995b]) enable regularities to be induced for
word classes which contain many entries, exploiting the fact that individual words that
belong to a POS class occur in different ambiguity patterns. Counting all possible POS
combinations in these ambiguity patterns over multiple patterns usually produces the
right combinations as the most frequent. Periods as many other closed-class words
cannot be successfully covered by such technique.

After bootstrapping we applied the forward-backward (Baum-Welch) algorithm
(Baum and Petrie 1966) and trained our tagger in the unsupervised mode, that is, with-
out using the annotation available in the Brown corpus and the WSJ corpus. For
evaluation purposes we trained (and bootstrapped) our tagger on the Brown corpus
and applied it to the WSJ corpus and vice versa. We preferred this method to tenfold
cross-validation because it allowed us to produce only two tagging models instead of
twenty and also enabled us to test the tagger in harsher conditions, that is, when it is
applied to texts that are very distant from the ones on which it was trained.

The overall performance of the tagger was close to 96%, which is a bit lower
than the best quoted results. This can be accounted for by the fact that training and
evaluation were performed on two very different text corpora, as explained above.
The performance of the tagger on our target categories (periods and proper names)
was very close to that of the DCA method, as can be seen in row F of Table 4.

10.2 POS Tagger and the DCA
We felt that the DCA method could be used as a complement to the POS tagger, since
these techniques employ different types of information: in-document distribution and
local context. Thus, a hybrid system can deliver at least two advantages. First, 10–15%
of the ambiguous capitalized words unassigned by the DCA can be assigned using
a standard POS-tagging method based on the local syntactic context rather than the
inaccurate lexical-lookup approach. Second, the local context can correct some of the
errors made by the DCA.

To implement this hybrid approach we incorporated the DCA system into the
POS tagger. We modiﬁed the tagger model by incorporating the DCA predictions
using linear interpolation:

P(combined) = λ ∗ P(tagger) + (1 − λ) ∗ P(DCA Strategy)

where P(DCA Strategy) is the accuracy of a speciﬁc DCA strategy and P(tagger ) is the
probability assigned by the tagger’s model. Although it was possible to estimate an

308


Mikheev

Periods, Capitalized Words, etc.

optimal value for λ from the tagged corpus, we decided simply to set it to be 0.5 (i.e.,
giving similar weight to both sources of information). Instead of using the SBD rule
set described in Section 3, in this conﬁguration, period assignments were handled by
the tagger’s model.

Row G of Table 4 displays the results of the application of the hybrid system. We
see an improvement on proper-name recognition in comparison to the DCA or POS-
tagging approaches (rows D and F) by about a 30–40% cut in the error rate: an overall
error rate of 1.87% on the Brown corpus and of 3.22% on the WSJ corpus. In turn this
enabled better tagging of sentence boundaries: a 0.20% error rate on the Brown corpus
and a 0.31% error rate on the WSJ corpus, which corresponds to about a 20% cut in
the error rate in comparison to the DCA or the POS-tagging approaches alone.

Thus, although for applications that rely on POS tagging it probably makes more
sense to have a single system that assigns both POS tags and sentence boundaries,
there is still a clear beneﬁt in using the DCA method because

•

•

•

•

the DCA method incorporated into the POS tagger signiﬁcantly reduced
the error rate on the target categories (periods and proper names).

the DCA method is domain independent, whereas taggers usually need
to be trained for each speciﬁc domain to obtain best results.

the DCA system was used in resource preparation for training the tagger.

the DCA system is signiﬁcantly faster than the tagger, does not require
resource development, and for tasks that do not require full POS
information, it is a preferable solution.

So in general, the DCA method can be seen as an enhancer for a POS tagger and
also as a lightweight alternative to such a tagger when full POS information is not
required.

11. Further Experiments

11.1 The Cache Extension
One of the features of the method advocated in this article is that the system collects
suggestive instances of usage for target words from each document, then applies this
information during the second pass through the document (actual processing), and
then “forgets” what it has learned before handling another document. The main rea-
son for not carrying over the information that has been inferred from one document
to process another document is that in general we do not know whether this new
document comes from the same corpus as the ﬁrst document, and thus the regular-
ities that have been identiﬁed in the ﬁrst document might not be useful, but rather
harmful, when applied to that new document. When we are dealing with documents
of reasonable length, this “forgetful” behavior does not matter much, because such
documents usually contain enough disambiguation clues. As we showed in Section 8,
however, when short documents of one to three sentences are being processed, quite
often there are not enough disambiguation clues within the document itself, which
leads to inferior performance.

To improve the performance on short documents, we introduced a special caching
module that propagates some information identiﬁed in previously processed docu-
ments to the processing of a new one. To propagate features of individual words from
one document to processing another one is a risky strategy, since words are very

309


Computational Linguistics

Volume 28, Number 3

ambiguous. Word sequences, however, are much more stable and can be propagated
across documents. We decided to accumulate in our cache all multiword proper names
and lower-cased word bigrams induced by the sequence strategy (Section 7.1). These
word sequences are used by the sequence strategy exactly as are word sequences in-
duced on the ﬂy, and then the induced on-the-ﬂy sequences are added to the cache.
We also add to the cache the bigrams of abbreviations and regular words induced by
the abbreviation-handling module, as explained in Section 6. These bigrams are used
together with the bigrams induced on the ﬂy. This strategy proved to be quite useful:
it covered another 2% of unresolved cases (before applying the lexical lookup), with
an error rate of less than 1%.

11.2 Handling Russian News
To test how easy it is to apply the DCA to a new language, we tested it on a corpus
of British Broadcasting Corporation (BBC) news in Russian. We collected this corpus
from the Internet (cid:5)http://news.bbc.co.uk/hi/russian/world/default.htm(cid:6) over a period of 30
days. This gave us a corpus of 300 short documents (one or two paragraphs each).
We automatically created the supporting resources from 364,000 documents from the
Russian corpus of the European Corpus Initiative, using the method described in
section 5.

Since, unlike English, Russian is a highly inﬂected language, we had to deal with
the case normalization issue. Before using the DCA method, we applied a Russian
morphological processor (Mikheev and Liubushkina 1995) to convert each word in
the text to its main form: nominative case singular for nouns and adjectives, inﬁnitive
for verbs, etc. For words that could be normalized to several main forms (polysemy),
when secondary forms of different words coincided, we retained all the main forms.
Since the documents in the BBC news corpus were rather short, we applied the cache
module, as described in Section 11.1. This allowed us to reuse information across the
documents.

Russian proved to be a simpler case than English for our tasks. First, on average,
Russian words are longer than English words: thus the identiﬁcation of abbreviations
is simpler. Second, proper names in Russian coincide less frequently with common
words; this makes the disambiguation of capitalized words in ambiguous positions
easier. The overall performance reached a 0.1% error rate on sentence boundaries and
a 1.8% error rate on ambiguous capitalized words, with the coverage on both tasks
at 100%.

12. Related Research

12.1 Research in Nonlocal Context
The use of nonlocal context and dynamic adaptation have been studied in language
modeling for speech recognition. Kuhn and de Mori (1998) proposed a cache model
that works as a kind of short-term memory by which the probability of the most re-
cent n words is increased over the probability of a general-purpose bigram or trigram
model. Within certain limits, such a model can adapt itself to changes in word frequen-
cies, depending on the topic of the text passage. The DCA system is similar in spirit
to such dynamic adaptation: it applies word n-grams collected on the ﬂy from the
document under processing and favors them more highly than the default assignment
based on prebuilt lists. But unlike the cache model, it uses a multipass strategy.

Clarkson and Robinson (1997) developed a way of incorporating standard n-grams
into the cache model, using mixtures of language models and also exponentially de-
caying the weight for the cache prediction depending on the recency of the word’s last

310


Mikheev

Periods, Capitalized Words, etc.

occurrence. In our experiments we applied simple linear interpolation to incorporate
the DCA system into a POS tagger. Instead of decaying nonlocal information, we opted
for not propagating it from one document for processing of another. For handling very
long documents with our method, however, the information decay strategy seems to
be the right way to proceed.

Mani and MacMillan (1995) pointed out that little attention had been paid in the
named-entity recognition ﬁeld to the discourse properties of proper names. They pro-
posed that proper names be viewed as linguistic expressions whose interpretation
often depends on the discourse context, advocating text-driven processing rather than
reliance on pre-existing lists. The DCA outlined in this article also uses nonlocal dis-
course context and does not heavily rely on pre-existing word lists. It has been applied
not only to the identiﬁcation of proper names, as described in this article, but also to
their classiﬁcation (Mikheev, Grover, and Moens 1998).

Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit
only one sense in a document or discourse (“one sense per discourse”). Since then
this idea has been applied to several tasks, including word sense disambiguation
(Yarowsky 1995) and named-entity recognition (Cucerzan and Yarowsky 1999). Gale,
Church, and Yarowsky’s observation is also used in our DCA, especially for the iden-
tiﬁcation of abbreviations. In capitalized-word disambiguation, however, we use this
assumption with caution and ﬁrst apply strategies that rely not just on single words
but on words together with their local contexts (n-grams). This is similar to “one sense
per collocation” idea of Yarowsky (1993).

The description of the EAGLE workbench for linguistic engineering (Baldwin et al.
1997) mentions a case normalization module that uses a heuristic in which a capitalized
word in an ambiguous position should be rewritten without capitalization if it is found
lower-cased in the same document. This heuristic also employs a database of bigrams
and unigrams of lower-cased and capitalized words found in unambiguous positions.
It is quite similar to our method for capitalized-word disambiguation. The description
of the EAGLE case normalization module provided by Baldwin et al. is, however, very
brief and provides no performance evaluation or other details.

12.2 Research in Text Preprocessing

12.2.1 Sentence Boundary Disambiguation. There exist two large classes of SBD sys-
tems: rule based and machine learning. The rule-based systems use manually built
rules that are usually encoded in terms of regular-expression grammars supplemented
with lists of abbreviations, common words, proper names, etc. To put together a few
rules is fast and easy, but to develop a rule-based system with good performance is
quite a labor-consuming enterprise. For instance, the Alembic workbench (Aberdeen et
al. 1995) contains a sentence-splitting module that employs over 100 regular-expression
rules written in Flex. Another well-acknowledged shortcoming of rule-based systems
is that such systems are usually closely tailored to a particular corpus or sublanguage
and are not easily portable across domains.

Automatically trainable software is generally seen as a way of producing sys-
tems that are quickly retrainable for a new corpus, for a new domain, or even for
another language. Thus, the second class of SBD systems employs machine learning
techniques such as decision tree classiﬁers (Riley 1989), neural networks (Palmer and
Hearst 1994), and maximum-entropy modeling (Reynar and Ratnaparkhi 1997). Ma-
chine learning systems treat the SBD task as a classiﬁcation problem, using features
such as word spelling, capitalization, sufﬁx, and word class found in the local con-
text of a potential sentence-terminating punctuation sign. Although training of such

311


Computational Linguistics

Volume 28, Number 3

systems is completely automatic, the majority of machine learning approaches to the
SBD task require labeled examples for training. This implies an investment in the data
annotation phase.

The main difference between the existing machine learning and rule-based meth-
ods for the SBD task and our approach is that we decomposed the SBD task into
several subtasks. We decided to tackle the SBD task through the disambiguation of
the period preceding and following words and then feed this information into a very
simple SBD rule set. In contrast, the standard practice in building SBD software is to
disambiguate conﬁgurations of a period with its ambiguous local context in a single
step, either by encoding disambiguation clues into the rules or inferring a classiﬁer
that accounts for the ambiguity of the words on the left and on the right of the period.
Our approach to SBD is closer in spirit to machine learning methods because its
retargeting does not require rule reengineering and can be done completely automat-
ically. Unlike traditional machine learning SBD approaches, however, our approach
does not require annotated data for training.

12.2.2 Disambiguation of Capitalized Words. Disambiguation of capitalized words
is usually handled by POS taggers, which treat capitalized words in the same way
as other categories, that is, by accounting for the immediate syntactic context and
using estimates collected from a training corpus. As Church (1988) rightly pointed
out, however, “Proper nouns and capitalized words are particularly problematic: some
capitalized words are proper nouns and some are not. Estimates from the Brown
Corpus can be misleading. For example, the capitalized word ‘Acts’ is found twice in
the Brown Corpus, both times as a proper noun (in a title). It would be misleading to
infer from this evidence that the word ‘Acts’ is always a proper noun.”

In the information extraction ﬁeld, the disambiguation of ambiguous capitalized
words has always been tightly linked to the classiﬁcation of proper names into seman-
tic classes such as person name, location, and company name. Named-entity recogni-
tion systems usually use sets of complex hand-crafted rules that employ a gazetteer
and a local context (Krupa and Hausman 1998). In some systems such dependencies
are learned from labeled examples (Bikel et al. 1997). The advantage of the named-
entity approach is that the system not only identiﬁes proper names but also determines
their semantic class. The disadvantage is in the cost of building a wide-coverage set of
contextual clues manually or producing annotated training data. Also, the contextual
clues are usually highly speciﬁc to the domain and text genre, making such systems
very difﬁcult to port.

Both POS taggers and named-entity recognizers are normally built using the local-
context paradigm. In contrast, we opted for a method that relies on the entire distri-
bution of a word in a document. Although it is possible to train some classes of POS
taggers without supervision, this usually leads to suboptimal performance. Thus the
majority of taggers are trained using at least some labeled data. Named-entity recog-
nition systems are usually hand-crafted or trained from labeled data. As was shown
above, our method does not require supervised training.

12.2.3 Disambiguation of Abbreviations. Not much information has been published
on abbreviation identiﬁcation. One of the better-known approaches is described in
Grefenstette and Tapanainen (1994), which suggested that abbreviations ﬁrst be ex-
tracted from a corpus using abbreviation-guessing heuristics akin to those described
in Section 6 and then reused in further processing. This is similar to our treatment of
abbreviation handling, but our strategy is applied on the document rather than corpus
level. The main reason for restricting abbreviation discovery to a single document is

312


Mikheev

Periods, Capitalized Words, etc.

that this does not presuppose the existence of a corpus in which the current document
is similar to other documents.

Park and Byrd (2001) recently described a hybrid method for ﬁnding abbrevia-
tions and their deﬁnitions. This method ﬁrst applies an “abbreviation recognizer” that
generates a set of “candidate abbreviations” for a document. Then for this set of can-
didates the system tries to ﬁnd in the text their deﬁnitions (e.g., United Kingdom for
UK). The abbreviation recognizer for these purposes is allowed to overgenerate signif-
icantly. There is no harm (apart from the performance issues) in proposing too many
candidate abbreviations, because only those that can be linked to their deﬁnitions will
be retained. Therefore the abbreviation recognizer treats as a candidate any token of
two to ten characters that contains at least one capital letter. Candidates then are ﬁl-
tered through a set of known common words and proper names. At the same time
many good abbreviations and acronyms are ﬁltered out because not for all of them
will deﬁnitions exist in the current document.

In our task we are interested in ﬁnding all and only abbreviations that end with
a period (proper abbreviations rather than acronyms), regardless of whether they can
be linked to their deﬁnitions in the current document or not. Therefore, in our method
we cannot tolerate candidate overgeneration or excessive ﬁltering and had to develop
more selective methods for ﬁnding abbreviations in text.</corps>
		<conclusion>Aucune conclusion trouvée.</conclusion>
		<discussion>13. Discussion

In this article we presented an approach that tackles three important aspects of text nor-
malization: sentence boundary disambiguation, disambiguation of capitalized words
when they are used in positions where capitalization is expected, and identiﬁcation of
abbreviations. The major distinctive features of our approach can be summarized as
follows:

• We tackle the sentence boundary task only after we have fully

disambiguated the word on the left and the word on the right of a
potential sentence boundary punctuation sign.

•

To disambiguate capitalized words and abbreviations, we use
information distributed across the entire document rather than their
immediate local context.

• Our approach does not require manual rule construction or data

annotation for training. Instead, it relies on four word lists that can be
generated completely automatically from a raw (unlabeled) corpus.

In this approach we do not try to resolve each ambiguous word occurrence individu-
ally. Instead, the system scans the entire document for the contexts in which the words
in question are used unambiguously, and this gives it grounds, acting by analogy, for
resolving ambiguous contexts.

We deliberately shaped our approach so that it largely does not rely on precom-
piled statistics, because the most interesting events are inherently infrequent and hence
are difﬁcult to collect reliable statistics for. At the same time precompiled statistics
would be smoothed across multiple documents rather than targeted to a speciﬁc docu-
ment. By collecting suggestive instances of usage for target words from each particular
document on the ﬂy, rather than relying on preacquired resources smoothed across the
entire document collection, our approach is robust to domain shifts and new lexica
and closely targeted to each document.

313


Computational Linguistics

Volume 28, Number 3

A signiﬁcant advantage of this approach is that it can be targeted to new domains
completely automatically, without human intervention. The four word lists that our
system uses for its operation can be generated automatically from a raw corpus and
require no human annotation. Although some SBD systems can be trained on relatively
small sets of labeled examples, their performance in such cases is somewhat lower than
their optimal performance. For instance, Palmer and Hearst (1997) report that the SATZ
system (decision tree variant) was trained on a set of about 800 labeled periods, which
corresponds to a corpus of about 16,000 words. This is a relatively small training set
that can be manually marked in a few hours’ time. But the error rate (1.5%) of the
decision tree classiﬁer trained on this small sample was about 50% higher than that
when trained on 6,000 labeled examples (1.0%).

The performance of our system does not depend on the availability of labeled
training examples. For its “training,” it uses a raw (unannotated in any way) corpus
of texts. Although it needs such a corpus to be relatively large (a few hundred thousand
words), this is normally not a problem, since when the system is targeted to a new
domain, such a corpus is usually already available at no extra cost. Therefore there is no
trade-off between the amount of human labor and the performance of the system. This
not only makes retargeting of such system easier but also enables it to be operational
in a completely autonomous way: it needs only to be pointed to texts from a new
domain, and then it can retarget itself automatically.

Although the DCA requires two passes through a document, the simplicity of the
underlying algorithms makes it reasonably fast. It processes about 3,000 words per
second using a Pentium II 400 MHz processor. This includes identiﬁcation of abbre-
viations, disambiguation of capitalized words, and then disambiguation of sentence
boundaries. This is comparable to the speed of other preprocessing systems.3 The oper-
ational speed is about 10% higher than the training speed because, apart from applying
the system to the training corpus, training also involves collecting, thresholding, and
sorting of the word lists—all done automatically but at extra time cost. Training on
the 300,000-word NYT text collection took about two minutes.

Despite its simplicity, the performance of our approach was on the level with
the previously highest reported results on the same test collections. The error rate
on sentence boundaries in the Brown corpus was not signiﬁcantly worse than the
lowest quoted before (Riley 1989: 0.28% vs. 0.20% error rate). On the WSJ corpus
our system performed slightly better than the combination of the Alembic and SATZ
systems described in Palmer and Hearst (1997) (0.44% vs. 0.5% error rate). Although
these error rates seem to be very small, they are quite signiﬁcant. Unlike general POS
tagging, in which it is unfair to expect an error rate of less than 2% because even human
annotators have a disagreement rate of about 3%, sentence boundaries are much less
ambiguous (with a disagreement of about 1 in 5,000). This shows that an error rate
of 1 in 200 (0.5%) is still far from reaching the disagreement level. On the other hand,
one error in 200 periods means that there is one error in every two documents in the
Brown corpus and one error in every four documents in the WSJ corpus.

With all its strong points, there are a number of restrictions to the proposed ap-
proach. First, in its present form it is suitable only for processing of reasonably “well-
behaved” texts that consistently use capitalization (mixed case) and do not contain
much noisy data. Thus, for instance, we do not expect our system to perform well
on single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on

3 Palmer and Hearst (1997) report a speed of over 10,000 sentences a minute, which with their average

sentence length of 20 words equals over 3,000 words per second, but on a slower machine (DEC Alpha
3000).

314


Mikheev

Periods, Capitalized Words, etc.

optical character reader–generated texts. We noted in Section 8 that very short doc-
uments of one to three sentences also present a difﬁculty for our approach. This is
where robust syntactic systems like SATZ (Palmer and Hearst 1997) or the POS tagger
reported in Mikheev (2000), which do not heavily rely on word capitalization and are
not sensitive to document length, have an advantage.

Our DCA uses information derived from the entire document and thus can be
used as a complement to approaches based on the local context. When we incorpo-
rated the DCA system into a POS tagger (Section 8), we measured a 30–35% cut in the
error rate on proper-name identiﬁcation in comparison to DCA or the POS-tagging
approaches alone. This in turn enabled better tagging of sentence boundaries: a 0.20%
error rate on the Brown corpus and a 0.31% error rate on the WSJ corpus, which corre-
sponds to about a 20% cut in the error rate in comparison to DCA or the POS-tagging
approaches alone.

We also investigated the portability of our approach to other languages and ob-
tained encouraging results on a corpus of news in Russian. This strongly suggests that
the DCA method can be applied to the majority of European languages, since they
share the same principles of capitalization and word abbreviation. Obvious exceptions,
though, are German and some Scandinavian languages in which capitalization is used
for things other than proper-name and sentence start signaling. This does not mean,
however, that the DCA in general is not suitable for preprocessing of German texts—it
just needs to be applied with different disambiguation clues.

Initially the system described in this article was developed as a text normalization
module for a named-entity recognition system (Mikheev, Grover, and Moens 1998) that
participated in MUC-7. There the ability to identify proper names with high accuracy
proved to be instrumental in enabling the entire system to achieve a very high level of
performance. Since then this text normalization module has been used in several other
systems, and its ability to be adapted easily to new domains enabled rapid develop-
ment of text analysis capabilities in medical, legal, and law enforcement domains.

Appendix A: SBD Rule Set

In this section we present the rule set used by our system to assign potential sentence
boundary punctuation as

Punctuation that signals end of sentence
Period that is part of abbreviation

FS
AP
AFS Period that is part of abbreviation and signals end of sentence

This rule set operates over tokens that are disambiguated as to whether or not they
are abbreviations and whether or not they are proper names. Tokens are categorized
into overlapping sets as follows:

No token (end of input)
NONE
ANY
Any token
ANY-OR-NONE Any token or no token at all
ABBR

Not ABBR

CLOSE PUNCT
OPEN PUNCT

Token that was disambiguated as “abbreviation”
(Note: . . . Ellipsis is treated as an abbreviation too)
Nonpunctuation token that was disambiguated as “not
abbreviation”
Closing quotes, closing brackets
Opening quotes, opening brackets

315


Computational Linguistics

Volume 28, Number 3

PUNCT

Punctuation token not CLOSE PUNCT or OPEN PUNCT
or [.!?;]
NUM
Number
LOW COMMON Lower-cased common word
CAP COMMON Capitalized word that was disambiguated as a common word
CAP PROP
PROPER NAME

Capitalized word that was disambiguated as a proper name
Proper name

Rule Set

word-2 word-1

FOCAL word+1

word+2

Assign Example

ANY
ANY
ABBR .
ANY

Not ABBR
[.?!]
CLOSE PUNCT [.?!]
[.?!]
;
.
.
.
.
.

ANY
ABBR
ABBR
ABBR
ABBR
ABBR

ABBR
ABBR
ABBR
ABBR
ABBR
ABBR

ABBR
ABBR
ABBR

.
.
.
.
.
.

.
.
.

NONE

ANY-OR-NONE ANY-OR-NONE FS
ANY-OR-NONE ANY-OR-NONE FS
ANY-OR-NONE ANY-OR-NONE FS
CAP COMMON ANY-OR-NONE FS
NONE
AFS
CAP COMMON ANY-OR-NONE AFS
CLOSE PUNCT CAP COMMON AFS
OPEN PUNCT
CAP COMMON AFS
CLOSE PUNCT CAP COMMON AFS
OPEN PUNCT
ANY-OR-NONE AP
PUNCT
[.?!]
ANY-OR-NONE AP
LOW COMMON ANY-OR-NONE AP
LOW COMMON AP
CLOSE PUNCT
OPEN PUNCT
LOW COMMON AP
CLOSE PUNCT
LOW COMMON AP
OPEN PUNCT
ABBR
.
AP
ANY-OR-NONE AP
NUM
PROPER NAME ANY-OR-NONE AP

book.
).
Tex.!
; The
Tex.EOF
Tex. The
kg.) This
kg. (This
kg.) (This

kg.,
Tex.!
kg. this
kg.) this
kg. (this
kg.) (this

Sen. Gen.
kg. 5
Dr. Smith

Acknowledgments
The work reported in this article was
supported in part by grant GR/L21952 (Text
Tokenization Tool) from the Engineering
and Physical Sciences Research Council,
U.K., and also it beneﬁted from the ongoing
efforts in building domain-independent
text-processing software at Infogistics Ltd. I
am also grateful to one anonymous
reviewer who put a lot of effort into making
this article as it is now.</discussion>
		<biblio>Aberdeen, John S., John D. Burger, David S.

Day, Lynette Hirschman, Patricia
Robinson, and Marc Vilain. 1995. “Mitre:
Description of the alembic system used
for MUC-6.” In Proceedings of the Sixth
Message Understanding Conference (MUC-6),
Columbia, Maryland, November. Morgan
Kaufmann.

Baldwin, Breck, Christine Doran, Jeffrey

Reynar, Michael Niv, Bangalore Srinivas,
and Mark Wasson. 1997. “EAGLE: An
extensible architecture for general
linguistic engineering.” In Proceedings of
Computer-Assisted Information Searching on

316

Internet (RIAO ’97), Montreal, June.
Baum, Leonard E. and Ted Petrie. 1966.
Statistical inference for probabilistic
functions of ﬁnite Markov chains. Annals
of Mathematical Statistics 37:1559–1563.

Bikel, Daniel, Scott Miller, Richard

Schwartz, and Ralph Weischedel. 1997.
“Nymble: A high performance learning
name-ﬁnder.” In Proceedings of the Fifth
Conference on Applied Natural Language
Processing (ANLP’97), pages 194–200.
Washington, D.C., Morgan Kaufmann.

Brill, Eric. 1995a. Transformation-based
error-driven learning and natural
language parsing: A case study in
part-of-speech tagging. Computational
Linguistics 21(4):543–565.

Brill, Eric. 1995b. “Unsupervised learning of
disambiguation rules for part of speech
tagging.” In David Yarovsky and Kenneth
Church, editors, Proceedings of the Third
Workshop on Very Large Corpora, pages
1–13, Somerset, New Jersey. Association
for Computational Linguistics.

Burnage, Gavin. 1990. CELEX: A Guide for
Users. Centre for Lexical Information,
Nijmegen, Netherlands.


Mikheev

Periods, Capitalized Words, etc.

Chinchor, Nancy. 1998. “Overview of

MUC-7.” In Seventh Message Understanding
Conference (MUC-7): Proceedings of a
Conference Held in Fairfax, April. Morgan
Kaufmann.

Church, Kenneth. 1988. “A stochastic parts
program and noun-phrase parser for
unrestricted text.” In Proceedings of the
Second ACL Conference on Applied Natural
Language Processing (ANLP’88), pages
136–143, Austin, Texas.

Church, Kenneth. 1995. “One term or two?”
In SIGIR’95, Proceedings of the 18th Annual
International ACM SIGIR Conference on
Research and Development in Information
Retrieval, pages 310–318, Seattle,
Washington, July. ACM Press.

Clarkson, Philip and Anthony J. Robinson.
1997. “Language model adaptation using
mixtures and an exponentially decaying
cache.” In Proceedings IEEE International
Conference on Speech and Signal Processing,
Munich, Germany.

Cucerzan, Silviu and David Yarowsky. 1999.
“Language independent named entity
recognition combining morphological and
contextual evidence.” In Proceedings of
Joint SIGDAT Conference on EMNLP and
VLC.

Francis, W. Nelson and Henry Kucera. 1982.
Frequency Analysis of English Usage: Lexicon
and Grammar. Houghton Mifﬂin, New
York.

Gale, William, Kenneth Church, and David

Yarowsky. 1992. “One sense per
discourse.” In Proceedings of the Fourth
DARPA Speech and Natural Language
Workshop, pages 233–237.

Grefenstette, Gregory and Pasi Tapanainen.

1994. “What is a word, what is a
sentence? Problems of tokenization.” In
The Proceedings of Third Conference on
Computational Lexicography and Text
Research (COMPLEX’94), Budapest,
Hungary.

Krupka, George R. and Kevin Hausman.
1998. Isoquest Inc.: Description of the
netowl extractor system as used for
MUC-7. In Proceedings of the Seventh
Message Understanding Conference (MUC-7),
Fairfax, VA. Morgan Kaufmann.

Kuhn, Roland and Renato de Mori. 1998. A
cache-based natural language model for
speech recognition. IEEE Transactions on
Pattern Analysis and Machine Intelligence
12:570–583.

Kupiec, Julian. 1992. Robust part-of-speech
tagging using a hidden Markov model.
Computer Speech and Language.

Mani, Inderjeet and T. Richard MacMillan.

1995. “Identifying unknown proper

names in newswire text.” In B. Boguraev
and J. Pustejovsky, editors, Corpus
Processing for Lexical Acquisition. MIT Press,
Cambridge, Massachusetts, pages 41–59.
Marcus, Mitchell, Mary Ann Marcinkiewicz,
and Beatrice Santorini. 1993. Building a
large annotated corpus of English: The
Penn treebank. Computational Linguistics
19(2):313–329.

Mikheev, Andrei. 1997. Automatic rule

induction for unknown word guessing.
Computational Linguistics 23(3):405–423.
Mikheev, Andrei. 1999. A knowledge-free

method for capitalized word
disambiguation. In Proceedings of the 37th
Conference of the Association for
Computational Linguistics (ACL’99), pages
159–168, University of Maryland, College
Park.

Mikheev, Andrei. 2000. “Tagging sentence
boundaries.” In Proceedings of the First
Meeting of the North American Chapter of the
Computational Linguistics (NAACL’2000),
pages 264–271, Seattle, Washington.
Morgan Kaufmann.

Mikheev, Andrei, Clair Grover, and Colin

Matheson. 1998. TTT: Text Tokenisation Tool.
Language Technology Group, University
of Edinburgh. Available at
http://www.ltg.ed.ac.uk/software/ttt/
index.html.

Mikheev, Andrei, Clair Grover, and Marc
Moens. 1998. Description of the ltg
system used for MUC-7. In Seventh
Message Understanding Conference
(MUC–7): Proceedings of a Conference Held in
Fairfax, Virginia. Morgan Kaufmann.
Mikheev, Andrei and Liubov Liubushkina.

1995. Russian morphology: An
engineering approach. Natural Language
Engineering 1(3):235–260.

Palmer, David D. and Marti A. Hearst. 1994.

“Adaptive sentence boundary
disambiguation.” In Proceedings of the
Fourth ACL Conference on Applied Natural
Language Processing (ANLP’94), pages
78–83, Stuttgart, Germany, October.
Morgan Kaufmann.

Palmer, David D. and Marti A. Hearst. 1997.
Adaptive multilingual sentence boundary
disambiguation. Computational Linguistics
23(2):241–269.

Park, Youngja and Roy J. Byrd. 2001.
“Hybrid text mining for ﬁnding
abbreviations and their deﬁnitions.” In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMLP’01), pages 16–19, Washington,
D.C. Morgan Kaufmann.

Ratnaparkhi, Adwait. 1996. “A maximum

entropy model for part-of-speech

317


Computational Linguistics

Volume 28, Number 3

tagging.” In Proceedings of Conference on
Empirical Methods in Natural Language
Processing, pages 133–142, University of
Pennsylvania, Philadelphia.

Reynar, Jeffrey C. and Adwait Ratnaparkhi.
1997. “A maximum entropy approach to
identifying sentence boundaries.” In
Proceedings of the Fifth ACL Conference on
Applied Natural Language Processing
(ANLP’97), pages 16–19. Morgan
Kaufmann.

Riley, Michael D. 1989. “Some applications
of tree-based modeling to speech and

language indexing.” In Proceedings of the
DARPA Speech and Natural Language
Workshop, pages 339–352. Morgan
Kaufmann.

Yarowsky, David. 1993. “One sense per
collocation.” In Proceedings of ARPA
Human Language Technology Workshop ’93,
pages 266–271, Princeton, New Jersey.

Yarowsky, David. 1995. “Unsupervised
word sense disambiguation rivaling
supervised methods.” In Meeting of the
Association for Computational Linguistics
(ACL’95), pages 189–196.

318</biblio>
	</article>
	<article>
		<preamble>probabilistic_sentence_reduction.txt</preamble>
		<titre>Probabilistic Sentence Reduction Using Support Vector Machines</titre>
		<auteur>Minh Le Nguyen, Akira Shimazu, Susumu Horiguchi</auteur>
		<abstract>This paper investigates a novel application of sup- port vector machines (SVMs) for sentence reduction. We also propose a new probabilistic sentence reduc- tion method based on support vector machine learn- ing. Experimental results show that the proposed methods outperform earlier methods in term of sen- tence reduction performance.</abstract>
		<introduction>The most popular methods of sentence reduc- tion for text summarization are corpus based methods. Jing (Jing 00) developed a method to remove extraneous phrases from sentences by using multiple sources of knowledge to de- cide which phrases could be removed. However, while this method exploits a simple model for sentence reduction by using statistics computed from a corpus, a better model can be obtained by using a learning approach. Knight and Marcu (Knight and Marcu 02) proposed a corpus based sentence reduction method using machine learning techniques. They discussed a noisy-channel based approach and a decision tree based approach to sentence reduction. Their algorithms provide the best way to scale up the full problem of sentence re- duction using available data. However, these al- gorithms require that the word order of a given sentence and its reduced sentence are the same. Nguyen and Horiguchi (Nguyen and Horiguchi 03) presented a new sentence reduction tech- nique based on a decision tree model without that constraint. They also indicated that se- mantic information is useful for sentence reduc- tion tasks. The major drawback of previous works on sentence reduction is that those methods are likely to output local optimal results, which may have lower accuracy. This problem is caused by the inherent sentence reduction model; that is, only a single reduced sentence can be obtained. As pointed out by Lin (Lin 03), the best sen- tence reduction output for a single sentence is not approximately best for text summarization. This means that “local optimal” refers to the best reduced output for a single sentence, while the best reduced output for the whole text is “global optimal”. Thus, it would be very valu- able if the sentence reduction task could gener- ate multiple reduced outputs and select the best one using the whole text document. However, such a sentence reduction method has not yet been proposed. Support Vector Machines (Vapnik 95), on the other hand, are strong learning methods in com- parison with decision tree learning and other learning methods (Sholkopf 97). The goal of this paper is to illustrate the potential of SVMs for enhancing the accuracy of sentence reduc- tion in comparison with previous work. Accord- ingly, we describe a novel deterministic method for sentence reduction using SVMs and a two- stage method using pairwise coupling (Hastie 98). To solve the problem of generating mul- tiple best outputs, we propose a probabilistic sentence reduction model, in which a variant of probabilistic SVMs using a two-stage method with pairwise coupling is discussed. The rest of this paper will be organized as follows: Section 2 introduces the Support Vec- tor Machines learning. Section 3 describes the previous work on sentence reduction and our deterministic sentence reduction using SVMs. We also discuss remaining problems of deter- ministic sentence reduction. Section 4 presents a probabilistic sentence reduction method using support vector machines to solve this problem. Section 5 discusses implementation and our ex- perimental results; Section 6 gives our conclu- sions and describes some problems that remain to be solved in the future.</introduction>
		<corps>2 Support Vector Machine
Support vector machine (SVM)(Vapnik 95) is a
technique of machine learning based on statisti-
cal learning theory. Suppose that we are given
l training examples (xi, yi), (1 ≤ i ≤ l), where
xi is a feature vector in n dimensional feature
space, yi is the class label {-1, +1 } of xi. SVM
ﬁnds a hyperplane w.x + b = 0 which correctly
separates the training examples and has a max-
imum margin which is the distance between two
hyperplanes w.x + b ≥ 1 and w.x + b ≤ −1. The
optimal hyperplane with maximum margin can
be obtained by solving the following quadratic
programming.

l(cid:80)

1
2 (cid:107)w(cid:107) + C0
yi(w.xi + b) ≥ 1 − ξi

ξi

i

min

s.t.
ξi ≥ 0

(1)

where C0 is the constant and ξi is a slack vari-
able for the non-separable case.
In SVM, the
optimal hyperplane is formulated as follows:

f (x) = sign

(cid:195)

l(cid:88)

1

(cid:33)

αiyiK(xi, x) + b

(2)

where αi

is the Lagrange multiple, and
K(x(cid:48), x(cid:48)(cid:48)) is a kernel function, the SVM calcu-
lates similarity between two arguments x(cid:48) and
x(cid:48)(cid:48). For instance, the Polynomial kernel func-
tion is formulated as follow:

K(x(cid:48), x(cid:48)(cid:48)) = (x(cid:48).x(cid:48)(cid:48))p

(3)

SVMs estimate the label of an unknown ex-
ample x whether the sign of f (x) is positive or
not.

3 Deterministic Sentence Reduction

Using SVMs

3.1 Problem Description
In the corpus-based decision tree approach, a
given input sentence is parsed into a syntax tree
and the syntax tree is then transformed into a
small tree to obtain a reduced sentence.

Let t and s be syntax trees of the original sen-
tence and a reduced sentence, respectively. The
process of transforming syntax tree t to small
tree s is called “rewriting process” (Knight and
Marcu 02), (Nguyen and Horiguchi 03). To
transform the syntax tree t to the syntax tree
s, some terms and ﬁve rewriting actions are de-
ﬁned.

An Input list consists of a sequence of words
subsumed by the tree t where each word in the
Input list is labelled with the name of all syntac-
tic constituents in t. Let CSTACK be a stack

that consists of sub trees in order to rewrite a
small tree. Let RSTACK be a stack that con-
sists of sub trees which are removed from the
Input list in the rewriting process.

• SHIFT action transfers the ﬁrst word from the
Input list into CSTACK. It is written mathe-
matically and given the label SHIFT.

• REDUCE(lk, X) action pops the lk syntactic
trees located at the top of CSTACK and com-
bines them in a new tree, where lk is an integer
and X is a grammar symbol.

• DROP X action moves subsequences of words
that correspond to syntactic constituents from
the Input list to RSTACK.

• ASSIGN TYPE X action changes the label of
trees at the top of the CSTACK. These POS
tags might be diﬀerent from the POS tags in
the original sentence.

• RESTORE X action takes the X element in
RSTACK and moves it into the Input list,
where X is a subtree.

For convenience, let conﬁguration be a status
of Input list, CSTACK and RSTACK. Let cur-
rent context be the important information in a
conﬁguration. The important information are
deﬁned as a vector of features using heuristic
methods as in (Knight and Marcu 02), (Nguyen
and Horiguchi 03).

The main idea behind deterministic sentence
reduction is that it uses a rule in the current
context of the initial conﬁguration to select a
distinct action in order to rewrite an input sen-
tence into a reduced sentence. After that, the
current context is changed to a new context and
the rewriting process is repeated for selecting
an action that corresponds to the new context.
The rewriting process is ﬁnished when it meets
a termination condition. Here, one rule corre-
sponds to the function that maps the current
context to a rewriting action. These rules are
learned automatically from the corpus of long
sentences and their reduced sentences (Knight
and Marcu 02), (Nguyen and Horiguchi 03).

3.2 Example
Figure 1 shows an example of applying a se-
quence of actions to rewrite the input sentence
(a, b, c, d, e), when each character is a word. It
illustrates the structure of the Input list, two
stacks, and the term of a rewriting process based
on the actions mentioned above. For example,
in the ﬁrst row, DROP H deletes the sub-tree
with its root node H in the Input list and stores


it in the RSTACK. The reduced tree s can be
obtained after applying a sequence of actions
as follows: DROP H; SHIFT; ASSIGN TYPE K;
DROP B; SHIFT; ASSIGN TYPE H; REDUCE 2
F; RESTORE H; SHIFT; ASSIGN TYPE D; RE-
DUCE 2G. In this example, the reduced sentence
is (b, e, a).

Figure 2: Example of Conﬁguration

that start with the ﬁrst unit in the Input list.
For example,
in Figure 2 the syntactic con-
stituents are labels of the current element in the
Input list from “VP” to the verb “convince”.
Semantic features
The following features are used in our model as
semantic information.

• Semantic information about current words
within the Input list; these semantic types
are obtained by using the named entities such
as Location, Person, Organization and Time
within the input sentence. To deﬁne these
name entities, we use the method described in
(Borthwick 99).

• Semantic information about whether or not the

word in the Input list is a head word.

• Word relations, such as whether or not a word
has a relationship with other words in the sub-
categorization table. These relations and the
sub-categorization table are obtained using the
Commlex database (Macleod 95).

Using the semantic information, we are able to
avoid deleting important segments within the
given input sentence. For instance, the main
verb, the subject and the object are essential
and for the noun phrase, the head noun is essen-
tial, but an adjective modiﬁer of the head noun
is not. For example, let us consider that the
verb “convince” was extracted from the Com-
lex database as follows.

convince
NP-PP: PVAL (“of”)
NP-TO-INF-OC

This entry indicates that the verb “convince”

Figure 1: An Example of the Rewriting Process

3.3 Learning Reduction Rules Using

SVMs

As mentioned above, the action for each conﬁg-
uration can be decided by using a learning rule,
which maps a context to an action. To obtain
such rules, the conﬁguration is represented by
a vector of features with a high dimension. Af-
ter that, we estimate the training examples by
using several support vector machines to deal
with the multiple classiﬁcation problem in sen-
tence reduction.

3.3.1 Features
One important task in applying SVMs to text
summarization is to deﬁne features. Here, we
describe features used in our sentence reduction
models.

The features are extracted based on the cur-
rent context. As it can be seen in Figure 2, a
context includes the status of the Input list and
the status of CSTACK and RSTACK. We de-
ﬁne a set of features for a current context as
described bellow.
Operation feature
The set of features as described in (Nguyen and
Horiguchi 03) are used in our sentence reduction
models.
Original tree features
These features denote the syntactic constituents


can be followed by a noun phrase and a preposi-
tional phrase starting with the preposition “of”.
It can be also followed by a noun phrase and a
to-inﬁnite phrase. This information shows that
we cannot delete an “of” prepositional phrase
or a to-inﬁnitive that is the part of the verb
phrase.

3.3.2 Two-stage SVM Learning using

Pairwise Coupling

Using these features we can extract training
data for SVMs. Here, a sample in our training
data consists of pairs of a feature vector and
an action. The algorithm to extract training
data from the training corpus is modiﬁed using
the algorithm described in our pervious work
(Nguyen and Horiguchi 03).

Since the original support vector machine
(SVM) is a binary classiﬁcation method, while
the sentence reduction problem is formulated as
multiple classiﬁcation, we have to ﬁnd a method
to adapt support vector machines to this prob-
lem. For multi-class SVMs, one can use strate-
gies such as one-vs all, pairwise comparison or
DAG graph (Hsu 02). In this paper, we use the
pairwise strategy, which constructs a rule for
discriminating pairs of classes and then selects
the class with the most winning among two class
decisions.

To boost the training time and the sentence
reduction performance, we propose a two-stage
SVM described below.

Suppose that the examples in training data
are divided into ﬁve groups m1, m2, ..., m5 ac-
cording to their actions. Let Svmc be multi-
class SVMs and let Svmc-i be multi-class SVMs
for a group mi. We use one Svmc classiﬁer to
identify the group to which a given context e
should be belong. Assume that e belongs to
the group mi. The classiﬁer Svmc-i is then used
to recognize a speciﬁc action for the context e.
The ﬁve classiﬁers Svmc-1, Svmc-2,..., Svmc-5
are trained by using those examples which have
actions belonging to SHIFT, REDUCE, DROP,
ASSIGN TYPE and RESTORE.

Table 1 shows the distribution of examples in

ﬁve data groups.

3.4 Disadvantage of Deterministic

Sentence Reductions

The idea of the deterministic algorithm is to
use the rule for each current context to select
the next action, and so on. The process termi-
nates when a stop condition is met. If the early
steps of this algorithm fail to select the best ac-

Table 1: Distribution of example data on ﬁve
data groups
Name
SHIFT-GROUP
REDUCE-GROUP
DROP-GROUP
ASSIGN-GROUP
RESTORE-GROUP
TOTAL

Number of examples
13,363
11,406
4,216
13,363
2,004
44,352

tions, then the possibility of obtaining a wrong
reduced output becomes high.

One way to solve this problem is to select mul-
tiple actions that correspond to the context at
each step in the rewriting process. However,
the question that emerges here is how to deter-
mine which criteria to use in selecting multiple
actions for a context.
If this problem can be
solved, then multiple best reduced outputs can
be obtained for each input sentence and the best
one will be selected by using the whole text doc-
ument.

In the next section propose a model for se-
lecting multiple actions for a context in sentence
reduction as a probabilistic sentence reduction
and present a variant of probabilistic sentence
reduction.

4 Probabilistic Sentence Reduction

Using SVM

a

set

of k

4.1 The Probabilistic SVM Models
Let A be
actions A =
{a1, a2...ai, ..., ak} and C be a set of n con-
texts C = {c1, c2...ci, ..., cn} . A probabilistic
model α for sentence reduction will select an
action a ∈ A for the context c with probability
pα(a|c). The pα(a|c) can be used to score ac-
tion a among possible actions A depending the
context c that is available at the time of deci-
sion. There are several methods for estimating
such scores; we have called these “probabilistic
sentence reduction methods”. The conditional
probability pα(a|c) is estimated using a variant
of probabilistic support vector machine, which
is described in the following sections.
4.1.1 Probabilistic SVMs using
Pairwise Coupling

For convenience, we denote uij = p(a = ai|a =
ai ∨aj, c). Given a context c and an action a, we
assume that the estimated pairwise class prob-
abilities rij of uij are available. Here rij can
be estimated by some binary classiﬁers. For
instance, we could estimate rij by using the


SVM binary posterior probabilities as described
in (Plat 2000). Then, the goal is to estimate
{pi}k
i=1 , where pi = p(a = ai|c), i = 1, 2, ..., k.
For this propose, a simple estimate of these
probabilities can be derived using the following
voting method:

pi = 2

(cid:88)

j:j(cid:54)=i

I{rij >rji}/k(k − 1)

where I is an indicator function and k(k − 1) is
the number of pairwise classes. However, this
model is too simple; we can obtain a better one
with the following method.

Assume that uij are pairwise probabilities of
the model subject to the condition that uij =
pi/(pi+pj). In (Hastie 98), the authors proposed
to minimize the Kullback-Leibler (KL) distance
between the rij and uij
(cid:88)

l(p) =

nijrijlog

(4)

rij
uij

i(cid:54)=j

where rij and uij are the probabilities of a pair-
wise ai and aj in the estimated model and in
our model, respectively, and nij is the number
of training data in which their classes are ai or
aj. To ﬁnd the minimizer of equation (6), they
ﬁrst calculate

∂l(p)
∂pi

=

(cid:88)

i(cid:54)=j

nij(−

rij
pi

+

1
pi + pj

).

Thus, letting ∆l(p) = 0, they proposed to ﬁnd
a point satisfying

(cid:88)

nijuij =

(cid:88)

nijrij,

j:j(cid:54)=i

j:j(cid:54)=i

k(cid:80)

i=1

pi = 1,

where i = 1, 2, ...k and pi > 0.
Such a point can be obtained by using an algo-
rithm described elsewhere in (Hastie 98). We
applied it to obtain a probabilistic SVM model
for sentence reduction using a simple method as
follows. Assume that our class labels belong to
l groups: M = {m1, m2...mi, ..., ml} , where l
is a number of groups and mi is a group e.g.,
SHIFT, REDUCE ,..., ASSIGN TYPE. Then
the probability p(a|c) of an action a for a given
context c can be estimated as follows.

p(a|c) = p(mi|c) × p(a|c, mi)

(5)

where mi is a group and a ∈ mi. Here, p(mi|c)
and p(a|c, mi) are estimated by the method in
(Hastie 98).

4.2 Probabilistic sentence reduction

algorithm

After obtaining a probabilistic model p, we then
use this model to deﬁne function score, by which

the search procedure ranks the derivation of in-
complete and complete reduced sentences. Let
d(s) = {a1, a2, ...ad} be the derivation of a small
tree s, where each action ai belongs to a set of
possible actions. The score of s is the product
of the conditional probabilities of the individual
actions in its derivation.

(cid:89)

Score(s) =

p(ai|ci)

(6)

ai∈d(s)

where ci is the context in which ai was decided.
The search heuristic tries to ﬁnd the best re-
duced tree s∗ as follows:

s∗ = argmax
(cid:124) (cid:123)(cid:122) (cid:125)
s∈tree(t)

Score(s)

(7)

where tree(t) are all the complete reduced trees
from the tree t of the given long sentence. As-
sume that for each conﬁguration the actions
{a1, a2, ...an} are sorted in decreasing order ac-
cording to p(ai|ci), in which ci is the context
of that conﬁguration. Algorithm 1 shows a
probabilistic sentence reduction using the top
K-BFS search algorithm. This algorithm uses
a breadth-ﬁrst search which does not expand
the entire frontier, but instead expands at most
the top K scoring incomplete conﬁgurations in
the frontier; it is terminated when it ﬁnds M
completed reduced sentences (CL is a list of re-
duced trees), or when all hypotheses have been
exhausted. A conﬁguration is completed if and
only if the Input list is empty and there is one
tree in the CSTACK. Note that the function
get-context(hi, j) obtains the current context of
the jth conﬁguration in hi, where hi is a heap at
step i. The function Insert(s,h) ensures that the
heap h is sorted according to the score of each
element in h. Essentially, in implementation we
can use a dictionary of contexts and actions ob-
served from the training data in order to reduce
the number of actions to explore for a current
context.</corps>
		<conclusion>6 Conclusions
We have presented a new probabilistic sentence
reduction approach that enables a long sentence
to be rewritten into reduced sentences based on
support vector models. Our methods achieves
better performance when compared with earlier
methods. The proposed reduction approach can
generate multiple best outputs. Experimental
results showed that the top 10 reduced sentences
returned by the reduction process might yield
accuracies higher than previous work. We be-
lieve that a good ranking method might improve
the sentence reduction performance further in a
text.</conclusion>
		<discussion>5 Experiments and Discussion
We used the same corpus as described in
(Knight and Marcu 02), which includes 1,067
pairs of sentences and their reductions. To
evaluate sentence reduction algorithms, we ran-
domly selected 32 pairs of sentences from our
parallel corpus, which is refered to as the test
corpus. The training corpus of 1,035 sentences
extracted 44,352 examples, in which each train-
ing example corresponds to an action. The
SVM tool, LibSVM (Chang 01) is applied to
train our model. The training examples were


Algorithm 1 A probabilistic sentence reduction
algorithm

1: CL={Empty};

i = 0; h0={ Initial conﬁguration}

2: while |CL| < M do
if hi is empty then
3:
4:
5:
6:
7:
8:

end if
u =min(|hi|, K)
for j = 1 to u do

break;

c=get-context(hi, j)
m(cid:80)

9:

10:
11:
12:

Select m so that

p(ai|c) < Q is maximal

for l=1 to m do

i=1

parameter=get-parameter(al);
Obtain a new conﬁguration s by performing action al
with parameter
if Complete(s) then

Insert(s, CL)

Insert(s, hi+1)

else

13:
14:
15:
16:
17:
18:
19:
20:
21: end while

end for
i = i + 1

end if
end for

divided into SHIFT, REDUCE, DROP, RE-
STORE, and ASSIGN groups. To train our
support vector model in each group, we used
the pairwise method with the polynomial ker-
nel function, in which the parameter p in (3)
and the constant C0 in equation (1) are 2 and
0.0001, respectively.

The algorithms (Knight and Marcu 02) and
(Nguyen and Horiguchi 03) served as the base-
line1 and the baseline2 for comparison with the
proposed algorithms. Deterministic sentence re-
duction using SVM and probabilistic sentence
reduction were named as SVM-D and SVMP, re-
spectively. For convenience, the ten top reduced
outputs using SVMP were called SVMP-10. We
used the same evaluation method as described
in (Knight and Marcu 02) to compare the pro-
posed methods with previous methods. For this
experiment, we presented each original sentence
in the test corpus to three judges who are spe-
cialists in English, together with three sentence
reductions: the human generated reduction sen-
tence, the outputs of the proposed algorithms,
and the output of the baseline algorithms.

The judges were told that all outputs were
generated automatically. The order of the out-
puts was scrambled randomly across test cases.
The judges participated in two experiments. In
the ﬁrst, they were asked to determine on a scale
from 1 to 10 how well the systems did with re-
spect to selecting the most important words in
the original sentence. In the second, they were
asked to determine the grammatical criteria of

reduced sentences.

Table 2 shows the results of English language
sentence reduction using a support vector ma-
chine compared with the baseline methods and
with human reduction. Table 2 shows compres-
sion rates, and mean and standard deviation re-
sults across all judges, for each algorithm. The
results show that the length of the reduced sen-
tences using decision trees is shorter than using
SVMs, and indicate that our new methods out-
perform the baseline algorithms in grammatical
and importance criteria. Table 2 shows that the

Table 2: Experiment results with Test Corpus

METHOD
Baseline1
Baseline2
SVM-D
SVMP-10
Human

Comp
Gramma
57.19% 8.60 ± 2.8
57.15% 8.60 ± 2.1
57.65% 8.76 ± 1.2
57.51% 8.80 ± 1.3
64.00% 9.05 ± 0.3

Impo
7.18 ± 1.92
7.42 ± 1.90
7.53 ± 1.53
7.74 ± 1.39
8.50 ± 0.80

ﬁrst 10 reduced sentences produced by SVMP-
10 (the SVM probabilistic model) obtained the
highest performances. We also compared the
computation time of sentence reduction using
support vector machine with that in previous
works. Table 3 shows that the computational
times for SVM-D and SVMP-10 are slower than
baseline, but it is acceptable for SVM-D.

Table 3: Computational times of performing re-
ductions on test-set. Average sentence length
was 21 words.
METHOD
Baseline1
SVM-D
SVMP-10

Computational times (sec)
138.25
212.46
1030.25

We also investigated how sensitive the pro-
posed algorithms are with respect to the train-
ing data by carrying out the same experi-
ment on sentences of diﬀerent genres. We
created the test corpus by selecting sentences
from the web-site of the Benton Foundation
(http://www.benton.org). The leading sen-
tences in each news article were selected as the
most relevant sentences to the summary of the
news. We obtained 32 leading long sentences
and 32 headlines for each item. The 32 sen-
tences are used as a second test for our methods.
We use a simple ranking criterion: the more the
words in the reduced sentence overlap with the
words in the headline, the more important the


sentence is. A sentence satisfying this criterion
is called a relevant candidate.

For a given sentence, we used a simple
method, namely SVMP-R to obtain a re-
duced sentence by selecting a relevant candi-
date among the ten top reduced outputs using
SVMP-10.

Table 4 depicts the experiment results for
the baseline methods, SVM-D, SVMP-R, and
SVMP-10. The results shows that, when ap-
plied to sentence of a diﬀerent genre, the per-
formance of SVMP-10 degrades smoothly, while
the performance of the deterministic sentence
reductions (the baselines and SVM determinis-
tic) drops sharply. This indicates that the prob-
abilistic sentence reduction using support vector
machine is more stable.
that

the performance of
SVMP-10 is also close to the human reduction
outputs and is better than previous works. In
addition, SVMP-R outperforms the determin-
istic sentence reduction algorithms and the dif-
ferences between SVMP-R’s results and SVMP-
10 are small. This indicates that we can ob-
tain reduced sentences which are relevant to the
headline, while ensuring the grammatical and
the importance criteria compared to the origi-
nal sentences.

Table 4 shows

Table 4: Experiment results with Benton Cor-
pus

Comp
Gramma
METHOD
54.14% 7.61 ± 2.10
Baseline1
53.13% 7.72 ± 1.60
Baseline2
SVM-D
56.64% 7.86 ± 1.20
SVMP-R 58.31% 8.25 ± 1.30
57.62% 8.60 ± 1.32
SVMP-10
64.00% 9.01 ± 0.25
Human

Impo
6.74 ± 1.92
7.02 ± 1.90
7.23 ± 1.53
7.54 ± 1.39
7.71 ± 1.41
8.40 ± 0.60</discussion>
		<biblio>A. Borthwick, “A Maximum Entropy Approach
to Named Entity Recognition”, Ph.D the-
sis, Computer Science Department, New York
University (1999).

C.-C. Chang

and C.-J.

Lin,
support

“LIB-
vec-
at

a

library

available

for
Software

SVM:
tor machines”,
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
H. Jing, “Sentence reduction for automatic
text summarization”, In Proceedings of the
First Annual Meeting of the North Ameri-
can Chapter of the Association for Compu-
tational Linguistics NAACL-2000.

T.T. Hastie and R. Tibshirani, “Classiﬁcation
by pairwise coupling”, The Annals of Statis-
tics, 26(1): pp. 451-471, 1998.

C.-W. Hsu and C.-J. Lin, “A comparison of
methods for multi-class support vector ma-
chines”, IEEE Transactions on Neural Net-
works, 13, pp. 415-425, 2002.

K. Knight and D. Marcu, “Summarization be-
yond sentence extraction: A Probabilistic ap-
proach to sentence compression”, Artiﬁcial
Intelligence 139: pp. 91-107, 2002.

C.Y. Lin, “Improving Summarization Perfor-
mance by Sentence Compression — A Pi-
lot Study”, Proceedings of the Sixth Inter-
national Workshop on Information Retrieval
with Asian Languages, pp.1-8, 2003.

C. Macleod and R. Grishman, “COMMLEX
syntax Reference Manual”; Proteus Project,
New York University (1995).

M.L. Nguyen and S. Horiguchi, “A new sentence
reduction based on Decision tree model”,
Proceedings of 17th Paciﬁc Asia Conference
on Language, Information and Computation,
pp. 290-297, 2003

V. Vapnik, “The Natural of Statistical Learning
Theory”, New York: Springer-Verlag, 1995.
J. Platt,“ Probabilistic outputs for support vec-
tor machines and comparison to regularized
likelihood methods,” in Advances in Large
Margin Classiﬁers, Cambridege, MA: MIT
Press, 2000.

B. Scholkopf et al, “Comparing Support Vec-
tor Machines with Gausian Kernels to Radius
Basis Function Classifers”, IEEE Trans. Sig-
nal Procesing, 45, pp. 2758-2765, 1997.</biblio>
	</article>
	<article>
		<preamble>Stolcke_1996_Automatic_linguistic.txt</preamble>
		<titre>AUTOMATIC LINGUISTIC SEGMENTATION OF CONVERSATIONAL SPEECH</titre>
		<auteur>Andreas Stolcke</auteur>
		<abstract>As speech recognition moves toward more unconstrained domains such as conversational speech, we encounter a need to be able to segment (or resegment) waveforms and recognizer output into lin- guistically meaningful units, such a sentences. Toward this end, we present a simple automatic segmenter of transcripts based on N-gram language modeling. We also study the relevance of sev- eral word-level features for segmentation performance. Using only word-level information, we achieve 85% recall and 70% precision on linguistic boundary detection. 1.</abstract>
		<introduction>Today’s large-vocabulary speech recognizers typically prefer to pro- cess a few tens of seconds of speech at a time, to keep the time and memory demands of the decoder within bounds. For longer inputs, the waveform is usually presegmented into shorter pieces based on simple acoustic criteria, such as nonspeech intervals (e.g., pauses) and turn boundaries (when several speakers are involved). We refer to such segmentations as acoustic segmentations. Acoustic segmentations generally do not reﬂect the linguistic struc- ture of utterances. They may fragment sentences or semantic units, or group together spans of unrelated units. We examine several rea- sons why such behavior is undesirable, and propose that linguistic segmentations be used instead. This requires algorithms for auto- matically ﬁnding linguistic units. In this paper we report on ﬁrst results from our ongoing efforts toward such an automatic linguis- tic segmentation. In all further discussion, unless otherwise noted, the terms ‘segment,’ ‘segmentation,’ etc. will refer to linguistic seg- mentations.</introduction>
		<corps>2. THE IMPORTANCE OF LINGUISTIC

SEGMENTATION

Acoustic segmentations are inadequate in cases where the output
of a speech recognizer is to serve as input for further processing
based on syntactically or semantically coherent units. This includes
most natural language (NL) parsers or NL understanding or transla-
tion systems. For such systems, the fragmented recognition output
would have to be put back together and large spans of unrelated
material would need to be resegmented into linguistic units.

Automatic detection of linguistic segments could also improve the
user interface of many speech systems. A spoken language system

could use the knowledge incorporated in an automatic segmenter
to help end-point a user’s speech input. A speech indexing and re-
trieval system (such as for transcribed broadcast audio) could pro-
cess its data in more meaningful units if the locations of linguistic
segment boundaries were known.

Our main motivation for the work reported here comes from speech
language modeling. Experiments at the 1995 Johns Hopkins Lan-
guage Modeling Workshop showed that the quality of a language
model (LM) can be improved if both training and test data are seg-
mented linguistically, rather than acoustically [8]. We showed in
[10] and [9] that proper modeling of ﬁlled pauses requires knowl-
edge of linguistic segment boundaries. We found for example that
segment-internal ﬁlled pauses condition the following words quite
differently from segment-initial ﬁlled pauses. Finally, recent efforts
in language modeling for conversational speech, such as [8], attempt
to capitalize on the internal structure of utterances and turns. Such
models are formulated in terms of linguistic units and therefore re-
quire linguistic segmentations to be applicable.

3. METHOD

Our main goal for this work was to examine to what extent various
kinds of lexical (word-based) information were useful for automatic
linguistic segmentation. This precluded a study based on the out-
put of existing speech recognition systems, which currently achieve
about 40-50% word error rate on the type of data used in our exper-
iments. At such high error rates, the analysis of any segmentation
algorithm and the features it uses would likely be confounded by
the unreliable nature of the input data. We therefore chose to elimi-
nate the problem of inaccurate speech recognition and tested our al-
gorithms on hand-transcribed word-level transcripts of spontaneous
speech from the Switchboard corpus [4]. An additional beneﬁt of
this approach is that the models employed by the segmentation al-
gorithms can also be directly used as language models for speech
recognizers for the same type of data, an application we are pursu-
ing as well.

The segmentation approaches we investigated all fell within the fol-
lowing framework. We ﬁrst trained a statistical language model
of the N-gram variety to model the distribution of both words and
segment boundaries. (For this purpose, segment boundaries were
represented as special tokens <s> within the text.) The segmenta-
tion information was removed from the test data, and the language
model was used to hypothesize the most probable locations of seg-


ment boundaries. The resulting segmentations were then evaluated
along a number of metrics.

As training data, we used 1.4 million words of Switchboard tran-
scripts annotated for linguistic segmentations by the UPenn Tree-
bank project [7], comprising a total of 193,000 segments. One half
of the standard Switchboard development test set, totaling 10,000
words and 1,300 segments, was used for testing.

The hand-annotated segments encompassed different kinds of lin-
guistic structures, including

(cid:15) Complete sentences

(cid:15) Stand-alone phrases

(cid:15) Disﬂuent sentences aborted in mid-utterance1

(cid:15) Interjections and back-channel responses

The following excerpt illustrates the character of the data. Linguis-
tic segment boundaries are marked <s>, whereas acoustic segmen-
tations are indicated by //.

B.44: Worried that they're not going to

get enough attention? <s> //

A.45: Yeah, <s> and, uh, you know, colds

and things like that <laughter> get -- //

B.46: Yeah. <s> //

A.47: -- spread real easy and things,

<s> but, // and they're expensive <s> and,

// <lipsmack> // course, // there's a lot

of different types of day care available,

too, // you know, where they teach them

academic things. <s> //

B.48: Yes. <s> //

This short transcript shows some of the ubiquitous features of spon-
taneous speech affecting segmentation, such as

(cid:15) Mismatch between acoustic and linguistic segmentations

(A.47)

(cid:15) segments spanning several turns (A.45 and A.47)

(cid:15) backchannel responses (B.46)

4. THE MODEL

computation yields the likelihoods of the states at each position k:

PNO-S (w1 : : : wk ) = PNO-S (w1 : : : wk(cid:0)1 ) (cid:2)

p(wk jwk(cid:0)2wk(cid:0)1 )
+PS (w1 : : : wk(cid:0)1 ) (cid:2)
p(wk j<s>wk(cid:0)1 )

PS (w1 : : : wk ) = PNO-S (w1 : : : wk(cid:0)1 ) (cid:2)

p(<s>jwk(cid:0)2wk(cid:0)1 )p(wk j<s>)

+PS (w1 : : : wk(cid:0)1 ) (cid:2)

p(<s>j<s>wk(cid:0)1)p(wk j<s>)

A corresponding Viterbi algorithm is used to ﬁnd the most likely
sequence of S and NO-S (i.e., a segmentation) for a given word
string. This language model is a full implementation of the model
approximated in [8]. The hidden disﬂuency model of [10] has a
similar structure. As indicated in the formulae above, we currently
use at most two words of history in the local conditional probabili-
ties p((cid:1)j(cid:1)). Longer N-grams can be used if more state information is
kept.

The local N-gram probabilities are estimated from the training data
by using Katz backoff with Good-Turing discounting [6].

5. RESULTS

5.1. Baseline Segmentation Model

The ﬁrst model we looked at models only plain words and segment
boundaries in the manner described. It was applied to the concate-
nation of all turns of a conversation side, with no additional con-
textual cues supplied. During testing, this model thus operates with
very minimal information, i.e., with only the raw word sequence to
be segmented. Table 1 shows results for bigram and trigram mod-
els. The performance metrics used are deﬁned as follows. Recall

Table 1: Baseline model performance

Model
Recall
65.5%
Bigram
Trigram 70.2%

Precision
56.9%
60.7%

FA
SER
1.9% 58.9%
2.0% 53.1%

The language models used were of the N-gram type commonly used
in speech recognition [5].
In N-gram models, a word wn from a
n (cid:0) 1 word history w1 : : : wn(cid:0)1. If the history contains a segment
boundary <s>, it is truncated before that location. During testing,
the model is run as a hidden segment model, hypothesizing segment
boundaries between any two words and implicitly computing the
probabilities of all possible segmentations.

Associated with each word position are two states, S and NO-S, cor-
responding to a segment starting or not before that word. A forward

1Although complete and disﬂuent sentences were marked differently in

the corpus, we modeled these with a single type of boundary token.

is the percentage of actual segment boundaries hypothesized. Pre-
cision is the percentage of hypothesized boundaries that are actual.
False Alarms (FA) are the fraction of potential boundaries incor-
rectly hypothesized as boundaries. Segment Error Rate (SER) is the
percentage of actual segments identiﬁed without intervening false
alarms.

As can be seen, word context alone can identify a majority of seg-
ment boundaries at a modest false alarm rate of about 2%. The tri-
gram model does better than the bigram, but this is expected since it
has access to a larger context around potential segment boundaries.
to use in its decision. Given these results, we only consider trigram
models in all following experiments.


5.2. Using Turn Information

Next we examined a richer model that incorporated information
about the turn-taking between speakers.2 Note that turn boundaries
are already present in acoustic segmentations, but in this case we
will only use them as a cue to the identiﬁcation of linguistic seg-
ments. Turn information is easily incorporated into the segmenta-
tion model by placing special tags at turn boundaries (in both train-
ing and testing). Model performance is summarized in Table 2.

Table 2: Segmentation performance using turn information

Model
Baseline
Turn-tagged

Recall
70.2%
76.9%

Precision
60.7%
66.9%

FA
SER
2.0% 53.1%
1.8% 44.9%

As can be seen, adding turn information improves performance on
all metrics. This improvement occurs even though turn boundaries
are far from perfectly correlated with segment boundaries. As illus-
trated earlier, turns can contain multiple segments, or segments may
span multiple turns.

5.3. Using Part-of-Speech Information

So far we have used only the identity of words.
It is likely that
segmentation is closely related to syntactic (as opposed to lexical)
structure. Short of using a full-scale parser on the input we could
use the parts of speech (POS) of words as a more suitable represen-
tation from which to predict segment boundaries. Parts of speech
should also generalize much better to contexts containing N-grams
not observed in the training data (assuming the POS of the words
involved is known).

We were able to test this hypothesis by using the POS-tagged ver-
sion of the Switchboard corpus. We built two models based on POS
from this data. Model I had all words replaced by their POS labels
during training and test, and also used turn boundary information.
Model II also used POS labels, but retained the word identities of
certain word classes that were deemed to be particularly relevant to
segmentation. These retained words include ﬁlled pauses, conjunc-
tions, and certain discourse markers such as “okay,” “so,” “well,”
etc. Results are shown in Table 3.

boundaries provide some of the strongest cues for these boundaries.
Apart from these strong lexical cues, it seems to be helpful to ab-
stract from word identity and use POS information instead. In other
words, the tag set could be optimized to provide the right level of
resolution for the segmentation task.

It should be noted that the results for POS-based models are op-
timistic in the sense that for an actual application one would ﬁrst
have to tag the input with POS labels, and then apply the segmenta-
tion model. The actual performance would be degraded by tagging
errors.

5.4. Error Trade-offs

As an aside to our search for useful features for the segmenta-
tion task, we observe that we can optimize any particular language
model by trading off recall performance for false alarm rate, or vice
versa. We did this by biasing the likelihoods of S states by some
constant factor, causing the Viterbi algorithm to choose these states
more often. Table 4 compares two bias values, and shows that the
bias can be used to increase both recall and precision, while also
reducing the segment error rate.

Table 4: Biasing segmentation

Model
Bias = 1
Bias = 2

Recall
76.9%
85.2%

Precision
66.9%
69.2%

FA
SER
1.8% 44.9%
2.7% 37.4%</corps>
		<conclusion>7. CONCLUSIONS

We have argued for the need for automatic speech segmentation al-
gorithms that can identify linguistically motivated, sentence-level
units of speech. We have shown that transcribed speech can be
segmented linguistically with good accuracy by using an N-gram
language model for the locations of the hidden segment boundaries.
We studied several word-level features for possible incorporation
in the model, and found that best performance so far was achieved
with a combination of function ‘cue’ words, POS labels, and turn
markers.

Acknowledgments

This research was supported by DARPA and NSF, under NSF Grant
IRI-9314967. The views herein are those of the authors and should
not be interpreted as representing the policies of DARPA or the
NSF.

3Such an integration can be achieved in a language model using the max-
imum entropy paradigm [1], but this would make the estimation process
considerably more expensive.</conclusion>
		<discussion>6. DISCUSSION

6.1. Error Analysis

To understand what type of errors the segmenter makes, we hand-
checked a set of 200 false alarms generated by the baseline trigram
model. The most frequent type (34%) of false alarm corresponded
to splitting of segments at sentence-internal clause boundaries, e.g.,
false alarms triggered by a conjunction that would be likely to start
a segment. For example, the <s> in the segmentation

i'm not sure how many active volcanos

there are now <s> and and what the amount

of material that they do uh put into the

Table 3: Segmentation performance using POS information

atmosphere

Model
Word-based
POS-based I
POS-based II

Recall
76.9%
68.9%
79.6%

Precision
66.9%
58.5%
73.5%

FA
SER
1.8% 44.9%
2.0% 59.3%
0.9% 39.9%

We see that POS tags alone (Model I) do not result in better segmen-
tations than words. The fact that Model II performs better than both
the all-word based model and the pure POS model indicates that
certain function words that tend to occur in the context of segment

represents a false alarm, presumably triggered by the following co-
ordinating conjunction “and.”

5% of the false alarms could be attributed to ﬁlled pauses at the
end of segments, which were often attached to the following seg-
ment. This actually reﬂects a labeling ambiguity that should not be
counted as an error. Another 7% of the false alarm we deemed to
be labeling errors. Thus, a total of 12% of false alarms could be
considered to be actually correct.

6.2. Other Segmentation Algorithms

2Speakers can talk over each other. We did not model this case sepa-
rately; instead, we adopted the serialization of turns implied by the tran-
scripts.

Our language-model-based segmentation algorithm is only one of
many that could be used to perform the linguistic segmentation task,
given a set of features. Conceptually, segmentation is just another</discussion>
		<biblio>Aucune bibliographie trouvée.</biblio>
	</article>
	<article>
		<preamble>Torres.txt</preamble>
		<titre>Summary Evaluation</titre>
		<auteur>with and without References Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Vel´azquez-Morales</auteur>
		<abstract>evaluation a new content-based method for systems without the human models which is used to produce system rankings. The research is carried out using a new content-based evaluation framework called FRESA to compute a variety of divergences among probability distributions. We apply our comparison framework to various well-established content-based evaluation measures in text summarization such as COVERAGE, their RESPONSIVENESS, PYRAMIDS associations including text generic multi-document summarization in English and French, summarization in English and focus-based multi-document generic single-document summarization in French and Spanish. summarization tasks and ROUGE in various studying Index Terms—Text summarization evaluation, content-based evaluation measures, divergences.</abstract>
		<introduction>issue complex and controversial T EXT summarization evaluation has always been a in computational linguistics. In the last decade, signiﬁcant advances have been made in this ﬁeld as well as various evaluation measures have been designed. Two evaluation campaigns have been led by the U.S. agence DARPA. The ﬁrst one, SUMMAC, ran from 1996 to 1998 under the auspices of the Tipster program [1], and the second one, entitled DUC (Document Understanding Conference) [2], was the main evaluation forum from 2000 until 2007. Nowadays, the Text Analysis Conference (TAC) [3] provides a forum for assessment of different information access technologies including text summarization. Evaluation in text summarization can be extrinsic or intrinsic [4]. In an extrinsic evaluation, the summaries are assessed in the context of an speciﬁc task carried out by a human or a machine. In an intrinsic evaluation, the summaries are evaluated in reference to some ideal model. SUMMAC was mainly extrinsic while DUC and TAC followed an intrinsic evaluation paradigm. In an intrinsic evaluation, an Manuscript received June 8, 2010. Manuscript accepted for publication July 25, 2010. Juan-Manuel and Torres-Moreno ´Ecole France (juan-manuel.torres@univ-avignon.fr). Polytechnique is with LIA/Universit´e de Montr´eal, d’Avignon, Canada Eric with SanJuan (eric.sanjuan@univ-avignon.fr). is LIA/Universit´e d’Avignon, France Horacio Saggion is with DTIC/Universitat Pompeu Fabra, Spain (horacio.saggion@upf.edu). Iria da Cunha is with IULA/Universitat Pompeu Fabra, Spain; LIA/Universit´e d’Avignon, France and Instituto de Ingenier´ıa/UNAM, Mexico (iria.dacunha@upf.edu). Patricia Vel´azquez-Morales (patricia velazquez@yahoo.com). is with VM Labs, France automatically generated summary (peer) has to be compared with one or more reference summaries (models). DUC used an interface called SEE to allow human judges to compare a peer with a model. Thus, judges give a COVERAGE score to each peer produced by a system and the ﬁnal system COVERAGE score is the average of the COVERAGE’s scores asigned. These system’s COVERAGE scores can then be used to rank summarization systems. In the case of query-focused summarization (e.g. when the summary should answer a question or series of questions) a RESPONSIVENESS score is also assigned to each summary, which indicates how responsive the summary is to the question(s). Because manual comparison of peer summaries with model summaries is an arduous and costly process, a body of research has been produced in the last decade on automatic content-based evaluation procedures. Early studies used text similarity measures such as cosine similarity (with or without weighting schema) to compare peer and model summaries [5]. Various vocabulary overlap measures such as n-grams overlap or longest common subsequence between peer and model have also been proposed [6], [7]. The BLEU machine translation evaluation measure [8] has also been tested in summarization [9]. The DUC conferences adopted the ROUGE package for content-based evaluation [10]. ROUGE implements a series of recall measures based on n-gram co-occurrence between a peer summary and a set of model summaries. These measures are used to produce systems’ rank. It has been shown that system rankings, produced by some ROUGE measures (e.g., ROUGE-2, which uses 2-grams), have a correlation with rankings produced using COVERAGE. In recent years the PYRAMIDS evaluation method [11] has been introduced. It is based on the distribution of “content” of a set of model summaries. Summary Content Units (SCUs) are ﬁrst identiﬁed in the model summaries, then each SCU receives a weight which is the number of models containing or expressing the same unit. Peer SCUs are identiﬁed in the peer, matched against model SCUs, and weighted accordingly. The PYRAMIDS score given to a peer is the ratio of the sum of the weights of its units and the sum of the weights of the best possible ideal summary with the same number of SCUs as the peer. The PYRAMIDS scores can be also used for ranking summarization systems. [11] showed that PYRAMIDS scores produced reliable system rankings when multiple (4 or more) models were used and that PYRAMIDS rankings correlate with rankings produced by ROUGE-2 and ROUGE-SU2 (i.e. ROUGE with skip 2-grams). However, this method requires the creation 13Polibits (42) 2010 of models and the identiﬁcation, matching, and weighting of SCUs in both: models and peers. non-random systems, no clear conclusion was reached on the value of each of the studied measures. [12] evaluated the effectiveness of the Jensen-Shannon (J S) [13] theoretic measure in predicting systems ranks in two summarization tasks: query-focused and update summarization. They have shown that ranks produced and those produced by J S measure by PYRAMIDS correlate. However, investigate the effect the measure in summarization tasks such as generic of multi-document 2), biographical summarization (DUC 2004 Task 5), opinion summarization (TAC 2008 OS), and summarization in languages other than English. (DUC 2004 Task summarization they did not In this paper we present a series of experiments aimed at a better understanding of the value of the J S divergence for ranking summarization systems. We have carried out experimentation with the proposed measure and we have veriﬁed that in certain tasks (such as those studied by there is a strong correlation among PYRAMIDS, [12]) RESPONSIVENESS and the J S divergence, but as we will show in this paper, there are datasets in which the correlation is not so strong. We also present experiments in Spanish and French showing positive correlation between the J S and ROUGE which is the de facto evaluation measure used in evaluation of non-English summarization. To the best of our knowledge this is the more extensive set of experiments interpreting the value of evaluation without human models. The rest of the paper is organized in the following way: First in Section II we introduce related work in the area of content-based evaluation identifying the departing point for our inquiry; then in Section III we explain the methodology adopted in our work and the tools and resources used for experimentation. In Section IV we present the experiments carried out together with the results. Section V discusses the results and Section VI concludes the paper and identiﬁes future work.</introduction>
		<corps>II. RELATED WORK

One of the ﬁrst works to use content-based measures in
text summarization evaluation is due to [5], who presented an
evaluation framework to compare rankings of summarization
systems produced by recall and cosine-based measures. They
showed that
there was weak correlation among rankings
produced by recall, but that content-based measures produce
rankings which were strongly correlated. This put forward
the idea of using directly the full document for comparison
purposes in text summarization evaluation. [6] presented a
set of evaluation measures based on the notion of vocabulary
overlap including n-gram overlap, cosine similarity, and
longest common subsequence, and they applied them to
summarization in English and Chinese.
multi-document
However,
the
measures in different summarization tasks. [7] also compared
various evaluation measures based on vocabulary overlap.
Although these measures were able to separate random from

they did not evaluate the performance of

Nowadays,

summarization

a widespread

evaluation
framework is ROUGE [14], which offers a set of statistics
that compare peer
It counts
co-occurrences of n-grams in peer and models to derive a
score. There are several statistics depending on the used
n-grams and the text processing applied to the input texts
(e.g., lemmatization, stop-word removal).

summaries with models.

[15] proposed a method of evaluation based on the
use of “distances” or divergences between two probability
distributions (the distribution of units in the automatic
summary and the distribution of units
in the model
summary). They studied two different Information Theoretic
measures of divergence: the Kullback-Leibler (KL) [16] and
Jensen-Shannon (J S) [13] divergences. KL computes the
divergence between probability distributions P and Q in the
following way:

DKL(P ||Q) =

1
2

(cid:88)

w

Pw log2

Pw
Qw

(1)

While J S divergence is deﬁned as follows:

w

1
2

(cid:88)

Pw log2

+ Qw log2

DJ S (P ||Q) =

2Pw
Pw + Qw

2Qw
Pw + Qw
(2)
These measures can be applied to the distribution of units in
system summaries P and reference summaries Q. The value
obtained may be used as a score for the system summary. The
method has been tested by [15] over the DUC 2002 corpus for
single and multi-document summarization tasks showing good
correlation among divergence measures and both coverage and
ROUGE rankings.

[12] went even further and, as in [5], they proposed to
compare directly the distribution of words in full documents
with the distribution of words in automatic summaries to
derive a content-based evaluation measure. They found a
high correlation between rankings produced using models
and rankings produced without models. This last work is the
departing point for our inquiry into the value of measures that
do not rely on human models.

III. METHODOLOGY

The followed methodology in this paper mirrors the one
adopted in past work (e.g. [5], [7], [12]). Given a particular
summarization task T , p data points to be summarized
with input material {Ii}p−1
i=0 (e.g. document(s), question(s),
topic(s)), s peer summaries {SUMi,k}s−1
k=0 for input i, and
m model summaries {MODELi,j}m−1
for input i, we will
j=0
compare rankings of the s peer summaries produced by various
evaluation measures. Some measures that we use compare
summaries with n of the m models:

MEASUREM (SUMi,k, {MODELi,j}n−1
j=0 )

(3)

Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales14Polibits (42) 2010
while other measures compare peers with all or some of the
input material:

MEASUREM (SUMi,k, I (cid:48)
i)

(4)

where I (cid:48)
is some subset of input Ii. The values produced
i
by the measures for each summary SUMi,k are averaged
for each system k = 0, . . . , s − 1 and these averages are
used to produce a ranking. Rankings are then compared
using Spearman Rank correlation [17] which is used to
measure the degree of association between two variables
whose values are used to rank objects. We have chosen
to use this correlation to compare directly results to those
presented in [12]. Computation of correlations is done using
the Statistics-RankCorrelation-0.12 package1, which computes
the rank correlation between two vectors. We also veriﬁed
the good conformity of the results with the correlation test
of Kendall τ calculated with the statistical software R. The
two nonparametric tests of Spearman and Kendall do not
really stand out as the treatment of ex-æquo. The good
correspondence between the two tests shows that they do not
introduce bias in our analysis. Subsequently will mention only
the ρ of Sperman more widely used in this ﬁeld.

A. Tools

based

evaluation measures

We carry out experimentation using a new summarization
evaluation framework: FRESA –FRamework for Evaluating
Summaries Automatically–, which includes document-based
summary
probabilities
distribution2. As in the ROUGE package, FRESA supports
different n-grams and skip n-grams probability distributions.
The FRESA environment can be used in the evaluation of
summaries in English, French, Spanish and Catalan, and it
integrates ﬁltering and lemmatization in the treatment of
summaries and documents. It is developed in Perl and will
be made publicly available. We also use the ROUGE package
[10] to compute various ROUGE statistics in new datasets.

on

B. Summarization Tasks and Data Sets

We have conducted our experimentation with the following

summarization tasks and data sets:

1) Generic multi-document-summarization

in English
(production of a short summary of a cluster of related
documents) using data from DUC’043,
task 2: 50
clusters, 10 documents each – 294,636 words.

2) Focused-based summarization in English (production of
a short focused multi-document summary focused on the
question “who is X?”, where X is a person’s name) using
data from the DUC’04 task 5: 50 clusters, 10 documents
each plus a target person name – 284,440 words.

3) Update-summarization task that consists of creating a
summary out of a cluster of documents and a topic. Two
sub-tasks are considered here: A) an initial summary has
to be produced based on an initial set of documents and
topic; B) an update summary has to be produced from
a different (but related) cluster assuming documents
used in A) are known. The English TAC’08 Update
Summarization dataset is used, which consists of 48
topics with 20 documents each – 36,911 words.

4) Opinion summarization where systems have to analyze
a set of blog articles and summarize the opinions
about a target
in the articles. The TAC’08 Opinion
Summarization in English4 data set (taken from the
Blogs06 Text Collection) is used: 25 clusters and targets
(i.e., target entity and questions) were used – 1,167,735
words.

5) Generic single-document summarization in Spanish
using the Medicina Cl´ınica5 corpus, which is composed
of 50 medical articles in Spanish, each one with its
corresponding author abstract – 124,929 words.

6) Generic single document summarization in French using
the “Canadien French Sociological Articles” corpus
from the journal Perspectives interdisciplinaires sur le
travail et la sant´e (PISTES)6. It contains 50 sociological
articles in French, each one with its corresponding
author abstract – 381,039 words.

7) Generic multi-document-summarization in French using
data from the RPM27 corpus [18], 20 different themes
consisting of 10 articles and 4 abstracts by reference
thematic – 185,223 words.

For experimentation in the TAC and the DUC datasets we use
directly the peer summaries produced by systems participating
in the evaluations. For experimentation in Spanish and French
(single and multi-document summarization) we have created
summaries at a similar ratio to those of reference using the
following systems:

– ENERTEX [19], a summarizer based on a theory of

textual energy;

– CORTEX [20], a single-document sentence extraction
system for Spanish and French that combines various
statistical measures of relevance (angle between sentence
and topic, various Hamming weights for sentences, etc.)
and applies an optimal decision algorithm for sentence
selection;

– SUMMTERM [21], a terminology-based summarizer that
is used for summarization of medical articles and
uses specialized terminology for scoring and ranking
sentences;

– REG [22], summarization system based on an greedy

algorithm;

1http://search.cpan.org/∼gene/Statistics-RankCorrelation-0.12/
2FRESA is available at: http://lia.univavignon.fr/ﬁleadmin/axes/TALNE/

Ressources.html

3http://www-nlpir.nist.gov/projects/duc/guidelines/2004.html

4http://www.nist.gov/tac/data/index.html
5http://www.elsevier.es/revistas/ctl servlet? f=7032&revistaid=2
6http://www.pistes.uqam.ca/
7http://www-labs.sinequa.com/rpm2

Summary Evaluation with and without References15Polibits (42) 2010
– J S summarizer, a summarization system that scores
and ranks sentences according to their Jensen-Shannon
divergence to the source document;

– a lead-based summarization system that selects the lead

sentences of the document;

– a random-based summarization system that

selects

sentences at random;

– Open Text Summarizer [23], a multi-lingual summarizer

based on the frequency and

– commercial systems: Word, SSSummarizer8, Pertinence9

and Copernic10.

C. Evaluation Measures

The following measures derived from human assessment of

the content of the summaries are used in our experiments:

– COVERAGE is understood as the degree to which one
peer summary conveys the same information as a model
summary [2]. COVERAGE was used in DUC evaluations.
This measure is used as indicated in equation 3 using
human references or models.

– RESPONSIVENESS ranks summaries in a 5-point scale
indicating how well
the summary satisﬁed a given
is used in focused-based
information need [2].
summarization tasks. This measure is used as indicated
in equation 4 since a human judges the summary
with respect
to a given input “user need” (e.g., a
question). RESPONSIVENESS was used in DUC and TAC
evaluations.

It

– PYRAMIDS [11] is a content assessment measure which
compares content units in a peer summary to weighted
content units in a set of model summaries. This
measure is used as indicated in equation 3 using human</corps>
		<conclusion>VI. CONCLUSIONS AND FUTURE WORK

This paper has presented a series of experiments in
content-based measures that do not rely on the use of model
summaries for comparison purposes. We have carried out
extensive experimentation with different summarization tasks
drawing a clearer picture of tasks where the measures could
be applied. This paper makes the following contributions:

– We have shown that if we are only interested in ranking
summarization systems according to the content of their
automatic summaries, there are tasks were models could
be subtituted by the full document in the computation of
the J S measure obtaining reliable rankings. However,
we have also found that
the substitution of models
by full-documents is not always advisable. We have

Summary Evaluation with and without References17Polibits (42) 2010
TABLE II
SPEARMAN ρ OF CONTENT-BASED MEASURES WITH COVERAGE IN DUC’04 TASK 2

Mesure
ROUGE-2
J S

COVERAGE
0.79
0.68

p-value
p < 0.0050
p < 0.0025

TABLE III
SPEARMAN ρ OF CONTENT-BASED MEASURES IN DUC’04 TASK 5

Mesure
ROUGE-2
J S

COVERAGE
0.78
0.40

p-value
p < 0.001
p < 0.050

RESPONSIVENESS
0.44
-0.18

p-value
p < 0.05
p < 0.25

TABLE IV
SPEARMAN ρ OF CONTENT-BASED MEASURES IN TAC’08 OS TASK

Mesure
J S

PYRAMIDS
-0.13

p-value
p < 0.25

RESPONSIVENESS
-0.14

p-value
p < 0.25

TABLE V
SPEARMAN ρ OF CONTENT-BASED MEASURES WITH ROUGE IN THE Medicina Cl´ınica CORPUS (SPANISH)

Mesure
J S
J S2
J S4
J SM

ROUGE-1
0.56
0.88
0.88
0.82

p-value
p < 0.100
p < 0.001
p < 0.001
p < 0.005

ROUGE-2
0.46
0.80
0.80
0.71

p-value
p < 0.100
p < 0.002
p < 0.002
p < 0.020

ROUGE-SU4
0.45
0.81
0.81
0.71

p-value
p < 0.200
p < 0.005
p < 0.005
p < 0.010

found weak correlation among different rankings in
complex summarization tasks such as the summarization
of biographical information and the summarization of
opinions.

– We have also carried out

large-scale experiments in
Spanish and French which show positive medium to
strong correlation among system’s ranks produced by
ROUGE and divergence measures that do not use the
model summaries.

– We have also presented a new framework, FRESA, for
the computation of measures based on J S divergence.
Following the ROUGE approach, FRESA package use
word uni-grams, 2-grams and skip n-grams computing
divergences. This framework will be available to the
community for research purposes.

Although we have made a number of contributions, this paper
leaves many open questions than need to be addressed. In
order to verify correlation between ROUGE and J S, in the
short term we intend to extend our investigation to other
languages such as Portuguese and Chinesse for which we
have access to data and summarization technology. We also
plan to apply FRESA to the rest of the DUC and TAC
summarization tasks, by using several smoothing techniques.
As a novel idea, we contemplate the possibility of adapting
the evaluation framework for the phrase compression task
[29], which, to our knowledge, does not have an efﬁcient
evaluation measure. The main idea is to calculate J S from
an automatically-compressed sentence taking the complete
sentence by reference. In the long term, we plan to incorporate

a representation of
the task/topic in the calculation of
measures. To carry out these comparisons, however, we are
dependent on the existence of references.

FRESA will also be used in the new question-answer task
campaign INEX’2010 (http://www.inex.otago.ac.nz/tracks/qa/
qa.asp) for the evaluation of long answers. This task aims
to answer a question by extraction and agglomeration of
sentences in Wikipedia. This kind of
task corresponds
to those for which we have found a high correlation
among the measures J S and evaluation methods with
human intervention. Moreover, the J S calculation will be
among the summaries produced and a representative set of
relevant passages from Wikipedia. FRESA will be used to
compare three types of systems, although different tasks: the
multi-document summarizer guided by a query, the search
systems targeted information (focused IR) and the question
answering systems.</conclusion>
		<discussion>V. DISCUSSION

The departing point for our inquiry into text summarization
evaluation has been recent work on the use of content-based

evaluation metrics that do not rely on human models but that
compare summary content to input content directly [12]. We
have some positive and some negative results regarding the
direct use of the full document in content-based evaluation.

and

We have veriﬁed that
in

in both generic muti-document
summarization
topic-based multi-document
summarization in English correlation among measures
that use human models
(PYRAMIDS, RESPONSIVENESS
and ROUGE) and a measure that does not use models
(J S divergence) is strong. We have found that correlation
among the same measures is weak for summarization of
biographical information and summarization of opinions in
blogs. We believe that in these cases content-based measures
should be considered, in addition to the input document, the
summarization task (i.e. text-based representation, description)
to better assess the content of the peers [25], the task being a
determinant factor in the selection of content for the summary.
Our multi-lingual experiments in generic single-document
summarization conﬁrm a strong correlation among the
J S divergence and ROUGE measures. It
is worth noting
that ROUGE is
the chosen framework for
presenting content-based evaluation results in non-English
summarization.

in general

For the experiments in Spanish, we are conscious that we
only have one model summary to compare with the peers.
Nevertheless, these models are the corresponding abstracts
written by the authors. As the experiments in [26] show, the
professionals of a specialized domain (as, for example, the
medical domain) adopt similar strategies to summarize their
texts and they tend to choose roughly the same content chunks
for their summaries. Previous studies have shown that author
abstracts are able to reformulate content with ﬁdelity [27] and
these abstracts are ideal candidates for comparison purposes.
Because of this, the summary of the author of a medical article
can be taken as reference for summaries evaluation. It is worth
noting that there is still debate on the number of models to be
used in summarization evaluation [28]. In the French corpus
PISTES, we suspect the situation is similar to the Spanish
case.</discussion>
		<biblio>[1] I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, and
B. Sundheim, “Summac: a text summarization evaluation,” Natural
Language Engineering, vol. 8, no. 1, pp. 43–68, 2002.

[2] P. Over, H. Dang, and D. Harman, “DUC in context,” IPM, vol. 43,

no. 6, pp. 1506–1520, 2007.

[3] Proceedings of the Text Analysis Conference. Gaithesburg, Maryland,

USA: NIST, November 17-19 2008.

[4] K. Sp¨arck Jones and J. Galliers, Evaluating Natural Language
Processing Systems, An Analysis and Review, ser. Lecture Notes in
Computer Science. Springer, 1996, vol. 1083.

[5] R. L. Donaway, K. W. Drummey, and L. A. Mather, “A comparison of
rankings produced by summarization evaluation measures,” in NAACL
Workshop on Automatic Summarization, 2000, pp. 69–78.

[6] H. Saggion, D. Radev, S. Teufel, and W. Lam, “Meta-evaluation
of Summaries in a Cross-lingual Environment using Content-based
Metrics,” in COLING 2002, Taipei, Taiwan, August 2002, pp. 849–855.
[7] D. R. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer, H. Qi, A. C¸ elebi,
D. Liu, and E. Dr´abek, “Evaluation challenges in large-scale document
summarization,” in ACL’03, 2003, pp. 375–382.

[8] K. Papineni, S. Roukos, T. Ward, , and W. J. Zhu, “BLEU: a method
for automatic evaluation of machine translation,” in ACL’02, 2002, pp.
311–318.

[9] K. Pastra and H. Saggion, “Colouring summaries BLEU,” in Evaluation
Initiatives in Natural Language Processing. Budapest, Hungary: EACL,
14 April 2003.

[10] C.-Y. Lin, “ROUGE: A Package for Automatic Evaluation of
Summaries,” in Text Summarization Branches Out: ACL-04 Workshop,
M.-F. Moens and S. Szpakowicz, Eds., Barcelona, July 2004, pp. 74–81.
[11] A. Nenkova and R. J. Passonneau, “Evaluating Content Selection in
Summarization: The Pyramid Method,” in HLT-NAACL, 2004, pp.
145–152.

[12] A. Louis and A. Nenkova, “Automatically Evaluating Content Selection
in Summarization without Human Models,” in Empirical Methods in
Natural Language Processing, Singapore, August 2009, pp. 306–314.
[Online]. Available: http://www.aclweb.org/anthology/D/D09/D09-1032
[13] J. Lin, “Divergence Measures based on the Shannon Entropy,” IEEE
Transactions on Information Theory, vol. 37, no. 145-151, 1991.
[14] C.-Y. Lin and E. Hovy, “Automatic Evaluation of Summaries Using
N-gram Co-occurrence Statistics,” in HLT-NAACL. Morristown, NJ,
USA: Association for Computational Linguistics, 2003, pp. 71–78.
[15] C.-Y. Lin, G. Cao, J. Gao, and J.-Y. Nie, “An information-theoretic
approach to automatic evaluation of summaries,” in HLT-NAACL,
Morristown, USA, 2006, pp. 463–470.

[16] S. Kullback and R. Leibler, “On information and sufﬁciency,” Ann. of

Math. Stat., vol. 22, no. 1, pp. 79–86, 1951.

[17] S. Siegel and N. Castellan, Nonparametric Statistics for the Behavioral

Sciences. McGraw-Hill, 1998.

Summary Evaluation with and without References19Polibits (42) 2010</biblio>
	</article>
	<article>
		<preamble>Word2Vec.txt</preamble>
		<titre>p e</titre>
		<auteur>S 7 ] L C . s c [ 3 v 1 8 7 3 . 1 0 3 1 : v i X r a Efﬁcient Estimation of Word Representations in</auteur>
		<abstract>We propose two novel model architectures for computing continuous vector repre- sentations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previ- ously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art perfor- mance on our test set for measuring syntactic and semantic word similarities. 1</abstract>
		<introduction>Many current NLP systems and techniques treat words as atomic units - there is no notion of similar- ity between words, as these are represented as indices in a vocabulary. This choice has several good reasons - simplicity, robustness and the observation that simple models trained on huge amounts of data outperform complex systems trained on less data. An example is the popular N-gram model used for statistical language modeling - today, it is possible to train N-grams on virtually all available data (trillions of words [3]). However, the simple techniques are at their limits in many tasks. For example, the amount of relevant in-domain data for automatic speech recognition is limited - the performance is usually dominated by the size of high quality transcribed speech data (often just millions of words). In machine translation, the existing corpora for many languages contain only a few billions of words or less. Thus, there are situations where simple scaling up of the basic techniques will not result in any signiﬁcant progress, and we have to focus on more advanced techniques. With progress of machine learning techniques in recent years, it has become possible to train more complex models on much larger data set, and they typically outperform the simple models. Probably the most successful concept is to use distributed representations of words [10]. For example, neural network based language models signiﬁcantly outperform N-gram models [1, 27, 17]. 1.1 Goals of the Paper The main goal of this paper is to introduce techniques that can be used for learning high-quality word vectors from huge data sets with billions of words, and with millions of words in the vocabulary. As far as we know, none of the previously proposed architectures has been successfully trained on more 1 than a few hundred of millions of words, with a modest dimensionality of the word vectors between 50 - 100. We use recently proposed techniques for measuring the quality of the resulting vector representa- tions, with the expectation that not only will similar words tend to be close to each other, but that words can have multiple degrees of similarity [20]. This has been observed earlier in the context of inﬂectional languages - for example, nouns can have multiple word endings, and if we search for similar words in a subspace of the original vector space, it is possible to ﬁnd words that have similar endings [13, 14]. Somewhat surprisingly, it was found that similarity of word representations goes beyond simple syntactic regularities. Using a word offset technique where simple algebraic operations are per- formed on the word vectors, it was shown for example that vector(”King”) - vector(”Man”) + vec- tor(”Woman”) results in a vector that is closest to the vector representation of the word Queen [20]. In this paper, we try to maximize accuracy of these vector operations by developing new model architectures that preserve the linear regularities among words. We design a new comprehensive test set for measuring both syntactic and semantic regularities1, and show that many such regularities can be learned with high accuracy. Moreover, we discuss how training time and accuracy depends on the dimensionality of the word vectors and on the amount of the training data. 1.2 Previous Work Representation of words as continuous vectors has a long history [10, 26, 8]. A very popular model architecture for estimating neural network language model (NNLM) was proposed in [1], where a feedforward neural network with a linear projection layer and a non-linear hidden layer was used to learn jointly the word vector representation and a statistical language model. This work has been followed by many others. Another interesting architecture of NNLM was presented in [13, 14], where the word vectors are ﬁrst learned using neural network with a single hidden layer. The word vectors are then used to train the NNLM. Thus, the word vectors are learned even without constructing the full NNLM. In this work, we directly extend this architecture, and focus just on the ﬁrst step where the word vectors are learned using a simple model. It was later shown that the word vectors can be used to signiﬁcantly improve and simplify many NLP applications [4, 5, 29]. Estimation of the word vectors itself was performed using different model architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting word vectors were made available for future research and comparison2. However, as far as we know, these architectures were signiﬁcantly more computationally expensive for training than the one proposed in [13], with the exception of certain version of log-bilinear model where diagonal weight matrices are used [23].</introduction>
		<corps>2 Model Architectures

Many different types of models were proposed for estimating continuous representations of words,
including the well-known Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA).
In this paper, we focus on distributed representations of words learned by neural networks, as it was
previously shown that they perform signiﬁcantly better than LSA for preserving linear regularities
among words [20, 31]; LDA moreover becomes computationally very expensive on large data sets.

Similar to [18], to compare different model architectures we deﬁne ﬁrst the computational complex-
ity of a model as the number of parameters that need to be accessed to fully train the model. Next,
we will try to maximize the accuracy, while minimizing the computational complexity.

1The test set is available at www.fit.vutbr.cz/˜imikolov/rnnlm/word-test.v1.txt
2http://ronan.collobert.com/senna/

http://metaoptimize.com/projects/wordreprs/
http://www.fit.vutbr.cz/˜imikolov/rnnlm/
http://ai.stanford.edu/˜ehhuang/

2


For all the following models, the training complexity is proportional to

O = E × T × Q,

(1)

where E is number of the training epochs, T is the number of the words in the training set and Q is
deﬁned further for each model architecture. Common choice is E = 3 − 50 and T up to one billion.
All models are trained using stochastic gradient descent and backpropagation [26].

2.1 Feedforward Neural Net Language Model (NNLM)

The probabilistic feedforward neural network language model has been proposed in [1]. It consists
of input, projection, hidden and output layers. At the input layer, N previous words are encoded
using 1-of-V coding, where V is size of the vocabulary. The input layer is then projected to a
projection layer P that has dimensionality N × D, using a shared projection matrix. As only N
inputs are active at any given time, composition of the projection layer is a relatively cheap operation.

The NNLM architecture becomes complex for computation between the projection and the hidden
layer, as values in the projection layer are dense. For a common choice of N = 10, the size of the
projection layer (P ) might be 500 to 2000, while the hidden layer size H is typically 500 to 1000
units. Moreover, the hidden layer is used to compute probability distribution over all the words in the
vocabulary, resulting in an output layer with dimensionality V . Thus, the computational complexity
per each training example is

Q = N × D + N × D × H + H × V,

(2)

where the dominating term is H × V . However, several practical solutions were proposed for
avoiding it; either using hierarchical versions of the softmax [25, 23, 18], or avoiding normalized
models completely by using models that are not normalized during training [4, 9]. With binary tree
representations of the vocabulary, the number of output units that need to be evaluated can go down
to around log2(V ). Thus, most of the complexity is caused by the term N × D × H.

In our models, we use hierarchical softmax where the vocabulary is represented as a Huffman binary
tree. This follows previous observations that the frequency of words works well for obtaining classes
in neural net language models [16]. Huffman trees assign short binary codes to frequent words, and
this further reduces the number of output units that need to be evaluated: while balanced binary tree
would require log2(V ) outputs to be evaluated, the Huffman tree based hierarchical softmax requires
only about log2(U nigram perplexity(V )). For example when the vocabulary size is one million
words, this results in about two times speedup in evaluation. While this is not crucial speedup for
neural network LMs as the computational bottleneck is in the N ×D ×H term, we will later propose
architectures that do not have hidden layers and thus depend heavily on the efﬁciency of the softmax
normalization.

2.2 Recurrent Neural Net Language Model (RNNLM)

Recurrent neural network based language model has been proposed to overcome certain limitations
of the feedforward NNLM, such as the need to specify the context length (the order of the model N ),
and because theoretically RNNs can efﬁciently represent more complex patterns than the shallow
neural networks [15, 2]. The RNN model does not have a projection layer; only input, hidden and
output layer. What is special for this type of model is the recurrent matrix that connects hidden
layer to itself, using time-delayed connections. This allows the recurrent model to form some kind
of short term memory, as information from the past can be represented by the hidden layer state that
gets updated based on the current input and the state of the hidden layer in the previous time step.

The complexity per training example of the RNN model is

Q = H × H + H × V,

(3)

where the word representations D have the same dimensionality as the hidden layer H. Again, the
term H × V can be efﬁciently reduced to H × log2(V ) by using hierarchical softmax. Most of the
complexity then comes from H × H.

3


2.3 Parallel Training of Neural Networks

To train models on huge data sets, we have implemented several models on top of a large-scale
distributed framework called DistBelief [6], including the feedforward NNLM and the new models
proposed in this paper. The framework allows us to run multiple replicas of the same model in
parallel, and each replica synchronizes its gradient updates through a centralized server that keeps
all the parameters. For this parallel training, we use mini-batch asynchronous gradient descent with
an adaptive learning rate procedure called Adagrad [7]. Under this framework, it is common to use
one hundred or more model replicas, each using many CPU cores at different machines in a data
center.

3 New Log-linear Models

In this section, we propose two new model architectures for learning distributed representations
of words that try to minimize computational complexity. The main observation from the previous
section was that most of the complexity is caused by the non-linear hidden layer in the model. While
this is what makes neural networks so attractive, we decided to explore simpler models that might
not be able to represent the data as precisely as neural networks, but can possibly be trained on much
more data efﬁciently.

The new architectures directly follow those proposed in our earlier work [13, 14], where it was
found that neural network language model can be successfully trained in two steps: ﬁrst, continuous
word vectors are learned using simple model, and then the N-gram NNLM is trained on top of these
distributed representations of words. While there has been later substantial amount of work that
focuses on learning word vectors, we consider the approach proposed in [13] to be the simplest one.
Note that related models have been proposed also much earlier [26, 8].

3.1 Continuous Bag-of-Words Model

The ﬁrst proposed architecture is similar to the feedforward NNLM, where the non-linear hidden
layer is removed and the projection layer is shared for all words (not just the projection matrix);
thus, all words get projected into the same position (their vectors are averaged). We call this archi-
tecture a bag-of-words model as the order of words in the history does not inﬂuence the projection.
Furthermore, we also use words from the future; we have obtained the best performance on the task
introduced in the next section by building a log-linear classiﬁer with four future and four history
words at the input, where the training criterion is to correctly classify the current (middle) word.
Training complexity is then

Q = N × D + D × log2(V ).
(4)
We denote this model further as CBOW, as unlike standard bag-of-words model, it uses continuous
distributed representation of the context. The model architecture is shown at Figure 1. Note that the
weight matrix between the input and the projection layer is shared for all word positions in the same
way as in the NNLM.

3.2 Continuous Skip-gram Model

The second architecture is similar to CBOW, but instead of predicting the current word based on the
context, it tries to maximize classiﬁcation of a word based on another word in the same sentence.
More precisely, we use each current word as an input to a log-linear classiﬁer with continuous
projection layer, and predict words within a certain range before and after the current word. We
found that increasing the range improves quality of the resulting word vectors, but it also increases
the computational complexity. Since the more distant words are usually less related to the current
word than those close to it, we give less weight to the distant words by sampling less from those
words in our training examples.

The training complexity of this architecture is proportional to

where C is the maximum distance of the words. Thus, if we choose C = 5, for each training word
we will select randomly a number R in range < 1; C >, and then use R words from history and

Q = C × (D + D × log2(V )),

(5)

4


Figure 1: New model architectures. The CBOW architecture predicts the current word based on the
context, and the Skip-gram predicts surrounding words given the current word.

R words from the future of the current word as correct labels. This will require us to do R × 2
word classiﬁcations, with the current word as input, and each of the R + R words as output. In the
following experiments, we use C = 10.

4 Results

To compare the quality of different versions of word vectors, previous papers typically use a table
showing example words and their most similar words, and understand them intuitively. Although
it is easy to show that word France is similar to Italy and perhaps some other countries, it is much
more challenging when subjecting those vectors in a more complex similarity task, as follows. We
follow previous observation that there can be many different types of similarities between words, for
example, word big is similar to bigger in the same sense that small is similar to smaller. Example
of another type of relationship can be word pairs big - biggest and small - smallest [20]. We further
denote two pairs of words with the same relationship as a question, as we can ask: ”What is the
word that is similar to small in the same sense as biggest is similar to big?”

Somewhat surprisingly, these questions can be answered by performing simple algebraic operations
with the vector representation of words. To ﬁnd a word that is similar to small in the same sense as
biggest is similar to big, we can simply compute vector X = vector(”biggest”) − vector(”big”) +
vector(”small”). Then, we search in the vector space for the word closest to X measured by cosine
distance, and use it as the answer to the question (we discard the input question words during this
search). When the word vectors are well trained, it is possible to ﬁnd the correct answer (word
smallest) using this method.

Finally, we found that when we train high dimensional word vectors on a large amount of data, the
resulting vectors can be used to answer very subtle semantic relationships between words, such as
a city and the country it belongs to, e.g. France is to Paris as Germany is to Berlin. Word vectors
with such semantic relationships could be used to improve many existing NLP applications, such
as machine translation, information retrieval and question answering systems, and may enable other
future applications yet to be invented.

5

w(t-2)w(t+1)w(t-1)w(t+2)w(t)SUM INPUT PROJECTION OUTPUTw(t) INPUT PROJECTION OUTPUTw(t-2)w(t-1)w(t+1)w(t+2) CBOW Skip-gram
Table 1: Examples of ﬁve types of semantic and nine types of syntactic questions in the Semantic-
Syntactic Word Relationship test set.

Type of relationship
Common capital city
All capital cities
Currency
City-in-state
Man-Woman
Adjective to adverb
Opposite
Comparative
Superlative
Present Participle
Nationality adjective
Past tense
Plural nouns
Plural verbs

Word Pair 1

Word Pair 2

Athens
Astana
Angola
Chicago
brother
apparent
possibly
great
easy
think
Switzerland
walking
mouse
work

Greece
Kazakhstan
kwanza
Illinois
sister
apparently
impossibly
greater
easiest
thinking
Swiss
walked
mice
works

Oslo
Harare
Iran
Stockton
grandson
rapid
ethical
tough
lucky
read
Cambodia
swimming
dollar
speak

Norway
Zimbabwe
rial
California
granddaughter
rapidly
unethical
tougher
luckiest
reading
Cambodian
swam
dollars
speaks

4.1 Task Description

To measure quality of the word vectors, we deﬁne a comprehensive test set that contains ﬁve types
of semantic questions, and nine types of syntactic questions. Two examples from each category are
shown in Table 1. Overall, there are 8869 semantic and 10675 syntactic questions. The questions
in each category were created in two steps: ﬁrst, a list of similar word pairs was created manually.
Then, a large list of questions is formed by connecting two word pairs. For example, we made a
list of 68 large American cities and the states they belong to, and formed about 2.5K questions by
picking two word pairs at random. We have included in our test set only single token words, thus
multi-word entities are not present (such as New York).

We evaluate the overall accuracy for all question types, and for each question type separately (se-
mantic, syntactic). Question is assumed to be correctly answered only if the closest word to the
vector computed using the above method is exactly the same as the correct word in the question;
synonyms are thus counted as mistakes. This also means that reaching 100% accuracy is likely
to be impossible, as the current models do not have any input information about word morphology.
However, we believe that usefulness of the word vectors for certain applications should be positively
correlated with this accuracy metric. Further progress can be achieved by incorporating information
about structure of words, especially for the syntactic questions.

4.2 Maximization of Accuracy

We have used a Google News corpus for training the word vectors. This corpus contains about
6B tokens. We have restricted the vocabulary size to 1 million most frequent words. Clearly, we
are facing time constrained optimization problem, as it can be expected that both using more data
and higher dimensional word vectors will improve the accuracy. To estimate the best choice of
model architecture for obtaining as good as possible results quickly, we have ﬁrst evaluated models
trained on subsets of the training data, with vocabulary restricted to the most frequent 30k words.
The results using the CBOW architecture with different choice of word vector dimensionality and
increasing amount of the training data are shown in Table 2.

It can be seen that after some point, adding more dimensions or adding more training data provides
diminishing improvements. So, we have to increase both vector dimensionality and the amount
of the training data together. While this observation might seem trivial, it must be noted that it is
currently popular to train word vectors on relatively large amounts of data, but with insufﬁcient size

6


Table 2: Accuracy on subset of the Semantic-Syntactic Word Relationship test set, using word
vectors from the CBOW architecture with limited vocabulary. Only questions containing words from
the most frequent 30k words are used.

Dimensionality / Training words
50
100
300
600

24M 49M 98M 196M 391M 783M
23.2
13.4
32.2
19.4
45.9
23.2
50.4
24.0

22.5
33.4
43.7
46.6

19.1
28.7
38.6
40.8

18.6
27.8
35.3
36.5

15.7
23.1
29.2
30.1

Table 3: Comparison of architectures using models trained on the same data, with 640-dimensional
word vectors. The accuracies are reported on our Semantic-Syntactic Word Relationship test set,
and on the syntactic relationship test set of [20]

Model
Architecture
RNNLM
NNLM
CBOW
Skip-gram

Semantic-Syntactic Word Relationship test set

Semantic Accuracy [%]
9
23
24
55

Syntactic Accuracy [%]
36
53
64
59

MSR Word Relatedness
Test Set [20]
35
47
61
56

(such as 50 - 100). Given Equation 4, increasing amount of training data twice results in about the
same increase of computational complexity as increasing vector size twice.

For the experiments reported in Tables 2 and 4, we used three training epochs with stochastic gradi-
ent descent and backpropagation. We chose starting learning rate 0.025 and decreased it linearly, so
that it approaches zero at the end of the last training epoch.

4.3 Comparison of Model Architectures

First we compare different model architectures for deriving the word vectors using the same training
data and using the same dimensionality of 640 of the word vectors. In the further experiments, we
use full set of questions in the new Semantic-Syntactic Word Relationship test set, i.e. unrestricted to
the 30k vocabulary. We also include results on a test set introduced in [20] that focuses on syntactic
similarity between words3.

The training data consists of several LDC corpora and is described in detail in [18] (320M words,
82K vocabulary). We used these data to provide a comparison to a previously trained recurrent
neural network language model that took about 8 weeks to train on a single CPU. We trained a feed-
forward NNLM with the same number of 640 hidden units using the DistBelief parallel training [6],
using a history of 8 previous words (thus, the NNLM has more parameters than the RNNLM, as the
projection layer has size 640 × 8).

In Table 3, it can be seen that the word vectors from the RNN (as used in [20]) perform well mostly
on the syntactic questions. The NNLM vectors perform signiﬁcantly better than the RNN - this is
not surprising, as the word vectors in the RNNLM are directly connected to a non-linear hidden
layer. The CBOW architecture works better than the NNLM on the syntactic tasks, and about the
same on the semantic one. Finally, the Skip-gram architecture works slightly worse on the syntactic
task than the CBOW model (but still better than the NNLM), and much better on the semantic part
of the test than all the other models.

Next, we evaluated our models trained using one CPU only and compared the results against publicly
available word vectors. The comparison is given in Table 4. The CBOW model was trained on subset

3We thank Geoff Zweig for providing us the test set.

7


Table 4: Comparison of publicly available word vectors on the Semantic-Syntactic Word Relation-
ship test set, and word vectors from our models. Full vocabularies are used.

Model

Vector
Dimensionality

Training
words

Accuracy [%]

Collobert-Weston NNLM
Turian NNLM
Turian NNLM
Mnih NNLM
Mnih NNLM
Mikolov RNNLM
Mikolov RNNLM
Huang NNLM
Our NNLM
Our NNLM
Our NNLM
CBOW
Skip-gram

50
50
200
50
100
80
640
50
20
50
100
300
300

Semantic
9.3
1.4
1.4
1.8
3.3
4.9
8.6
13.3
12.9
27.9
34.2
15.5
50.0

Syntactic Total
11.0
2.1
1.8
5.8
8.8
12.7
24.6
12.3
20.3
43.2
50.8
36.1
53.3

12.3
2.6
2.2
9.1
13.2
18.4
36.5
11.6
26.4
55.8
64.5
53.1
55.9

660M
37M
37M
37M
37M
320M
320M
990M
6B
6B
6B
783M
783M

Table 5: Comparison of models trained for three epochs on the same data and models trained for
one epoch. Accuracy is reported on the full Semantic-Syntactic data set.

Model

Vector
Dimensionality

Training
words

Accuracy [%]

Training time
[days]

3 epoch CBOW
3 epoch Skip-gram
1 epoch CBOW
1 epoch CBOW
1 epoch CBOW
1 epoch Skip-gram
1 epoch Skip-gram
1 epoch Skip-gram

300
300
300
300
600
300
300
600

Semantic
15.5
50.0
13.8
16.1
15.4
45.6
52.2
56.7

Syntactic Total
36.1
53.3
33.6
36.1
36.2
49.2
53.8
55.5

53.1
55.9
49.9
52.6
53.3
52.2
55.1
54.5

783M
783M
783M
1.6B
783M
783M
1.6B
783M

1
3
0.3
0.6
0.7
1
2
2.5

of the Google News data in about a day, while training time for the Skip-gram model was about three
days.

For experiments reported further, we used just one training epoch (again, we decrease the learning
rate linearly so that it approaches zero at the end of training). Training a model on twice as much
data using one epoch gives comparable or better results than iterating over the same data for three
epochs, as is shown in Table 5, and provides additional small speedup.

4.4 Large Scale Parallel Training of Models

As mentioned earlier, we have implemented various models in a distributed framework called Dis-
tBelief. Below we report the results of several models trained on the Google News 6B data set,
with mini-batch asynchronous gradient descent and the adaptive learning rate procedure called Ada-
grad [7]. We used 50 to 100 model replicas during the training. The number of CPU cores is an

8


Table 6: Comparison of models trained using the DistBelief distributed framework. Note that
training of NNLM with 1000-dimensional vectors would take too long to complete.

Model

Vector
Dimensionality

Training
words

Accuracy [%]

Training time
[days x CPU cores]

NNLM
CBOW
Skip-gram

100
1000
1000

Semantic
34.2
57.3
66.1

Syntactic Total
50.8
63.7
65.6

64.5
68.9
65.1

6B
6B
6B

14 x 180
2 x 140
2.5 x 125

Table 7: Comparison and combination of models on the Microsoft Sentence Completion Challenge.

Architecture
4-gram [32]
Average LSA similarity [32]
Log-bilinear model [24]
RNNLMs [19]
Skip-gram
Skip-gram + RNNLMs

Accuracy [%]
39
49
54.8
55.4
48.0
58.9

estimate since the data center machines are shared with other production tasks, and the usage can
ﬂuctuate quite a bit. Note that due to the overhead of the distributed framework, the CPU usage of
the CBOW model and the Skip-gram model are much closer to each other than their single-machine
implementations. The result are reported in Table 6.

4.5 Microsoft Research Sentence Completion Challenge

The Microsoft Sentence Completion Challenge has been recently introduced as a task for advancing
language modeling and other NLP techniques [32]. This task consists of 1040 sentences, where one
word is missing in each sentence and the goal is to select word that is the most coherent with the
rest of the sentence, given a list of ﬁve reasonable choices. Performance of several techniques has
been already reported on this set, including N-gram models, LSA-based model [32], log-bilinear
model [24] and a combination of recurrent neural networks that currently holds the state of the art
performance of 55.4% accuracy on this benchmark [19].

We have explored the performance of Skip-gram architecture on this task. First, we train the 640-
dimensional model on 50M words provided in [32]. Then, we compute score of each sentence in
the test set by using the unknown word at the input, and predict all surrounding words in a sentence.
The ﬁnal sentence score is then the sum of these individual predictions. Using the sentence scores,
we choose the most likely sentence.

A short summary of some previous results together with the new results is presented in Table 7.
While the Skip-gram model itself does not perform on this task better than LSA similarity, the scores
from this model are complementary to scores obtained with RNNLMs, and a weighted combination
leads to a new state of the art result 58.9% accuracy (59.2% on the development part of the set and
58.7% on the test part of the set).

5 Examples of the Learned Relationships

Table 8 shows words that follow various relationships. We follow the approach described above: the
relationship is deﬁned by subtracting two word vectors, and the result is added to another word. Thus
for example, Paris - France + Italy = Rome. As it can be seen, accuracy is quite good, although
there is clearly a lot of room for further improvements (note that using our accuracy metric that

9


Table 8: Examples of the word pair relationships, using the best word vectors from Table 4 (Skip-
gram model trained on 783M words with 300 dimensionality).

Relationship
France - Paris
big - bigger
Miami - Florida
Einstein - scientist
Sarkozy - France
copper - Cu
Berlusconi - Silvio
Microsoft - Windows
Microsoft - Ballmer
Japan - sushi

Example 1
Italy: Rome
small: larger
Baltimore: Maryland
Messi: midﬁelder
Berlusconi: Italy
zinc: Zn
Sarkozy: Nicolas
Google: Android
Google: Yahoo
Germany: bratwurst

Example 2
Japan: Tokyo
cold: colder
Dallas: Texas
Mozart: violinist
Merkel: Germany
gold: Au
Putin: Medvedev
IBM: Linux
IBM: McNealy
France: tapas

Example 3
Florida: Tallahassee
quick: quicker
Kona: Hawaii
Picasso: painter
Koizumi: Japan
uranium: plutonium
Obama: Barack
Apple: iPhone
Apple: Jobs
USA: pizza

assumes exact match, the results in Table 8 would score only about 60%). We believe that word
vectors trained on even larger data sets with larger dimensionality will perform signiﬁcantly better,
and will enable the development of new innovative applications. Another way to improve accuracy is
to provide more than one example of the relationship. By using ten examples instead of one to form
the relationship vector (we average the individual vectors together), we have observed improvement
of accuracy of our best models by about 10% absolutely on the semantic-syntactic test.

It is also possible to apply the vector operations to solve different tasks. For example, we have
observed good accuracy for selecting out-of-the-list words, by computing average vector for a list of
words, and ﬁnding the most distant word vector. This is a popular type of problems in certain human
intelligence tests. Clearly, there is still a lot of discoveries to be made using these techniques.</corps>
		<conclusion>6 Conclusion

In this paper we studied the quality of vector representations of words derived by various models on
a collection of syntactic and semantic language tasks. We observed that it is possible to train high
quality word vectors using very simple model architectures, compared to the popular neural network
models (both feedforward and recurrent). Because of the much lower computational complexity, it
is possible to compute very accurate high dimensional word vectors from a much larger data set.
Using the DistBelief distributed framework, it should be possible to train the CBOW and Skip-gram
models even on corpora with one trillion words, for basically unlimited size of the vocabulary. That
is several orders of magnitude larger than the best previously published results for similar models.

An interesting task where the word vectors have recently been shown to signiﬁcantly outperform the
previous state of the art is the SemEval-2012 Task 2 [11]. The publicly available RNN vectors were
used together with other techniques to achieve over 50% increase in Spearman’s rank correlation
over the previous best result [31]. The neural network based word vectors were previously applied
to many other NLP tasks, for example sentiment analysis [12] and paraphrase detection [28]. It can
be expected that these applications can beneﬁt from the model architectures described in this paper.

Our ongoing work shows that the word vectors can be successfully applied to automatic extension
of facts in Knowledge Bases, and also for veriﬁcation of correctness of existing facts. Results
In the future, it would be also
from machine translation experiments also look very promising.
interesting to compare our techniques to Latent Relational Analysis [30] and others. We believe that
our comprehensive test set will help the research community to improve the existing techniques for
estimating the word vectors. We also expect that high quality word vectors will become an important
building block for future NLP applications.

10


7 Follow-Up Work

After the initial version of this paper was written, we published single-machine multi-threaded C++
code for computing the word vectors, using both the continuous bag-of-words and skip-gram archi-
tectures4. The training speed is signiﬁcantly higher than reported earlier in this paper, i.e. it is in the
order of billions of words per hour for typical hyperparameter choices. We also published more than
1.4 million vectors that represent named entities, trained on more than 100 billion words. Some of
our follow-up work will be published in an upcoming NIPS 2013 paper [21].</conclusion>
		<discussion>Aucune discussion trouvée.</discussion>
		<biblio>[1] Y. Bengio, R. Ducharme, P. Vincent. A neural probabilistic language model. Journal of Ma-

chine Learning Research, 3:1137-1155, 2003.

[2] Y. Bengio, Y. LeCun. Scaling learning algorithms towards AI. In: Large-Scale Kernel Ma-

chines, MIT Press, 2007.

[3] T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. Large language models in machine
translation. In Proceedings of the Joint Conference on Empirical Methods in Natural Language
Processing and Computational Language Learning, 2007.

[4] R. Collobert and J. Weston. A Uniﬁed Architecture for Natural Language Processing: Deep
Neural Networks with Multitask Learning. In International Conference on Machine Learning,
ICML, 2008.

[5] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa. Natural Lan-
guage Processing (Almost) from Scratch. Journal of Machine Learning Research, 12:2493-
2537, 2011.

[6] J. Dean, G.S. Corrado, R. Monga, K. Chen, M. Devin, Q.V. Le, M.Z. Mao, M.A. Ranzato, A.
Senior, P. Tucker, K. Yang, A. Y. Ng., Large Scale Distributed Deep Networks, NIPS, 2012.

[7] J.C. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and

stochastic optimization. Journal of Machine Learning Research, 2011.

[8] J. Elman. Finding Structure in Time. Cognitive Science, 14, 179-211, 1990.

[9] Eric H. Huang, R. Socher, C. D. Manning and Andrew Y. Ng. Improving Word Representations
via Global Context and Multiple Word Prototypes. In: Proc. Association for Computational
Linguistics, 2012.

[10] G.E. Hinton, J.L. McClelland, D.E. Rumelhart. Distributed representations. In: Parallel dis-
tributed processing: Explorations in the microstructure of cognition. Volume 1: Foundations,
MIT Press, 1986.

[11] D.A. Jurgens, S.M. Mohammad, P.D. Turney, K.J. Holyoak. Semeval-2012 task 2: Measuring
degrees of relational similarity. In: Proceedings of the 6th International Workshop on Semantic
Evaluation (SemEval 2012), 2012.

[12] A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y. Ng, and C. Potts. Learning word vectors for

sentiment analysis. In Proceedings of ACL, 2011.

[13] T. Mikolov. Language Modeling for Speech Recognition in Czech, Masters thesis, Brno Uni-

versity of Technology, 2007.

[14] T. Mikolov, J. Kopeck´y, L. Burget, O. Glembek and J. ˇCernock´y. Neural network based lan-

guage models for higly inﬂective languages, In: Proc. ICASSP 2009.

[15] T. Mikolov, M. Karaﬁ´at, L. Burget, J. ˇCernock´y, S. Khudanpur. Recurrent neural network

based language model, In: Proceedings of Interspeech, 2010.

[16] T. Mikolov, S. Kombrink, L. Burget, J. ˇCernock´y, S. Khudanpur. Extensions of recurrent neural

network language model, In: Proceedings of ICASSP 2011.

[17] T. Mikolov, A. Deoras, S. Kombrink, L. Burget, J. ˇCernock´y. Empirical Evaluation and Com-
bination of Advanced Language Modeling Techniques, In: Proceedings of Interspeech, 2011.

4The code is available at https://code.google.com/p/word2vec/

11


[18] T. Mikolov, A. Deoras, D. Povey, L. Burget, J. ˇCernock´y. Strategies for Training Large Scale
Neural Network Language Models, In: Proc. Automatic Speech Recognition and Understand-
ing, 2011.

[19] T. Mikolov. Statistical Language Models based on Neural Networks. PhD thesis, Brno Univer-

sity of Technology, 2012.

[20] T. Mikolov, W.T. Yih, G. Zweig. Linguistic Regularities in Continuous Space Word Represen-

tations. NAACL HLT 2013.

[21] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed Representations of

Words and Phrases and their Compositionality. Accepted to NIPS 2013.

[22] A. Mnih, G. Hinton. Three new graphical models for statistical language modelling. ICML,

2007.

[23] A. Mnih, G. Hinton. A Scalable Hierarchical Distributed Language Model. Advances in Neural

Information Processing Systems 21, MIT Press, 2009.

[24] A. Mnih, Y.W. Teh. A fast and simple algorithm for training neural probabilistic language

models. ICML, 2012.

[25] F. Morin, Y. Bengio. Hierarchical Probabilistic Neural Network Language Model. AISTATS,

2005.

[26] D. E. Rumelhart, G. E. Hinton, R. J. Williams. Learning internal representations by back-

propagating errors. Nature, 323:533.536, 1986.

[27] H. Schwenk. Continuous space language models. Computer Speech and Language, vol. 21,

2007.

[28] R. Socher, E.H. Huang, J. Pennington, A.Y. Ng, and C.D. Manning. Dynamic Pooling and

Unfolding Recursive Autoencoders for Paraphrase Detection. In NIPS, 2011.

[29] J. Turian, L. Ratinov, Y. Bengio. Word Representations: A Simple and General Method for
Semi-Supervised Learning. In: Proc. Association for Computational Linguistics, 2010.

[30] P. D. Turney. Measuring Semantic Similarity by Latent Relational Analysis. In: Proc. Interna-

tional Joint Conference on Artiﬁcial Intelligence, 2005.

[31] A. Zhila, W.T. Yih, C. Meek, G. Zweig, T. Mikolov. Combining Heterogeneous Models for

Measuring Relational Similarity. NAACL HLT 2013.

[32] G. Zweig, C.J.C. Burges. The Microsoft Research Sentence Completion Challenge, Microsoft

Research Technical Report MSR-TR-2011-129, 2011.

12</biblio>
	</article>
</articles>
