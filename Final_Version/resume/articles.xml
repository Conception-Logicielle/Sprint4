<articles>
    <article>
        <preamble>ACL2004-HEADLINE.txt</preamble>
        <titre>Hybrid Headlines: Combining Topics and Sentence Compression</titre>
        <auteur>David Zajic, Bonnie Dorr, Stacy President</auteur>
        <abstract>This paper presents Topiary, a headline- generation system that creates very short, informative
            summaries for news stories by combining sentence compres- sion and unsupervised topic discovery. We will
            show that the combination of linguistically motivated sentence com- pression with statistically selected
            topic terms performs better than either alone, according to some automatic summary evaluation measures. In
            addition we de- scribe experimental results establishing an appropriate extrinsic task on which to measure
            the effect of summarization on human performance. We demonstrate the usefulness of headlines in compar- ison
            to full texts in the context of this extrinsic task.
        </abstract>
        <introduction>In this paper we present Topiary, a headline- generation system that creates very short, infor-
            mative summaries for news stories by combining sentence compression and unsupervised topic dis- covery.
            Hedge Trimmer performs sentence com- pression by removing constituents from a parse tree of the lead
            sentence according to a set of linguistically-motivated heuristics until a length threshold is reached.
            Unsupervised Topic Discov- ery is a statistical method for deriving a set of topic models from a document
            corpus, assigning mean- ingful names to the topic models, and associating sets of topics with speciﬁc
            documents. The top- ics and sentence compressions are combined in a manner that preserves the advantages of
            each ap- proach: the ﬂuency and event-oriented informa- tion from the lead sentence with the broader cov-
            erage of the topic models. The next section presents previous work in the area of automatic summarization.
            Following this we describe Hedge Trimmer and Unsupervised Topic Discovery in more detail, and describe the
            algorithm for combining sentence compression with topics. Next we show that Topiary scores higher than
            either Hedge Trimmer or Unsuper- vised Topic Discovery alone according to certain automatic evaluation tools
            for summarization. Fi- nally we propose event tracking as an extrinsic task using automatic summarization
            for measur- ing how human performance is affected by auto- matic summarization, and for correlating human
            peformance with automatic evaluation tools. We describe an experiment that supports event track- ing as an
            appropriate task for this purpose, and show results that suggest that a well-written hu- man headline is
            nearly as useful for event tracking as the full text.
        </introduction>
        <corps>2 Previous Work

            Hedge Trimmer is a sentence compression algo-
            rithm based on linguistically-motivated heuristics.
            Previous work on sentence compression (Knight
            and Marcu, 2000) uses a noisy-channel model to
            ﬁnd the most probable short string that gener-
            ated the observed full sentence. Other work (Eu-
            ler, 2002) combines a word-list with syntactic in-


            formation to decide which words and phrases to
            cancel. Our approach differs from Knight’s in
            that we do not use a statistical model, so we do
            not require any prior training on a large corpus
            of story/headline pairs. Topiary shares with Eu-
            ler the combination of topic lists and sentence
            compression. However Euler uses the topic lists
            to guide sentence selection and compression to-
            wards a query-speciﬁc summary, whereas Topiary
            uses topics to augment the concept coverage of a
            generic summary.

            Summaries can also consist of lists of words or
            short phrases indicating that the topic or concept
            they denote is important in the document. Extrac-
            tive topic summaries consist of keywords or key
            phrases that occur in the document. (Bergler et al.,
            2003) achieves this by choosing noun phrases that
            represent the most important text entities, as repre-
            sented by noun phrase coreference chains. (Zhou
            and Hovy, 2003) imposes ﬂuency onto a topic list
            by ﬁnding phrase clusters early in the text that con-
            tain important topic words found throughout the
            text. In text categorization documents are assigned
            to pre-deﬁned categories. This is equivalent to as-
            signing topics to a document from a static topic
            list, so the words in the summary need not actually
            appear in the document. (Lewis, 1992) describes
            a probabilistic feature-based method for assigning
            Reuters topics to news stories. OnTopic (Schwartz
            et al., 1997) uses a HMM to assign topics from a
            topic-annotated corpus to a new document.

            3 Algorithm Description

            Topiary produces headlines by combining the out-
            put of Hedge Trimmer, a sentence compression
            algorithm, with Unsupervised Topic Detection
            (UTD). In this section we will give brief descrip-
            tions of Hedge Trimmer, recent modiﬁcations to
            Hedge Trimmer, and UTD. We will then describe
            how Hedge Trimmer and UTD are combined.

            3.1 Hedge Trimmer

            Hedge Trimmer (Dorr et al., 2003b) generates
            a headline for a news story by compressing the
            lead (or main) topic sentence according to a lin-
            guistically motivated algorithm. For news stories,
            the ﬁrst sentence of the document is taken to be
            the lead sentence. The compression consists of

            parsing the sentence using the BBN SIFT parser
            (Miller et al., 1998) and removing low-content
            syntactic constituents. Some constituents, such as
            certain determiners (the, a) and time expressions
            are always removed, because they rarely occur in
            human-generated headlines and are low-content
            in comparison to other constituents. Other con-
            stituents are removed one-by-one until a length
            threshold has been reached. These include, among
            others, relative clauses, verb-phrase conjunction,
            preposed adjuncts and prepositional phrases that
            do not contain named entities. 1 The threshold can
            be speciﬁed either in number of words or number
            of characters. If the threshold is speciﬁed in num-
            ber of characters, Hedge Trimmer will not include
            partial words.

            3.2 Recent Hedge Trimmer Work

            Recently we have investigated a rendering of the
            summary as “Headlinese” (M˚ardh, 1980) in which
            certain constituents are dropped with no loss of
            meaning. The result of this investigation has been
            used to enhance Hedge Trimmer, most notably
            the removal of certain instances of have and be.
            For example, the previous headline generator pro-
            duced summaries such as Sentence (2), whereas
            the have/be removal produces (3).

            (1)

            Input: The senior Olympic ofﬁcial who lev-
            eled stunning allegations of corruption within
            the IOC said Sunday he had been “muzzled”
            by president Juan Antonio Samaranch and
            might be thrown out of the organization.

            (2) Without participle have/be removal: Senior

            Olympic ofﬁcial said he had been muzzled

            (3) With participle have/be removal:

            Senior
            Olympic ofﬁcial said he muzzled by presi-
            dent Juan Antonio Samaranch

            Have and be are removed if they are part of a past
            or present participle construction.
            In this exam-
            ple, the removal of had been allows a high-content
            constituent by president Juan Antonio Samaranch
            to ﬁt into the headline.

            The removal of forms of to be allows Hedge
            Trimmer to produce headlines that concentrate

            1More details of the Hedge Trimmer algorithm can be

            found in (Dorr et al., 2003b) and (Dorr et al., 2003a).


            more information in the allowed space. The re-
            moval of forms of to be results in sentences that
            are not grammatical in general English, but are
            typical of Headlinese English. For example, sen-
            tences (5), (6) and all other examples in this paper
            were trimmed to ﬁt in 75 characters.

            emergency shortening methods which are only
            to be used when the alternative is truncating the
            headline after the threshold, possibly cutting the
            middle of a word. These include removal of ad-
            verbs and adverbial phrases, adjectives and adjec-
            tive phrases, and nouns that modify other nouns.

            (4)

            Input: Leading maxi yachts Brindabella, Say-
            onara and Marchioness were locked in a
            three-way duel down the New South Wales
            state coast Saturday as the Sydney to Hobart
            ﬂeet faced deteriorating weather.

            (5) Without to be removal: Sayonara and Mar-

            chioness were locked in three

            (6) With to be removal: Leading maxi yachts
            and Marchioness

            Brindabella Sayonara
            locked in three

            When have and be occur with a modal verb, the
            modal verb is also removed. Sentence (9) shows
            an example of this.
            It could be argued that by
            removing modals such as should and would the
            meaning is vitally changed. The intended use of
            the headline must be considered. If the headlines
            are to be used for determining query relevance, re-
            moval of modals may not hinder the user while
            making room for additional high-content words
            may help.

            (7)

            Input: Organizers of December’s Asian
            Games have dismissed press reports that a
            sports complex would not be completed on
            time, saying preparations are well in hand, a
            local newspaper said Friday.

            (8) Without Modal-Have/Be Removal: Organiz-
            ers have dismissed press reports saying

            (9) With Modal-Have/Be Removal: Organizers
            dismissed press reports sports complex not
            completed saying

            In addition when it or there appears as a subject
            with a form of be or have, as in extraposition (It
            was clear that the thief was hungry) or existential
            clauses (There have been a spate of dog maulings),
            the subject and the verb are removed.

            Finally,

            for situations in which the length
            threshold is a hard constraint, we added some

            3.3 Unsupervised Topic Discovery

            Unsupervised Topic Discovery (UTD) is used
            when we do not have a corpus annotated with top-
            ics. It takes as input a large unannotated corpus
            in any language and automatically creates a set of
            topic models with meaningful names. The algo-
            rithm has several stages. First, it analyzes the cor-
            pus to ﬁnd strings of words that occur frequently.
            (It does this using a Minimum Description Length
            criterion.) These are frequently phrases that are
            meaningful names of topics.

            Second, it ﬁnds the high-content words in each
            document (using a modiﬁed tf.idf measure). These
            It
            are possible topic names for each document.
            keeps only those names that occur in at least four
            different documents. These are taken to be an ini-
            tial set of topic names.

            In the third stage UTD trains topic models cor-
            responding to these topic names. The modiﬁed
            EM procedure of OnTopicTMis used to determine
            which words in the documents often signify these
            topic names. This produces topic models.

            Fourth, these topic models are used to ﬁnd the
            most likely topics for each document. This often
            adds new topics to documents, even though the
            topic name did not appear in the document.

            We found, in various experiments, that the top-
            ics derived by this procedure were usually mean-
            ingful and that the topic assignment was about as
            good as when the topics were derived from a cor-
            pus that was annotated by people. We have also
            used this procedure on different languages and
            shown the same behavior.

            Sentence (10) is a topic list generated for a story
            about the investigation into the bombing of the
            U.S. Embassy in Nairobi on August 7, 1998.

            (10) BIN LADEN EMBASSY BOMBING PO-
            LICE OFFICIALS PRISON HOUSE FIRE
            KABILA


            3.4 Combination of Hedge Trimmer and

            Topics: Topiary

            The Hedge Trimmer algorithm is constrained to
            take its headline from a single sentence. It is of-
            ten the case that there is no single sentence that
            contains all the important information in a story.
            The information can be spread over two or three
            sentences, with pronouns or ellipsis used to link
            them. In addition, our algorithms do not always
            select the ideal sentence and trim it perfectly.

            Topics alone also have drawbacks. UTD rarely
            generates any topic names that are verbs. Thus
            topic lists are good at indicating the general sub-
            ject are but rarely give any direct indication of
            what events took place.

            Topiary is a modiﬁcation of the enhanced
            Hedge Trimmer algorithm to take a list of top-
            ics with relevance scores as additional input. The
            compression threshold is lowered so that there
            will be room for the highest scoring topic term
            that isn’t already in the headline. This amount of
            threshold lowering is dynamic, because the trim-
            ming of the sentence can remove a previously in-
            eligible high-scoring topic term from the headline.
            After trimming is complete, additional topic terms
            that do not occur in the headline are added to use
            up any remaining space.

            This often results in one or more main topics
            about the story and a short sentence that says what
            happened concerning them. The combination is
            often more concise than a fully ﬂuent sentence and
            compensates for the fact that the topic and the de-
            scription of what happened to it do not appear in
            the same sentence in the original story.

            Sentences (11) and (12) are the output of Hedge
            Trimmer and Topiary for the same story for which
            the topics in Sentence (10) were generated.

            (11) FBI agents this week began questioning rel-

            atives of the victims

            (12) BIN LADEN EMBASSY BOMBING FBI
            agents this week began questioning relatives

            Topiary was submitted to the Document Under-
            standing Conference Workshop. Figure 1 shows
            how Topiary peformed in comparison with other
            DUC2004 participants on task 1, using ROUGE.
            Task 1 was to produce a summary for a single news

            document no more than than 75 characters. The
            different ROUGE variants are sorted by overall
            performance of the systems. The key observations
            are that there was a wide range of performance
            among the submitted systems, and that Topiary
            scored ﬁrst or second among the automatic sys-
            tems on each ROUGE measure.

            4 Evaluation

            We used two automatic evaluation systems, BLEU
            (Papineni et al., 2002) and ROUGE (Lin and
            Hovy, 2003), to evaluate nine variants of our head-
            line generation systems. Both measures make n-
            gram comparisons of the candidate systems to a
            set of reference summaries.
            In our evaluations
            four reference summaries for each document were
            used. The nine variants were run on 489 stories
            from the DUC2004 single-document summariza-
            tion headline generation task. The threshold was
            75 characters, and longer headlines were truncated
            to 75 characters. We also evaluated a baseline
            that consisted of the ﬁrst 75 characters of the doc-
            ument. The systems and the average lengths of
            the headlines they produced are shown in Table
            1. Trimmer headlines tend to be shorter than the
            threshold because Trimmer removes constituents
            until the length is below the threshold. Sometimes
            it must remove a large constituent in order to get
            below the threshold. Topiary is able to make full
            use of the space by ﬁlling in topic words.

            4.1 ROUGE

            ROUGE is a recall-based measure for summa-
            rizations. This automatic metric counts the num-
            ber of n-grams in the reference summaries that
            occur in the candidate and divides by the num-
            ber of n-grams in the reference summaries. The
            size of the n-grams used by ROUGE is conﬁg-
            urable. ROUGE-n uses 1-grams through n-grams.
            ROUGE-L is based on longest common subse-
            quences, and ROUGE-W-1.2 is based on weighted
            longest common subsequences with a weighting
            of 1.2 on consecutive matches of length greater
            than 1.

            The ROUGE scores for the nine systems and the
            baseline are shown in Table 2. Under ROUGE-
            1 the Topiary variants scored signiﬁcantly higher
            than the Trimmer variants, and both scored signif-


            0.35

            0.3

            0.25

            0.2

            0.15

            0.1

            0.05

            0

            ROUGE-1

            ROUGE-L

            ROUGE-W-1.2

            ROUGE-2

            ROUGE-3

            ROUGE-4

            Automatic Summaries

            Reference Summaries

            Topiary

            Figure 1: ROUGE Scores for DUC2004 Automatic Summaries, Reference Summaries and Topiary

            System
            Trim

            Trim.E

            Trim.HB

            Trim.HB.E

            Top

            Top.E

            Top.HB

            Top.HB.E

            UTD

            Description
            Trimmer
            no have/be removal
            no emergency shortening
            Trimmer
            no have/be removal
            emergency shortening
            Trimmer
            have/be removal
            no emergency shortening
            Trimmer
            have/be removal
            emergency shortening
            Topiary
            no have/be removal
            no emergency shortening
            Topiary
            no have/be removal
            emergency shortening
            Topiary
            have/be removal
            no emergency shortening
            Topiary
            have/be removal
            emergency shortening
            UTD Topics

            Words Chars
            8.7

            57.3

            8.7

            57.1

            8.6

            57.7

            8.6

            57.4

            10.8

            73.3

            10.8

            73.2

            10.7

            73.2

            10.7

            73.2

            9.5

            71.1

            Table 1: Systems and Headline Lengths

            icantly higher than the UTD topic lists with 95%
            conﬁdence. Since ﬂuency is not measured at all
            by unigrams, we must conclude that the Trimmer
            headlines, by selecting the lead sentence, included
            more or better topic words than UTD. The high-
            est scoring UTD topics tend to be very meaning-
            ful while the ﬁfth and lower scoring topics tend
            to be very noisy. Thus the higher scores of Topi-
            ary can be attributed to including only the best of
            the UTD topics while preserving the lead sentence
            topics. The same groupings occur with ROUGE-L
            and ROUGE-W, indicating that the longest com-
            mon subsequences are dominated by sequences of
            length one.

            Under the higher order ROUGE evaluations
            the systems group by the presence or absence of
            have/be removal, with higher scores going to sys-
            tems in which have/be removal was performed.
            This indicates that the removal of these light con-
            tent verbs makes the summaries more like the lan-
            guage of headlines. The value of emergency short-
            ening over truncation is not clear.


            Top.HB.E
            Top.HB
            Top.E
            Top
            baseline
            Trim.HB.E
            Trim.HB
            Trim.E
            Trim
            UTD

            ROUGE-1 ROUGE-2 ROUGE-3
            0.06449
            0.24914
            0.06595
            0.24873
            0.06169
            0.24812
            0.06309
            0.24621
            0.06370
            0.22136
            0.06571
            0.20415
            0.06565
            0.20380
            0.06226
            0.20105
            0.06283
            0.20061
            0.01585
            0.15913

            0.02122
            0.02267
            0.01874
            0.01995
            0.02118
            0.02527
            0.02508
            0.02221
            0.02266
            0.00087

            ROUGE-4 ROUGE-L ROUGE-W-1.2
            0.00712
            0.00826
            0.00562
            0.00639
            0.00707
            0.00950
            0.00945
            0.00774
            0.00792
            0.00000

            0.11891
            0.11970
            0.11837
            0.11861
            0.16955
            0.11127
            0.11118
            0.11003
            0.10996
            0.07797

            0.19951
            0.20061
            0.19856
            0.19856
            0.11738
            0.18506
            0.18472
            0.18287
            0.18248
            0.13041

            Table 2: ROUGE Scores sorted by ROUGE-1

            4.2 BLEU

            BLEU is a system for automatic evaluation of ma-
            chine translation that uses a modiﬁed n-gram pre-
            cision measure to compare machine translations to
            reference human translations. This automatic met-
            ric counts the number of n-grams in the candidate
            that occur in any of the reference summaries and
            divides by the number of n-grams in the candidate.
            The size of the n-grams used by BLEU is conﬁg-
            urable. BLEU-n uses 1-grams through n-grams. In
            our evaluation of headline generation systems, we
            treat summarization as a type of translation from
            a verbose language to a concise one, and compare
            automatically generated headlines to human gen-
            erated headlines.

            The BLEU scores for the nine systems and
            the baseline are shown in Table 3. For BLEU-1
            the Topiary variants score signiﬁcantly better than
            the Trimmer variants with 95% conﬁdence. Un-
            der BLEU-2 the Topiary scores are higher than
            the Trimmer scores, but not signiﬁcantly. Under
            BLEU-4 the Trimmer variants score slightly but
            not signiﬁcantly higher than the Topiary variants,
            and at BLEU-3 there is no clear pattern. Trim-
            mer and Topiary variants score signiﬁcantly higher
            than UTD for all settings of BLEU with 95% con-
            ﬁdence.

            5 Extrinsic Task

            For an automatic summarization evaluation tool to
            be of use to developers it must be shown to cor-
            relate well with human performance on a speciﬁc
            extrinsic task. In selecting the extrinsic task it is
            important that the task be unambiguous enough
            that subjects can perform it with a high level of
            If the task is so difﬁcult that sub-
            agreement.

            Top.HB.E
            Top.HB
            Top.E
            Top
            Trim.HB.E
            Trim.HB
            baseline
            Trim.E
            Trim
            UTD

            BLEU-1 BLEU-2 BLEU-3 BLEU-4
            0.4368
            0.4362
            0.4310
            0.4288
            0.3712
            0.3705
            0.3695
            0.3636
            0.3635
            0.2859

            0.0849
            0.0885
            0.0739
            0.0832
            0.0939
            0.0943
            0.0853
            0.0897
            0.0922
            0.0000

            0.2443
            0.2463
            0.2389
            0.2415
            0.2333
            0.2331
            0.2214
            0.2285
            0.2297
            0.0954

            0.1443
            0.1476
            0.1381
            0.1417
            0.1495
            0.1493
            0.1372
            0.1442
            0.1461
            0.0263

            Table 3: BLEU Scores sorted by BLEU-1

            jects cannot perform with a high level of agree-
            ment – even when they are shown the entire docu-
            ment – it will not be possible to detect signiﬁcant
            differences among different summarization meth-
            ods because the amount of variation due to noise
            will overshadow the variation due to summariza-
            tion method.

            In an earlier experiment we attempted to use
            document selection in the context of informa-
            tion retrieval as an extrinsic task. Subjects were
            asked to decide if a document was highly rele-
            vant, somewhat relevant or not relevant to a given
            query. However we found that subjects who had
            been shown the entire document were only able
            to agree with each other 75% of the time and
            agreed with the allegedly correct answers only
            70% of the time. We were unable to draw any
        </corps>
        <conclusion>conclusions about the relative performance of the
            summarization systems, and thus were not able
            to make any correlations between human perfor-
            mance and scores on automatic summarization
            evaluation tools. For more details see (Zajic et al.,
            2004).

            We propose a more constrained type of docu-


            ment relevance judgment as an appropriate extrin-
            sic task for evaluating human performance using
            automatic summarizations. The task, event track-
            ing, has been reported in NIST TDT evaluations
            to provide the basis for more reliable results. Sub-
            jects are asked to decide if a document contains
            information related to a particular event in a spe-
            ciﬁc domain. The subject is told about a speciﬁc
            event, such as the bombing of the Murrah Federal
            Building in Oklahoma City. A detailed descrip-
            tion is given about what information is considered
            relevent to an event in the given domain. For in-
            stance, in the criminal case domain, information
            about the crime, the investigation, the arrest, the
            trial and the sentence are relevant.

            We performed a small event tracking experi-
            ment to compare human performance using full
            news story text against performance using human-
            generated headlines of the same stories. Seven
            events and twenty documents per event were cho-
            sen from the 1999 Topic Detection and Tracking
            (TDT3) corpus. Four subjects were asked to judge
            the full news story texts or story headlines as rel-
            evant or not relevant to each speciﬁed event. The
            documents in the TDT3 corpus were already an-
            notated as relevant or not relevant to each event
            by NIST annotators. The NIST annotations were
            taken to be the correct answers by which to judge
            the overall performance of the subjects. The sub-
            jects were shown a practice event, three events
            with full story text and three events with story
            headlines.

            We calculated average agreement between sub-
            jects as the number of documents on which two
            subjects made the same judgment divided by the
            number of documents on which the two subjects
            had both made judgments. The average agreement
            between subjects was 86% for full story texts and
            80% for headlines. The average agreement with
            the NIST annotations was slightly higher when us-
            ing the full story text than the headline, with text
            producing 86% overall agreement with NIST and
            headlines producing 84% agreement with NIST.
            Use of headlines resulted in a signiﬁcant increase
            in speed. Subjects spent an average of 30 seconds
            per document when shown the entire text, but only
            7.7 seconds per document when shown the head-
            line. Table 4 shows the precision, recall and (cid:0)(cid:2)(cid:1)

            Full Text
            Headline

            Precision
            0.831
            0.842

            Recall
            0.900
            0.842

            (cid:3)(cid:5)(cid:4)(cid:7)(cid:6)

            0.864
            0.842

            Table 4: Results of Event Tracking Experiment

            with (cid:9)(cid:11)(cid:10)(cid:13)(cid:12)(cid:5)(cid:14)(cid:16)(cid:15) .

            The small difference in NIST agreement be-
            tween full texts and headlines seems to suggest
            that the best human-written headlines can supply
            sufﬁcient information for performing event track-
            ing. However it is possible that subjects found the
            task of reading entire texts dull, and allowed their
            performance to diminish as they grew tired.

            Full texts yielded a higher recall than head-
            lines, which is not surprising. However headlines
            yielded a slightly higher precision than full texts
            which means that subjects were able to reject non-
            relevant documents as well with headlines as they
            could by reading the entire document. We ob-
            served that subjects sometimes marked documents
            as relevant if the full text contained even a brief
            mention of the event or any detail that could be
            construed as satisfying the domain description. If
            avoiding false positives (or increasing precision) is
            an important goal, these results suggest that use of
            headlines provides an advantage over reading the
            entire text.

            Further event tracking experiments will include
            a variety of methods for automatic summariza-
            tion. This will give us the ability to compare hu-
            man performance using the summarization meth-
            ods against one another and against human perfor-
            mance using full text. We do not expect that any
            summarization method will allow humans to per-
            form event tracking better than reading the entire
            document, however we hope that we can improve
            human performance time while introducing only
            a small, acceptable loss in performance. We also
            plan to calibrate automatic summarization evalu-
            ation tools, such as BLEU and ROUGE, to ac-
            tual human performance on event tracking for each
            METHOD.

            6 Conclusions and Future Work

            We have shown the effectiveness of combining
            sentence compression and topic lists to construct
            informative summaries. We have compared three

            (cid:8)

            the American Association for Artiﬁcial Intelligence
            AAAI2000, Austin, Texas.

            David Lewis. 1992. An evaluation of phrasal and clus-
            tered representations on a text categorization task.
            In Proceedings of the 15th annual international
            ACM SIGIR conference on Research and develop-
            ment in information retrieval, pages 37–50, Copen-
            hagen, Denmark.

            Chin-Yew Lin and Eduard Hovy.

            2003. Auto-
            matic Evaluation of Summaries Using N-gram Co-
            Occurrences Statistics. In Proceedings of the Con-
            ference of the North American Chapter of the As-
            sociation for Computational Linguistics, Edmonton,
            Alberta.

            Ingrid M˚ardh. 1980. Headlinese: On the Grammar of

            English Front Page Headlines. Malmo.

            S. Miller, M. Crystal, H. Fox, L. Ramshaw,
            R. Schwartz, R. Stone, and R. Weischedel. 1998.
            Algorithms that Learn to Extract Information; BBN:
            Description of the SIFT System as Used for MUC-7.
            In Proceedings of the MUC-7.

            K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
            Bleu: a Method for Automatic Evaluation of Ma-
            chine Translation. In Proceedings of Association of
            Computational Linguistics, Philadelphia, PA.

            R. Schwartz, T. Imai, F. Jubala, L. Nguyen, and
            J. Makhoul. 1997. A maximum likelihood model
            for topic classiﬁcation of broadcast news.
            In
            Eurospeech-97, Rhodes, Greece.

            David Zajic, Bonnie Dorr, Richard Schwartz, and Stacy
            President. 2004. Headline evaluation experiment
            results, umiacs-tr-2004-18. Technical report, Uni-
            versity of Maryland Institute for Advanced Comput-
            ing Studies, College Park, Maryland.

            Liang Zhou and Eduard Hovy. 2003. Headline sum-
            marization at isi. In Proceedings of the 2003 Doc-
            ument Understanding Conference, Draft Papers,
            pages 174–178, Edmonton, Candada.

            approaches to automatic headline generation (Top-
            iary, Hedge Trimmer and Unsupervised Topic Dis-
            covery) using two automatic summarization evalu-
            ation tools (BLEU and ROUGE). We have stressed
            the importance of correlating automatic evalua-
            tions with human performance of an extrinsic task,
            and have proposed event tracking as an appropri-
            ate task for this purpose.

            We plan to perform a study in which Topiary,
            Hedge Trimmer, Unsupervised Topic Discovery
            and other summarization methods will be evalu-
            ated in the context of event tracking. We also plan
            to extend the tools described in this paper to the
            domains of transcribed broadcast news and cross-
            language headline generation.

            Acknowledgements

            The University of Maryland authors are sup-
            ported,
            in part, by BBNT Contract 020124-
            7157, DARPA/ITO Contract N66001-97-C-8540,
            and NSF CISE Research Infrastructure Award
            EIA0130422.
        </conclusion>
        <discussion>Aucune discussion trouvée.</discussion>
        <biblio>Sabine Bergler, Ren´e Witte, Michelle Khalife,
            Zhuoyan Li, and Frank Rudzicz.
            2003. Using
            knowledge-poor coreference resolution for text sum-
            marization. In Proceedings of the 2003 Document
            Understanding Conference, Draft Papers, pages 85–
            92, Edmonton, Candada.

            Bonnie Dorr, David Zajic, and Richard Schwartz.
            Cross-language headline generation for
            2003a.
            hindi. ACM Transactions on Asian Language Infor-
            mation Processing (TALIP), 2:2.

            Bonnie Dorr, David Zajic, and Richard Schwartz.
            2003b. Hedge trimmer: A parse-and-trim approach
            to headline generation. In Proceedings of the HLT-
            NAACL 2003 Text Summarization Workshop, Ed-
            monton, Alberta, Canada, pages 1–8.

            T. Euler. 2002. Tailoring text using topic words: Se-
            lection and compression.
            In Proceedings of 13th
            International Workshop on Database and Expert
            Systems Applications (DEXA 2002), 2-6 Septem-
            ber 2002, Aix-en-Provence, France, pages 215–222.
            IEEE Computer Society.

            Kevin Knight and Daniel Marcu. 2000. Statistics-
            based summarization – step one: Sentence com-
            In The 17th National Conference of
            pression.
        </biblio>
    </article>
    <article>
        <preamble>compression.txt</preamble>
        <titre>Multi-Candidate Reduction: Sentence Compression as a Tool for Document Summarization Tasks∗</titre>
        <auteur>David Zajic1, Bonnie J. Dorr1, Jimmy Lin1, Richard Schwartz2 1University of Maryland</auteur>
        <abstract>This article examines the application of two single-document sentence compression techniques to the
            problem of multi-document summarization—a “parse-and-trim” approach and a statisti- cal noisy-channel
            approach. We introduce the Multi-Candidate Reduction (MCR) framework for multi-document summarization, in
            which many compressed candidates are generated for each source sentence. These candidates are then selected
            for inclusion in the ﬁnal summary based on a com- bination of static and dynamic features. Evaluations
            demonstrate that sentence compression is a valuable component of a larger multi-document summarization
            framework. Keywords: headline generation, summarization, parse-and-trim, Hidden Markov Model PACS: Artiﬁcial
            intelligence, 07.05.Mh; Computer science and technology, 89.20.Ff; Spoken lan- guages, processing of,
            43.71.Sy
        </abstract>
        <introduction>This article presents an application of two diﬀerent single-document sentence compression methods
            to the problem of multi-document summarization. The ﬁrst, a “parse-and-trim” approach, has been implemented
            in a system called Trimmer and its extended version called Topiary. The second, an HMM-based approach, has
            been implemented in a system called HMM Hedge. These systems share the basic premise that a textual summary
            can be constructed by selecting a subset of words, in order, from the original text, with morphological
            variation allowed for some word classes. Trimmer selects subsequences of words using a
            linguistically-motivated algorithm to trim syntactic constituents from sentences until a desired length has
            been reached. In the context of ∗Please cite as: David Zajic, Bonnie Dorr, Jimmy Lin, and Richard Schwartz.
            Multi-Candidate Reduction: Sentence Compression as a Tool for Document Summarization Tasks. Information
            Processing and Management, 43(6):1549-1570, 2007. [DOI: 10.1016/j.ipm.2007.01.016] This is the pre-print
            version of a published article. Citations to and quotations from this work should reference that
            publication. If you cite this work, please check that the published form contains precisely the material to
            which you intend to refer. (Received 18 July 2006; revised 3 January 2007; accepted 8 January 2007) This
            document prepared August 31, 2007, and may have minor diﬀerences from the published version. 1 Figure 1: The
            Multi-Candidate Reduction (MCR) framework. very short single-document summarization, or headline generation,
            this approach is applied to the lead sentence (the ﬁrst non-trivial sentence) in order to produce a short
            headline tens of characters long. HMM Hedge (Hidden Markov Model HEaDline GEnerator), on the other hand, is
            a statistical headline generation system that ﬁnds the subsequence of words most likely to be a headline for
            a given story. This approach is similar to techniques used in statistical machine translation in that the
            observed story is treated as the garbled version of an unseen headline transmitted through a noisy channel.
            In their original conﬁgurations, both Trimmer and HMM Hedge create sentence compressions that mimic the
            style of Headlinese, a form of compressed English associated with newspaper headlines (M˚ardh, 1980). The
            goal of our work is to transition from a single-document summarization task with sentence- level
            character-based length constraint to a multi-document summarization task with summary-level word-based
            length constraint. In headline generation, summaries should ﬁll as much of the available space as possible,
            without going over the limit. Headlines that fall below the length limit miss the opportunity to include
            additional information. Headlines that exceed the length limit must be trun- cated, resulting in
            ungrammatical sentences and loss of information. When the length constraint is applied to a collection of
            sentences instead of a single sentence, concerns about individual sentence length become less important. A
            short sentence does not result in wasted space and it is acceptable to include a lengthy sentence as long as
            it includes important, non-redundant information. To transition from single-document to multi-document
            summarization, we examined diﬀerent com- binations of possible sentence compressions to construct the best
            summary. A key innovation of this work—tested for the ﬁrst time in 2005 at the Document Understanding
            Conference (DUC-2005) (Zajic et al., 2005b) and Multilingual Summarization Evaluation (MSE-2005) (Zajic et
            al., 2005a)—is the use of multiple compressed candidates for sentences in the source documents. Sentence
            compression is used for diﬀerent purposes in single- and multi-document summarization tasks. In the ﬁrst
            case, the goal is to ﬁt long sentences into the available space while preserving important information. In
            multi- document summarization, sentence compression is used to generate multiple candidates that capture
            relevant, non-redundant information. Sentence selection algorithms can then be applied to determine which
            compressed candidates provide the best combination of topic coverage and brevity. We introduce a framework
            for extractive multi-document summarization called Multi-Candidate Reduction (MCR), whose architecture is
            shown in Figure 1. Sentence compression techniques are em- ployed in the intermediate stage of the
            processing pipeline to generate multiple compressed variants of source sentences. A sentence selector then
            builds the ﬁnal summary by choosing among these candi- dates, based on features propagated from the sentence
            compression method, features of the candidates 2 Sentence FilteringSentence CompressionSentence
            SelectionDocumentsSummarySentencesCandidatesTask-Specific Features(e.g., query) themselves, and features of
            the present summary state. Under this framework, our previously-developed systems, Trimmer and HMM Hedge,
            are employed to generate the compressed candidates. The MCR architecture also includes a ﬁltering module
            that chooses the source sentences from which compressed candidates are generated. This work does not examine
            the ﬁltering process in detail, as we only employ very simple approaches, e.g., retain ﬁrst n sentences from
            each document. Sentence compression supports extractive multi-document summarization by reducing the length
            of summary candidates while preserving their relevant content, thus allowing space for the inclusion of
            additional material. However, given the interaction of relevance and redundancy in a multi-sentence summary,
            it is unlikely that a single algorithm or scoring metric can provide the “best” compression of a sentence.
            This is the motivation for MCR, which provides several alternative candidates for each source sentence.
            Subsequent processes can then select among many alternatives when constructing the ﬁnal summary. This
            article is organized as follows: The next section relates our approach to other existing summa- rization
            systems. In Section 3, we describe Trimmer, a variant of Trimmer called Topiary, and HMM Hedge. These
            systems implement two fundamentally diﬀerent approaches to sentence compression. Section 4 describes how
            these tools can be applied to a multi-document summarization task. Section 5 describes evaluations of our
            framework. Our systems have also been applied to several diﬀerent types of texts; some of these applications
            will be brieﬂy covered in Section 6. Finally, we propose future work in the area before concluding.
        </introduction>
        <corps>2 Related Work

            We have developed a “parse-and-trim” approach to sentence compression, implemented in a system
            called Trimmer (Dorr et al., 2003b). The system generates a headline for a news story by compressing
            the lead (or main) topic sentence according to a linguistically-motivated algorithm that operates on
            syntactic structures. Syntactic approaches to compression have been used in single-document sum-
            marization systems such as Cut-and-Paste (Jing and McKeown, 2000) and also in multi-document
            summarization systems such as SC (Blair-Goldensohn et al., 2004) and CLASSY (Conroy et al., 2005;
            Conroy et al., 2006). The SC system pre-processes input to remove appositives and relative clauses.
            CLASSY uses an HMM sentence selection approach combined with a conservative sentence compression
            method based on shallow parsing to detect lexical cues to trigger phrase eliminations. The approach
            taken by Trimmer is most similar to that of Jing and McKeown (2000), in which algorithms for remov-
            ing syntactic constituents from sentences are informed by analysis of human summarizers. Trimmer
            diﬀers from these systems in that multiple compressed candidates of each sentence are generated. The
            potential of multiple alternative compressions has also been explored by Vanderwende et al. (2006).

            Summaries may also contain lists of words or short phrases that denote important topics or concepts
            in the document. In particular, extractive topic summaries consist of keywords or key phrases that
            occur in the document. We have built Topiary, an extension to Trimmer, that constructs headlines
            combining compressed sentences with topic terms. This approach is similar to the work of Euler (2002),
            except that Euler uses topic lists to guide sentence selection and compression toward a query-speciﬁc
            summary, whereas Topiary uses topics to augment the concept coverage of a generic summary. Our
            system was demonstrated to be the top-performing single-document summarization system in the
            DUC-2004 evaluation (Zajic et al., 2004).

            In contrast to Topiary, which combines sentence compression with topic terms, others have con-
            structed summaries directly from topic terms. For example, Bergler et al. (2003) choose noun phrases
            that represent the most important entities as determined by noun phrase coreference chains. Wang et
            al. (2005) propose a baseline system that constructs headlines from topic descriptors identiﬁed using
            term frequency counts; this system was reported to outperform LexTrim, their Topiary-style system.
            Zhou and Hovy (2003) construct ﬂuent summaries from a topic list by ﬁnding phrase clusters early in

            3


            the text that contain important topic words found throughout the text.

            The task of assigning topic terms to documents is related to text categorization, in which documents
            are assigned to pre-deﬁned categories. The categories can be labeled with topic terms, so that the
            decision to put a document in a category is equivalent to assigning that category’s label to the document.
            Assigning topic terms to documents by categorization permits the assignment of terms that do not occur
            in the document. Lewis (1999) describes a probabilistic feature-based method for assigning Reuters
            topics to news stories. Along these lines, OnTopicTM (Schwartz et al., 1997) uses an HMM to assign
            topics to a document.

            In addition to Trimmer, we have implemented HMM Hedge (Hidden Markov Model HEaDline
            GEnerator), a statistical headline generation system that ﬁnds the most likely headline for a given story.
            This approach is similar to techniques used in statistical machine translation in that the observed story
            is treated as the garbled version of an unseen headline transmitted through a noisy channel. The noisy-
            channel approach has been used for a wide range of Natural Language Processing (NLP) applications
            including speech recognition (Bahl et al., 1983); machine translation (Brown et al., 1990); spelling
            correction (Mays et al., 1990); language identiﬁcation (Dunning, 1994); and part-of-speech tagging
            (Cutting et al., 1992). Of those who have taken a noisy-channel approach to sentence compression
            for text summarization Banko et al. (2000), Knight and Marcu (2000), Knight and Marcu (2002),
            and Turner and Charniak (2005) have used the technique to ﬁnd the most probable short string that
            generated the observed full sentence.

            Like other summarization systems based on the noisy-channel model, HMM Hedge treats the ob-
            served data (the story) as the result of unobserved data (headlines) that have been distorted by trans-
            mission through a noisy channel. The eﬀect of the noisy channel is to add story words between the
            headline words. Our approach diﬀers from Knight and Marcu (2000), Banko et al. (2000), and Turner
            and Charniak (2005), in that HMM Hedge does not require a corpus of paired stories and summaries.
            HMM Hedge uses distinct language models of news stories and headlines, but does not require explicit
            pairings of stories and summaries.

            To transition our single-sentence summarization techniques to the problem of multi-document sum-
            marization, we must consider how to select candidates for inclusion in the ﬁnal summary. A common
            approach is to rank candidate sentences according to a set of features and iteratively build the sum-
            mary, appropriately re-ranking the candidates at each step to avoid redundancy. MEAD (Radev et al.,
            2004) scores source sentences according to a linear combination of features including centroid, position,
            and ﬁrst-sentence overlap. These scores are then reﬁned to consider cross-sentence dependencies, such
            as redundancy, chronological order, and source preferences. MCR diﬀers in that multiple variants of
            the same source sentences are available as candidates for inclusion in the ﬁnal summary.

            Minimization of redundancy is an important element of a multi-document summarization system.
            Carbonell and Goldstein (1998) propose a technique called Maximal Marginal Relevance (MMR) for
            ranking documents returned by an information retrieval system so that the front of the ranked list will
            contain diversity as well as high relevance. Goldstein et al. (2000) extend MMR to multi-document
            summarization. MCR borrows the ranking approach of MMR, but uses a diﬀerent set of features. Like
            MEAD, these approaches use feature weights that are optimized to maximize an automatic metric on
            training data.

            Several researchers have shown the importance of summarization in domains other than written
            news (Muresan et al., 2001; Clarke and Lapata, 2006). Within the MCR framework, we discuss the
            portability of Trimmer and HMM Hedge to a variety of diﬀerent texts: written news, broadcast news
            transcriptions, email threads, and text in foreign language.

            4


            3 Single-Sentence Compression

            Our general approach to the generation of a summary from a single document is to produce a headline
            by selecting words in order from the text of the story. Consider the following excerpt from a news story
            and corresponding headline:

            (1)

            (i) After months of debate following the Sept. 11 terrorist hijackings, the Transportation
            Department has decided that airline pilots will not be allowed to have guns in the
            cockpits.

            (ii) Pilots not allowed to have guns in cockpits.

            The bold words in (1i) form a ﬂuent and accurate headline, as shown in (1ii).
            This basic approach has been realized in two ways. The ﬁrst, Trimmer, uses a linguistically-
            motivated algorithm to remove grammatical constituents from the lead sentence until a length threshold
            is met. Topiary is a variant of Trimmer that combines ﬂuent text from a compressed sentence with
            topic terms to produce headlines. The second, HMM Hedge, employs a noisy-channel model to ﬁnd
            the most likely headline for a given story. The remainder of this section will present Trimmer, Topiary,
            and HMM Hedge in more detail.

            3.1 Trimmer

            Our ﬁrst approach to sentence compression involves iteratively removing grammatical constituents from
            the parse tree of a sentence using linguistically-motivated rules until a length threshold has been met.
            When applied to the lead sentence, or ﬁrst non-trivial sentence of a story, our algorithm generates a
            very short summary, or headline. This idea is implemented in our Trimmer system, which can leverage
            the output of any constituency parser that uses the Penn Treebank conventions. At present we use
            Charniak’s parser (Charniak, 2000).

            The insights that form the basis and justiﬁcation for the Trimmer rules come from our previous
            study, which compared the relative prevalence of certain constructions in human-written summaries
            and lead sentences in stories. This study used 218 human-written summaries of 73 documents from
            the TIPSTER corpus (Harman and Liberman, 1993) dated January 1, 1989. The 218 summaries and
            the lead sentences of the 73 stories were parsed using the BBN SIFT parser (Miller et al., 2000). The
            parser produced 957 noun phrases (NP nodes in the parse trees) and 315 clauses (S nodes in the parse
            trees) for the 218 summaries. For the 73 lead sentences, the parser produced 817 noun phrases and
            316 clauses.

            At each level (sentence, clause, and noun phrase), diﬀerent types of linguistic phenomena were

            counted.

            • At the sentence level, the numbers of preposed adjuncts, conjoined clauses, and conjoined verb
            phrases were counted. Children of the root S node that occur to the left of the ﬁrst NP are
            considered to be preposed adjuncts. The bracketed phase in “[According to police] the crime rate
            has gone down” is a prototypical example of a preposed adjunct.

            • At the clause level, temporal expressions, trailing SBAR nodes, and trailing PP nodes were

            counted. Trailing constituents are those not designated as an argument of a verb phrase.

            • At both the sentence and clause levels, conjoined S nodes and conjoined VP nodes were counted.

            • At the NP level, determiners and relative clauses were counted.

            The counts and prevalence of the phenomena in the human-generated headlines and lead sentences
            are shown in Table 1. The results of this analysis illuminated the opportunities for trimming con-
            stituents and guided the development of our Trimmer rules, detailed below.

            5


            Summary

            Level
            Sentence

            Clause

            Noun Phrase

            Phenomenon
            preposed adjuncts
            conjoined S
            conjoined VP
            temporal expression
            trailing PP
            trailing SBAR
            relative clause
            determiner

            0/218
            1/218
            7/218
            5/315
            165/315
            24/315
            3/957
            31/957

            Lead Sentence
            2.7%
            2/73
            0%
            4%
            0.5% 3/73
            27%
            3%
            20/73
            24%
            1.5% 77/316
            58%
            52% 184/316
            16%
            8%
            49/316
            3.5%
            0.3% 29/817
            25%
            3%

            205/817

            Table 1: Counts and prevalence of phenomena found in summaries and lead sentences.

            3.1.1 Trimmer Algorithm

            Trimmer applies syntactic compression rules to a parse tree according the following algorithm:

            1. Remove temporal expressions

            2. Select Root S node

            3. Remove preposed adjuncts

            4. Remove some determiners

            5. Remove conjunctions

            6. Remove modal verbs

            7. Remove complementizer that

            8. Apply the XP over XP rule

            9. Remove PPs that do not contain named entities

            10. Remove all PPs under SBARs

            11. Remove SBARSs

            12. Backtrack to state before Step 9

            13. Remove SBARs

            14. Remove PPs that do not contain named entities

            15. Remove all PPs

            Steps 1 and 4 of the algorithm remove low-content units from the parse tree.
            Temporal expressions—although certainly not content-free—are not usually vital for summarizing
            the content of an article. Since the goal is to provide an informative headline, the identiﬁcation and
            elimination of temporal expressions (Step 1) allow other more important details to remain in the length-
            constrained headline. The use of BBN’s IdentiFinderTM (Bikel et al., 1999) for removal of temporal
            expressions is described in Section 3.1.2.

            The determiner rule (Step 4) removes leaf nodes that are assigned the part-of-speech tag DT and
            have the surface form the, a, or an. The intuition for this rule is that the information carried by

            6


            articles is expendable in summaries, even though this makes the summaries ungrammatical for general
            English. Omitting articles is one of the most salient features of newspaper headlines. Sentences (2)
            and (3), taken from the New York Times website on September 27, 2006, illustrate this phenomenon.
            The italicized articles did not occur in the actual newspaper headlines.

            (2) The Gotti Case Ends With a Mistrial for the Third Time in a Year

            (3) A Texas Case Involving Marital Counseling Is the Latest to Test the Line Between Church and

            State

            Step 2 identiﬁes nodes in the parse tree of a sentence that could serve as the root of a compression
            for the sentence. Such nodes will be referred to as Root S nodes. A node in a tree is a Root S node if it
            is labeled S in the parse tree and has children labeled NP and VP, in that order. The human-generated
            headlines we studied always conform to this rule. It has been adopted as a constraint in the Trimmer
            algorithm that the lowest leftmost Root S node is taken to be the root node of the headline. An
            example of this rule application is shown in (4). The boldfaced material in the parse is retained and
            the italicized material is eliminated.

            (4)

            (i)

            Input: Rebels agreed to talks with government oﬃcials, international observers said Tuesday.

            (ii) Parse: [S [S [NP Rebels][VP agreed to talks with government oﬃcials]], interna-

            tional observers said Tuesday.]

            (iii) Output: Rebels agreed to talks with government oﬃcials.

            When the parser produces a usable parse tree, this rule selects a valid starting point for compression.

            However, the parser sometimes produces incorrect output, as in the cases below (from DUC-2003):

            (5)

            (i)

            Parse: [S[SBAR What started as a local controversy][VP has evolved into an
            international scandal.]]

            (ii) Parse: [NP[NP Bangladesh][CC and][NP[NP India][VP signed a water sharing

            accord.]]]

            In (5i), an S exists, but it does not conform to the requirements of the Root S rule because it does
            not have as children an NP followed by a VP. The problem is resolved by selecting the lowest leftmost
            In (5ii), no S is present in the parse. This problem is
            S, ignoring the constraints on the children.
            resolved by selecting the root of the entire parse tree as the root of the headline. These parsing errors
            occur infrequently—only 6% of the sentences in the DUC-2003 evaluation data exhibit these problems,
            based on parses generated by the BBN SIFT parser.

            The motivation for removing preposed adjuncts (Step 3) is that all of the human-generated headlines
            omit what we refer to as the preamble of the sentence. Preposed adjuncts are constituents that precede
            the ﬁrst NP (the subject) under the Root S chosen in Step 2; the preamble of a sentence consists of its
            preposed adjuncts. The impact of preposed adjunct removal can be seen in example (6).

            (6)

            (i)

            Input: According to a now ﬁnalized blueprint described by U.S. oﬃcials and other sources,
            the Bush administration plans to take complete, unilateral control of a post-Saddam Hussein
            Iraq.

            (ii) Parse: [S[PP According to a now ﬁnalized blueprint described by U.S. oﬃcials and other
            sources], [Det the] Bush administration plans to take complete, unilateral control
            of[Det a] post-Saddam Hussein Iraq.]

            7


            (iii) Output: Bush administration plans to take complete unilateral control of post-Saddam

            Hussein Iraq.

            The remaining steps of the algorithm remove linguistically peripheral material through successive
            deletions of constituents until the sentence is shorter than a length threshold. Each stage of the
            algorithm corresponds to the application of one of the rules. Trimmer ﬁrst ﬁnds the pool of nodes in
            the parse to which a rule can be applied. The rule is then iteratively applied to the deepest, rightmost
            remaining node in the pool until the length threshold is reached or the pool is exhausted. After a rule
            has been applied at all possible nodes in the parse tree, the algorithm moves to the next step.

            In the case of a conjunction with two children (Step 5), one of the children will be removed. If the
            conjunction is and , the second child is removed. If the conjunction is but, the ﬁrst child is removed.
            This rule is illustrated by the following examples, where the italicized text is trimmed.

            (7) When Sotheby’s sold a Harry S Truman signature that turned out to be a reproduction, the

            prestigious auction house apologized and bought it back .

            (8) President Clinton expressed sympathy after a car-bomb explosion in a Jerusalem market wounded
            24 people but said the attack should not derail the recent land-for-security deal between Israel
            and the Palestinians.

            The modal verb rule (Step 6) applies to verb phrases in which the head is a modal verb and the
            head of the child verb phrase is a form of have or be. In such cases, the modal and auxiliary verbs
            are removed. Sentences (9) and (10) show examples of this rule application. Note that although in
            Sentence (10) the omission of trimmed material changes the meaning, given a tight space constraint,
            the loss of the modality is preferable to the loss of other content information.

            (9) People’s palms and ﬁngerprints may be used to diagnose schizophrenia.

            (10) Agents may have ﬁred potentially ﬂammable tear gas cannisters.

            The complementizer rule (Step 7) removes the word that when it occurs as a complementizer.

            Sentence (11) shows an example in which two complementizers can be removed.

            (11) Hoﬀman stressed that study is only preliminary and can’t prove that treatment useful.

            The XP-over-XP rule (Step 8) is a linguistic generalization that allows a single rule to cover two
            diﬀerent phenomena. XP in the name of the rule is a variable that can take two values: NP and VP.
            In constructions of the form [XP [XP ...] ...], the other children of the higher XP are removed. Note
            that the child XP must be the ﬁrst child of the parent XP. When XP = NP the rule removes relative
            clauses (as in Sentence (12)) and appositives (as in Sentence (13)).

            (12) Schizophrenia patients whose medication couldn’t stop the imaginary voices in their heads gained

            some relief.

            (13) A team led by Dr. Linda Brzustowicz, assistant professor of neuroscience at Rutgers University’s
            Center for Molecular and Behavioral Neuroscience in Newark, studied DNA of dozens of members
            of 22 families.

            The rules that remove prepositional phrases and subordinate clauses (Steps 9 through 15) are
            sometimes prone to removing important content. Thus, these rules are applied last, only when there
            are no other types of rules to apply. Moreover, these rules are applied with a backoﬀ option to avoid

            8


            over-trimming the parse tree. First, the PP rule is applied (Steps 9 and 10),1 followed by the SBAR
            rule (Step 11). If the desired sentence length has not been reached, the system reverts to the parse tree
            as it was before any prepositional phrases were removed (Step 12) and applies the SBAR rule (Step
            13). If the desired length still has not been reached, the PP rule is applied again (Steps 14 and 15).

            The intuition behind this ordering is that, when removing constituents from a parse tree, it is prefer-
            able to remove smaller fragments before larger ones and prepositional phrases tend to be smaller than
            subordinate clauses. Thus, Trimmer ﬁrst attempts to achieve the desired length by removing smaller
            constituents (PPs), but if this cannot be accomplished, the system restores the smaller constituents,
            removes a larger constituent, and then resumes the deletion of the smaller constituents. To reduce the
            risk of removing prepositional phrases that contain important information, BBN’s IdentiFinder is used
            to distinguish PPs containing temporal expressions and named entities, as described next.

            3.1.2 Use of BBN’s IdentiFinder in Trimmer

            BBN’s IdentiFinder is used both for the elimination of temporal expressions and for conservative
            deletion of PPs containing named entities. The elimination of temporal expressions (Step 1) is a two-
            [NP [X] ...]
            step process: (a) use IdentiFinder to mark temporal expressions; and (b) remove [PP ...
            ...] and [NP [X]] where X is tagged as part of a temporal expression. The following examples illustrate
            the application of temporal expression removal rule:

            (14) (i)

            Input: The State Department on Friday lifted the ban it had imposed on foreign ﬂiers.
            (ii) Parse: [S [NP[Det The] State Department [PP [IN on] [NP [NNP Friday]]] [VP lifted

            [Det the] ban it had imposed on foreign ﬂiers.]]

            (iii) Output: State Department lifted ban it had imposed on foreign ﬂiers.

            (15) (i)

            Input: An international relief agency announced Wednesday that it is withdrawing from
            North Korea.

            (ii) Parse: [S [NP [Det An] international relief agency][VP announced [NP [NNP Wednes-

            day]] that it is withdrawing from North Korea.]]

            (iii) Output: International relief agency announced that it is withdrawing from North Korea.

            IdentiFinder is also used to ensure that prepositional phrases containing named entities are not
            removed during the ﬁrst round of PP removal (Step 9). However, prepositional phrases containing
            named entities that are descendants of SBARs are removed before the parent SBAR is removed, since
            we should remove a smaller constituent before removing a larger constituent that subsumes it. Sentence
            (16) shows an example of a SBAR subsuming two PPs, one of which contains a named entity.

            (16) The commercial ﬁshing restrictions in Washington will not be lifted [SBAR unless the salmon

            population increases [PP to a sustainable number] [PP in the Columbia River]].

            If the PP rule were not sensitive to named entities, the PP in the Columbia River would be the
            ﬁrst prepositional phrase to be removed, because it is the lowest rightmost PP in the parse. However,
            this PP provides an important piece of information: the location of the salmon population. The rule in
            Step 9 will skip the last prepositional phrase and remove the penultimate PP to a sustainable number .
            This concludes an overview of the Trimmer rules and our syntactic sentence compression algorithm.
            Given a length limit, the system will produce a single compressed version of the target sentence.
            Multiple compressions can be generated by setting the length limit to be very small and storing the state
            of the sentence after each rule application as a compressed variant. In section 4, we will describe how
            multiple compressed candidates generated by Trimmer are used as a component of a multi-document
            summarization system.

            1The reason for breaking PP removal into two stages is discussed in Section 3.1.2.

            9


            3.2 Topiary

            We have used the Trimmer approach to compression in another variant of single-sentence summarization
            called Topiary. This system combines Trimmer with a topic discovery approach (described next) to
            produce a ﬂuent summary along with additional context.

            The Trimmer algorithm is constrained to build a headline from a single sentence. However, it is
            often the case that no single sentence contains all the important information in a story. Relevant
            information can be spread over multiple sentences, linked by anaphora or ellipsis.
            In addition, the
            choice of lead sentence may not be ideal and our trimming rules are imperfect.

            On the other hand, approaches that construct headlines from lists of topic terms (Lewis, 1999;
            Schwartz et al., 1997) also have limitations. For example, Unsupervised Topic Discovery (UTD)—
            described below—rarely generates any topic terms that are verbs. Thus, topic lists are good at indi-
            cating the general subject but rarely give any direct indication of what events took place. Intuitively,
            we need both ﬂuent text to tell what happened and topic terms to provide context.

            3.2.1 Topic Term Generation: UTD and OnTopic

            OnTopic (Schwartz et al., 1997) uses an HMM to assign topics to a document; topic models are derived
            from an annotated corpus. However, it is often diﬃcult to acquire such data, especially for a new genre
            or language. UTD (Sista et al., 2002) was developed to overcome this limitation: it takes as input a
            large unannotated corpus and automatically creates a set of topic models with meaningful names.

            The UTD algorithm has several stages. First, it analyzes the corpus to ﬁnd multi-word sequences
            that can be treated as single tokens.
            It does this using two methods. One method is a minimum
            description length criterion that detects phrases that occur frequently relative to the individual words.
            The second method uses BBN’s IdentiFinder to detect multi-word names. These names are added to
            the text as additional tokens. They are also likely to be chosen as potential topic names. In the second
            stage of UTD, we ﬁnd those terms (both single-word and multi-word) with high tf.idf. Only those
            topic names that occur as high-content terms in at least four diﬀerent documents are kept. The third
            stage trains topic models corresponding to these topic terms. The modiﬁed Expectation Maximization
            procedure of BBN’s OnTopic system is used to determine which words in the documents often signify
            these topic names. This produces topic models. Fourth, these topic models are used to ﬁnd the most
            likely topics for each document, which is equivalent to assigning the name of the topic model to the
            document as a topic term. This often assigns topics to documents where the topic name does not occur
            in the document text.

            We found, in various experiments (Sista et al., 2002), that the topic names derived by this procedure
            were usually meaningful and that the topic assignment was about as good as when the topics were
            derived from a corpus that was annotated by people. We have also used this procedure on diﬀerent
            languages and shown the same behavior. Since UTD is unsupervised, it can run equally well on a new
            language, as long as the documents can be divided into strings that approximate words.

            The topic list in (17) was generated by UTD and OnTopic for a story about the FBI investigation

            of the 1998 bombing of the U.S. embassy in Nairobi.

            (17) BIN LADEN, EMBASSY, BOMBING, POLICE OFFICIALS, PRISON, HOUSE, FIRE, KA-

            BILA

            Topiary uses UTD to generate topic terms for the collection of documents to be summarized and
            uses OnTopic to assign the topic terms to the documents. The next section will describe how topic
            terms and sentence compressions are combined to form Topiary summaries.

            10


            3.2.2 Topiary Algorithm

            As each Trimmer rule is applied to a sentence, the resulting state of the sentence is stored as a
            compressed variant of the source sentence. Topiary selects from the variants the longest one such
            that there is room to prepend the highest scoring non-redundant topic term. Suppose the highest
            scoring topic term is “terrorism” and the length threshold is 75 characters. To make room for the topic
            “terrorism”, the length threshold is lowered by 10 characters: 9 characters for the topic and 1 character
            as a separator. Thus, Topiary chooses the longest trimmed variant under 65 characters that does not
            contain the word “terrorism”, If there is no such candidate, i.e., all the trimmed variants contain the
            word terrorism, Topiary would consider the second highest scoring topic word, “bomb”. Topiary would
            select the longest trimmed variant under 70 characters that does not contain the word “bomb”. After
            Topiary has selected a trimmed variant and prepended a topic to it, it checks to see how much unused
            space remains under the threshold. Additional topic words are added between the ﬁrst topic word and
            the compressed sentence until all space is exhausted.

            This process results in a headline that contains one or more main topics about the story and a
            short sentence that says what happened concerning them. The combination is often more concise than
            a fully ﬂuent sentence and compensates for the fact that the information content from the topic and
            the compressed sentence do not occur together in any single sentence from the source text.

            As examples, sentences (18) and (19) are the outputs of Trimmer and Topiary, respectively, for the

            same story in which UTD selected the topic terms in (17).

            (18) FBI agents this week began questioning relatives of the victims

            (19) BIN LADEN, EMBASSY, BOMBING: FBI agents this week began questioning relatives

            By combining topics and parse-and-trim compression, Topiary achieved the highest score on the
            single-document summarization task (i.e., headline generation task) in DUC-2004 (Zajic et al., 2004).

            3.3 HMM Hedge

            Our second approach to sentence compression, implemented in HMM Hedge, treats the observed data
            (the story) as the result of unobserved data (headlines) that have been distorted by transmission
            through a noisy channel. The eﬀect of the noisy channel is to add story words between the headline
            words. The model is biased by parameters to make the resulting headlines more like Headlinese, the
            observed language of newspaper headlines created by copy editors.

            Formally, we consider a story S to be a sequence of N words. We want to ﬁnd a headline H, a

            subsequence of words from S, that maximizes the likelihood that H generated the story S, or:

            It is diﬃcult to directly estimate P (H|S), but this probability can be expressed in terms of other
            probabilities that are easier to compute, using Bayes’ rule:

            argmaxH P (H|S)

            P (H|S) = P (S|H)P (H)

            P (S)

            Since the goal is to maximize this expression over H, and P (S) is constant with respect to H, the
            denominator of the above expression can be omitted. Thus we wish to ﬁnd:

            argmaxH P (S|H)P (H)

            Let H be a headline consisting of words h1, h2, ..., hn. Let the special symbols start and end represent
            the beginning and end of a headline, respectively. P (H) can be estimated using a bigram model of
            Headlinese:

            P (H) = P (h1|start)P (h2|h1)...P (end|hn)

            11


            The bigram probabilities of the words in the headline language were computed from a corpus of
            English headlines taken from 242,918 AP newswire stories in the TIPSTER corpus. The headlines
            contain 2,848,194 words from a vocabulary of 88,627 distinct words.

            Given a story S and a headline H, the action of the noisy channel is to form S by adding non-headline
            words to H. Let G be the non-headline words added by the channel to the headline: g1, g2, ..., gm. For
            the moment, we assume that the headline words are transmitted through the channel with probability
            1. We estimate P (S|H), the probability that the channel added non-headline words G to headline H
            to form story S. This is accomplished using a unigram model of newspaper stories that we will refer
            to as the general language, in contrast to the headline language. Let Pgl(g) be the probability of
            non-headline word g in the general language and Pch(h) = 1 be the probability that headline word h
            is transmitted through the channel as story word h.

            P (S|H) = Pgl(g1)Pgl(g2)...Pgl(gm)Pch(h1)Pch(h2)...Pch(hn)

            = Pgl(g1)Pgl(g2)...Pgl(gm)

            The unigram probabilities of the words in the general language were computed from 242,918 English
            AP news stories in the TIPSTER corpus. The stories contain 135,150,288 words from a vocabulary of
            428,633 distinct words.

            The process by which the noisy channel generates a story from a headline can be represented by a
            Hidden Markov Model (HMM) (Baum, 1972). An HMM is a weighted ﬁnite-state automaton in which
            each state probabilistically emits a string. The simplest HMM for generating headlines consists of two
            states: an H state that emits words that occur in the headline and a G state that emits all the other
            words in the story.

            Since we use a bigram model of headlines, each state that emits headline words must “remember”
            the previously emitted headline word. If we did not constrain headline words to actually occur in the
            story, we would need an H state for each word in the headline vocabulary. However, because headline
            words are chosen from the story words, it is suﬃcient to have an H state for each story word. For any
            story, the HMM consists of a start state S, an end state E, an H state for each word in the story, a
            corresponding G state for each H state, and a state Gstart that emits words that occur before the ﬁrst
            headline word in the story. An H state can emit only the word it represents. The corresponding G
            state remembers which word was emitted by its H state and can emit any word in the story language.
            A headline corresponds to a path through the HMM from S to E that emits all the words in the story
            in the correct order. In practice the HMM is constructed with states for only the ﬁrst N words of the
            story, where N is a constant (60), or N is the number of words in the ﬁrst sentence.2

            In example (1i), given earlier, the H states will emit the words in bold (pilots, not, allowed, to,
            have, guns, in, cockpits) and the G states will emit all the other words. The HMM will transition
            between the H and G states as needed to generate the words of the story. In the current example,
            the model will have states Start, Gstart, End, and 28 H states with 28 corresponding G states.3 The
            headline given in example (1ii) corresponds to the following sequence of states: Start, Gstart 17 times,
            Hpilots, Gpilots, Hnot, Gnot, Hallowed, Hto, Hhave, Hguns, Hin, Gin, Hcockpits, End. This path is not the
            only one that could generate the story in (1i). Other possibilities are:

            (20) (i) Transportation Department decided airline pilots not to have guns.

            (ii) Months of the terrorist has to have cockpits.

            2Limiting consideration of headline words to the early part of the story is justiﬁed in Dorr et al. (2003a)
            where it
            was shown that more than half of the headline words are chosen from the ﬁrst sentence of the story. Other
            methods for
            selecting the window of story words are possible and will be explored in future research.

            3The subscript of a G state indicates the H state it is associated with, not the story word it emits. In the
            example,

            Gpilots emits story word will , Gnot emits story word be, and Gin emits story word the.

            12


            Although (20i) and (20ii) are possible headlines for (1i), the conditional probability of (20ii) given (1i)
            will be lower than the conditional probability of (20i) given (1i).

            The Viterbi algorithm (Viterbi, 1967) is used to select the most likely headline for a given story.
            We use length constraints to ﬁnd the most likely headlines consisting of W words, where W ranges
            from 5 to 15. Multiple backpointers are used so that we can ﬁnd the n most likely headlines at each
            length.

            HMM Hedge is enhanced by three additional decoding parameters to help the system choose outputs
            that best mimic actual headlines: a position bias, a clump bias, and a gap bias. The incorporation
            of these biases changes the score produced by the decoder from a probability to a relative desirability
            score. The three parameters were motivated by analysis of system output and their values were set by
            trial and error. A logical extension to this work would be to learn the best setting of these biases, e.g.,
            through Expectation Maximization.

            The position bias favors headlines that include words near the front of the story. This reﬂects our
            observations of human-constructed headlines, in which headline words tend to appear near the front
            of the story. The initial position bias p is a positive number less than one. The story word in the
            nth position is assigned a position bias of log(pn). When an H state emits a story word, the position
            bias is added to the desirability score. Thus, words near the front of the story carry less of a position
            bias than words farther along. Note that this generalization often does not hold in the case of human
            interest and sports stories, which may start with a hook to get the reader’s attention, rather than a
            topic sentence.

            We also observed that human-constructed headlines tend to contain contiguous blocks of story
            words. Example (1ii), given earlier, illustrates this with the string “allowed to have guns”. The
            string bias is used to favor “clumpiness”. i.e., the tendency to generate headlines composed of clumps
            of contiguous story words. The log of the clump bias is added to the desirability score with each
            transition from an H state to its associated G state. With high clump biases, the system will favor
            headlines consisting of fewer but larger clumps of contiguous story words.

            The gap bias is used to disfavor headline “gappiness”, i.e., large gaps of non-headline words in the
            story between clumps of headline words. Although humans are capable of constructing ﬂuent headlines
            by selecting widely spaced words, we observed that HMM Hedge was more likely to combine unrelated
            material by doing this. At each transition from a G state to an H state, corresponding to the end
            of a sequence of non-headline words in the story, a gap bias is applied that increases with the size of
            the gap between the current headline and the last headline word to be emitted. This can also be seen
            as a penalty for spending too much time in one G state. With high gap biases, the system will favor
            headlines with few large gaps.

            One characteristic diﬀerence between newspaper headline text and newspaper story text is that
            headlines tend to be in present tense while story sentences tend to be in the past tense. Past tense
            verbs occur more rarely in the headline language than in the general language. HMM Hedge mimics this
            aspect of Headlinese by allowing morphological variation between headline verbs and the corresponding
            story verbs. Morphological variation for verbs is achieved by creating an H state for each available
            variant of a story verb. These H states still emit the story verb but they are labeled with the variant.
            HMM Hedge can generate a headline in which proposes is the unobserved headline word that emits the
            observed story word proposed , even though proposes does not occur in the story.

            (21) (i) A group has proposed awarding $1 million in every general election to one randomly chosen

            voter.

            (ii) Group proposes awarding $1 million to randomly chosen voter.

            Finally, we constrain each headline to contain at least one verb. That is to say, we ignore headlines

            that do not contain at least one verb, no matter how desirable the decoding is.

            13


            Although we have described an application of HMM Hedge to blocks of story words without reference
            to sentence boundaries, it is also possible to use HMM as a single sentence compressor by limiting the
            block to the words in a single sentence.

            Also, as we will see shortly, multiple alternative compressions of a sentence may be generated with
            HMM Hedge. The Viterbi algorithm is capable of discovering n-best compressions of a window of
            story words and can be constrained to consider only paths that include a speciﬁc number of H states,
            corresponding to compressions that contain a speciﬁc number of words. We use HMM Hedge to generate
            55 compressions for each sentence by computing the ﬁve best headlines at each length, from 5 to 15
            words. In Section 4 we will describe how HMM Hedge is used as a component of a multi-document
            summarization system.

            4 Multi-Document Summarization

            The sentence compression tools we developed for single-document summarization have been incor-
            porated into our Multi-Candidate Reduction framework for multi-document summarization. MCR
            produces a textual summary from a collection of relevant documents in three steps. First, sentences
            are selected from the source documents for compression. The most important information occurs near
            the front of the stories, so we select the ﬁrst ﬁve sentences of each document for compression. Second,
            multiple compressed versions of each sentence are produced using Trimmer or HMM Hedge to create a
            pool of candidates for inclusion in the summary. Finally, a sentence selector constructs the summary
            by iteratively choosing from the pool of candidates based on a linear combination of features until the
            summary reaches a desired length.

            At present, weights for the features are determined by manually optimizing on a set of training
            data to maximize the Rouge-2 recall score (Lin and Hovy, 2003), using Rouge version 1.5.5. A
            typical summarization task might call for the system to generate a 250-word summary from a couple
            of dozen news stories. These summaries may be query-focused, in the sense that the summaries should
            be responsive to a particular information need, or they may be generic, in that a broad overview of the
            documents is desired.

            Our sentence selector adopts certain aspects of Maximal Marginal Relevance (MMR) (Carbonell
            and Goldstein, 1998), an approach that attempts to balance relevance and anti-redundancy. In MCR’s
            selection module, the highest scoring sentence from the pool of eligible candidates is chosen for inclusion
            in the summary. Features that contribute to a candidate’s score can be divided into two types: dynamic
            and static. When a candidate is chosen, all other compressed variants of that sentence are eliminated.
            After a candidate is added to the summary, the dynamic features are re-computed, and the candidates
            are re-ranked. Candidates are added to the summary until the desired length is achieved. The ordering
            of candidates in the summary is the same as the order in which they were selected for inclusion. The
            ﬁnal sentence of the summary is truncated if it causes the summary to exceed the length limit.

            4.1 Static Features

            Static features are calculated before sentence selection begins and do not change during the process of
            summary construction:

            • Position. The zero-based position of the sentence in the document.

            • Sentence Relevance. The relevance score of the sentence to the query.

            • Document Relevance. The relevance score of the document to the query.

            • Sentence Centrality. The centrality score of the sentence to the topic cluster.

            14


            • Document Centrality. The centrality score of the document to the topic cluster.

            • Scores from the Compression Modules:

            – Trim rule application counts. For Trimmer-based MCR, the number of Trimmer rule in-

            stances applied to produce the candidate.

            – Negative Log Desirability. For HMM-based MCR, the relative desirability score of the

            candidate.

            We use the Uniform Retrieval Architecture (URA), University of Maryland’s software infrastructure
            for information retrieval tasks, to compute relevance and centrality scores for each compressed candi-
            date. There are four such scores: the relevance score between a compressed sentence and the query,
            the relevance score between the document containing the compressed sentence and the query, the cen-
            trality score between a compressed sentence and the topic cluster, and the centrality score between
            the document containing the compressed sentence and the topic cluster. We deﬁne the topic cluster
            to be the entire collection of documents relevant to this particular summarization task. Centrality is
            a concept that quantiﬁes how similar a piece of text is to all other texts that discuss the same general
            topic. We assume that sentences having higher term overlap with the query and sources more “central”
            to the topic cluster are preferred for inclusion in the ﬁnal summary.

            The relevance score between a compressed sentence and the query is an idf-weighted count of
            overlapping terms (number of terms shared by the two text segments). Inverse document frequency
            (idf), a commonly-used measure in the information retrieval literature, roughly captures term salience.
            The idf of a term t is deﬁned by log(N/ct), where N is the total number of documents in a particular
            corpus and ct is the number of documents containing term t; these statistics were calculated from one
            year’s worth of LA Times articles. Weighting term overlap by inverse document frequency captures
            the intuition that matching certain terms is more important than matching others.

            Lucene, a freely-available oﬀ-the-shelf information retrieval system, is used to compute the three
            other scores. The relevance score between the document containing the compressed sentence and
            the query is computed using Lucene’s built-in similarity function. The centrality score between the
            compressed sentence and the topic cluster is the mean of the similarity between the sentence and each
            document comprising the cluster (once again, as computed by Lucene’s built-in similarity function).
            The document-cluster centrality score is also computed in much the same way, by taking the mean
            In order to
            of the similarity of the particular document with every other document in the cluster.
            obtain an accurate distribution of term frequencies to facilitate the similarity calculation, we indexed
            all relevant documents (i.e., the topic cluster) along with a comparable corpus (one year of the LA
            Times)—this additional text essentially serves as a background model for non-relevant documents.

            Some features are derived from the sentence compression modules used to generate candidates. For
            Trimmer, the rule application count feature of a candidate is the number of rules that were applied to
            a source sentence to produce the candidate. The rules are not presumed to be equally eﬀective, so the
            rule application counts are broken down by rule type. For HMM Hedge, we use the relative desirability
            score calculated by the decoder, expressed as a negative log.

            The features discussed in this section are assigned to the candidates before summary generation
            begins and remain ﬁxed throughout the process of summary sentence selection. The next section
            discusses how candidate features are assigned new values as summary geneneration proceeds.

            4.2 Dynamic Features

            Dynamic features change during the process of sentence selection to reﬂect changes in the state of the
            summary as sentences are added.4 The dynamic features are:

            4At present the dynamic features are properties of the candidates, calculated with respect to the current
            summary
            state. There are no features directly relating to the amount of space left in the summary, so there is no
            mechanism that

            15


            • Redundancy. A measure of how similar the sentence is to the current summary.

            • Sentence-from-doc. The number of sentences already selected from the sentence’s document.

            The intuition behind our redundancy measure is that candidates containing words that occur much
            more frequently in the current state of the summary than they do in general English are redundant
            to the summary. We imagine that sentences in the summary are generated by the underlying word
            distribution of the summary rather than the distribution of words in the general language. If a sentence
            appears to have been generated by the summary rather than by the general language, we take it to
            be redundant to the summary. Suppose we have a summary about earthquakes. The presence in a
            candidate of words like earthquake, seismic, and Richter Scale, which have a high likelihood in the
            summary, will make us think that the candidate is redundant to the summary.

            To estimate the extent to which a candidate is more likely to have been generated by a summary
            than by the general language, we consider the probabilities of the words in the candidate. We estimate
            that the probability that a word w occurs in a candidate generated by the summary is

            P (w) = λP (w|D) + (1 − λ)P (w|C)

            where D is the summary, C is the general language corpus5, λ is a parameter estimating the probability
            that the word was generated by the summary and (1−λ) is the probability that the word was generated
            by the general language. We have set λ = 0.3, as a general estimate of the portion of words in a text
            that are speciﬁc to the text’s topic. We estimate the probabilities by counting the words6 in the current
            summary and the general language corpus:

            P (w|D) =

            count of w in D
            size of D

            P (w|C) =

            count of w in C
            size of C

            We take the probability of a sentence to be the product of the probabilities of its words, so we calculate
            the probability that a sentence was generated by the summary, i.e. our redundancy metric, as:

            Redundancy(S) =

            Y

            s∈S

            λP (s|D) + (1 − λ)P (s|C)

            For ease of computation, we actually use log probabilities:

            log(λP (s|D) + (1 − λ)P (s|C))

            X

            s∈S

            Redundancy is a dynamic feature because the word distribution of the current summary changes with
            every iteration of the sentence selector.

            4.3 Examples of System Output

            We applied our MCR framework to test data from the DUC-2006 evaluation (Dang and Harman, 2006).
            Given a topic description and a set of 25 documents related to the topic (drawn from AP newswire,
            the New York Times, and the Xinhua News Agency English Service), the system’s task was to create

            would aﬀect the distribution of compressed candidates over the iterations of the sentence selector. This
            issue will be
            addressed as future work in Section 7.

            5The documents in the set being summarized are used to estimate the general language model.
            6Actually, preprocessing for redundancy includes stopword removal and applying the Porter Stemmer (Porter,
            1980).

            16


            Title: Native American Reservation System—pros and cons
            Narrative Description: Discuss conditions on American Indian reservations or among Native
            American communities. Include the beneﬁts and drawbacks of the reservation system. Include
            legal privileges and problems.

            Figure 2: Topic D0601A from the DUC-2006 multi-document summarization task.

            a 250-word summary that addressed the information need expressed in the topic. One of the topic
            descriptions is shown in Figure 2. The 25 documents in the document set have an average size of 1170
            words, so a 250-word summary represents a compression ratio of 0.86%.

            Figures 3, 4 and 5 show examples of MCR output using Trimmer compression, HMM Hedge com-
            pression, or no compression. For readability, we use ◦ as a sentence delimiter; this is not part of the
            actual system output. The sentences compressed by Trimmer mimic Headlinese by omitting determin-
            ers and auxiliary verbs. For example, the ﬁrst sentence in Figure 3 is a compression of the following
            source sentence:

            Seeking to get a more accurate count of the country’s American Indian population, the
            Census Bureau is turning to tribal leaders and residents on reservations to help overcome
            long-standing feelings of wariness or anger toward the federal government.

            Three determiners and a form of be have been removed from the source sentence in the compression
            that appears in the summary. The removal of this material makes the sentence appear more like a
            headline.

            In comparison with Trimmer compressions, HMM compressions are generally less readable and

            more likely to be misleading. Consider the ﬁnal sentence in Figure 4.

            (22) main purpose of reservation to pay American Indians by poverty proposals

            This is a compression of the following source sentence:

            (23) But the main purpose of the visit—the ﬁrst to a reservation by a president since Franklin
            Roosevelt—was simply to pay attention to American Indians, who are so raked by grinding
            poverty that Clinton’s own advisers suggested he come up with special proposals geared speciﬁ-
            cally to the Indians’ plight.

            Because HMM Hedge uses a bigram model of Headlines, it is unable to capture sentence-level
            grammaticality. The same limitation makes it diﬃcult to prevent misleading or incorrect compressions.
            For example, the third sentence from the end of Figure 4 seems to say that a court legalized gambling

            on Indian reservations:

            (24) Supreme Court allows legalized gambling Indian reservations

            However, it is a compression of the following source sentence:

            (25) Only Monday, the California Supreme Court overturned a ballot measure that would have allowed

            expansion of legalized gambling on Indian reservations.

            Nevertheless, we can see from the examples that sentence compression allows a summary to include

            more material from other sources. This increases the topic coverage of system output.

            17


            Seeking to get more accurate count of country’s American Indian population, Census Bureau turning to
            tribal leaders and residents on reservations to help overcome long-standing feelings. ◦ American Indian
            reservations would get infusion. ◦ Smith and thousands seeking help for substance abuse at American
            Indian Community House, largest of handful of Native American cultural institutions. ◦ Clinton going
            to Pine Ridge Reservation for visit with Oglala Sioux nation and to participate in conference on Native
            American homeownership and economic development. ◦ Said Glen Revere, nutritionist with Indian Health
            Services on 2.8 million-acre Wind River Reservation, about 100 miles east of Jackson, Wyo. “Then we
            came up with idea for this community garden, and it been bigger than we ever expected.” ◦ Road leading
            into Shinnecock Indian reservation is not welcoming one But main purpose of visit – ﬁrst to reservation
            by president since Franklin Roosevelt – was simply to pay attention to American Indians, who raked by
            grinding poverty Clinton’s own advisers suggested he come up with special proposals geared speciﬁcally to
            Indians’ plight. ◦ “This highlights what going on out there, since beginning of reservation system,” said
            Sidney Harring, professor at City University of New York School of Law and expert on Indian crime and
            criminal law. ◦ American Indians are victims. ◦ President Clinton turned attention to arguably poorest,
            most forgotten ◦ U.S. citizens: American Indians. ◦ When American Indians began embracing gambling,
            Hualapai tribe moved quickly to open casino. ◦ members of Northern Arapaho Tribe on Wind River
            Reservation started seven-acre community garden with donated land, seeds and

            Figure 3: MCR Summary for DUC-2006 Topic D0601A, using Trimmer for sentence compression.

            David Rocchio deputy legal counsel to Vermont Gov. Howard Dean who has been involved in discussions
            on Indian gambling through the National Governors Association said that the concern that governors have
            is not with the beneﬁt casinos bring to tribes ◦ Native Americans living on reservations that maintain 50
            percent or more unemployment are exempt from the national ﬁve year family limit on welfare beneﬁts ◦
            Smith and thousands like her are seeking help for their substance abuse at the American Indian Community
            House the largest of a handful of Native American cultural institutions in the New York area ◦ Juvenile
            crime is one strand in the web of social problems facing urban and reservation Indian communities the
            report said ◦ Soldierwolf’s family represents the problems that plague many of the 1.3 million American
            Indians who live on reservations of whom 49 percent are unemployed ◦ Powless said the Onondaga people
            want to work with the community outside the reservation to improve the economy of the region perhaps
            creating tourism destinations that might include Indian culture or setting up a free trade zone at unused
            manufacturing sites ◦ As Indian communities across the nation struggle with short funds and a long list
            of problems they are watching the Navajo Nation’s legal battle with the federal government ◦ recognize
            Indians not only Native Americans as Americans ◦ go on reservation system Harring Indian ◦ Supreme
            Court allows legalized gambling Indian reservations ◦ American Indian reservations tribal colleges rise
            faster
            than ◦ main purpose of reservation to pay American Indians by poverty proposals

            Figure 4: MCR Summary for DUC-2006 Topic D0601A, using HMM Hedge for sentence compression

            18


            Seeking to get a more accurate count of the country’s American Indian population, the Census Bureau is
            turning to tribal leaders and residents on reservations to help overcome long-standing feelings of wariness
            or anger toward the federal government. ◦ American Indian reservations would get an infusion of $1.2
            billion in federal money for education, health care and law enforcement under President Clinton’s proposed
            2001 budget ◦ Smith and thousands like her are seeking help for their substance abuse at the American
            Indian Community House, the largest of a handful of Native American cultural institutions in the New
            York area. ◦ Clinton was going to the Pine Ridge Reservation for a visit with the Oglala Sioux nation
            and to participate in a conference on Native American homeownership and economic development. ◦ said
            Glen Revere, a nutritionist with the Indian Health Services on the 2.8 million-acre Wind River Reservation,
            about 100 miles east of Jackson, Wyo. “Then we came up with the idea for this community garden, and
            it’s been bigger than we ever expected in so many ways.” ◦ The road leading into the Shinnecock Indian
            reservation is not a welcoming one ◦ But the main purpose of the visit – the ﬁrst to a reservation by a
            president since Franklin Roosevelt – was simply to pay attention to American Indians, who are so raked by
            grinding poverty that Clinton’s own advisers suggested he come up with special proposals geared speciﬁcally
            to the Indians’ plight. ◦ “This highlights what has been going on out there for 130 years,

            Figure 5: MCR Summary for DUC-2006 Topic D0601A, with no sentence compression

            19


            R1 Recall

            R1 Precision

            R1 F

            R2 Recall

            R2 Precision

            R2 F

            HMM
            Sentence
            0.23552
            (0.23014-
            0.24082)
            0.21896
            (0.21386-
            0.22384)
            0.22496
            (0.21983-
            0.22978)
            0.06838
            (0.06546-
            0.07155)
            0.06287
            (0.06017-
            0.06576)
            0.06488
            (0.06209-
            0.06785)

            HMM
            60 Block
            0.21381
            (0.20912-
            0.21827)
            0.18882
            (0.18444-
            0.19301)
            0.19966
            (0.19505-
            0.20391)
            0.06133
            (0.05848-
            0.06414)
            0.05351
            (0.05097-
            0.05588)
            0.05686
            (0.05420-
            0.05942)

            Trimmer

            Topiary

            0.21014
            (0.20436-
            0.21594)
            0.20183
            (0.19627-
            0.20722)
            0.20179
            (0.19612-
            0.20718)
            0.06337
            (0.06030-
            0.06677)
            0.06230
            (0.05887-
            0.06617)
            0.06079
            (0.05788-
            0.06401)

            0.25143
            (0.24632-
            0.25663)
            0.23038
            (0.22567-
            0.23522)
            0.23848
            (0.23373-
            0.24328)
            0.06637
            (0.06345-
            0.06958)
            0.06024
            (0.05747-
            0.06326)
            0.06252
            (0.05976-
            0.06561)

            Table 2: Rouge scores and 95% conﬁdence intervals for 624 documents from DUC-2003 test set.

            5 System Evaluations

            We tested four single-document summarization systems on the DUC-2003 Task 1 test set:

            • HMM Hedge using the ﬁrst sentence of each document (HMM Sentence)

            • HMM Hedge using the ﬁrst 60 words of each document (HMM 60 block)

            • Trimmer

            • Topiary

            Task 1 from DUC-2003 was to construct generic 75-byte summaries for 624 documents drawn from AP
            Newswire and the New York Times. The average size of the documents was 3,997 bytes, so a 75-byte
            summary represents a compression ratio of 1.9%.

            An automatic summarization evaluation tool, Rouge (Lin and Hovy, 2003), was used to evaluate
            the results. The system parameters were optimized by hand to maximize the Rouge-1 recall on
            a comparable training corpus, 500 AP Newswire and New York Times articles from the DUC-2004
            single-document short summary test data.

            The Rouge results are shown in Table 2. Results show that HMM Hedge 60 scored signiﬁcantly
            lower than most other systems and that Topiary scored higher than all other systems for all R1
            In addition, HMM Hedge Sentence scored signiﬁcantly higher than Trimmer for the R1
            measures.
            measures.

            We also evaluated Trimmer and HMM Hedge as components in our Multi-Candidate Reduction
            framework, along with a baseline that uses the same sentence selector but does not use sentence
            compression. All three systems considered the ﬁrst ﬁve sentences of each document and used the
            sentence selection algorithm presented in Section 4. The feature weights were manually optimized to
            maximize Rouge-2 recall on a comparable training corpus, 1,593 Financial Times and Los Angeles
            Times articles grouped into 50 topics from the DUC-2005 query-focused multi-document summarization

            20


            Trimmer

            HMM Hedge No

            R1 Recall

            R2 Recall

            0.29391
            (0.28560-
            0.30247)
            0.06718
            (0.06332-
            0.07111)

            0.27311
            (0.26554-
            0.28008)
            0.06251
            (0.05873-
            0.06620)

            Compression
            0.27576
            (0.26772-
            0.28430)
            0.06126
            (0.05767-
            0.06519)

            Table 3: Rouge scores and 95% conﬁdence intervals for 50 DUC-2006 test topics, comparing three
            MCR variants.

            MCR Score
            Higher
            Not Diﬀerent
            Range
            Lower

            Rouge-2
            0.0805
            1
            23
            0.0678-0.0899
            11

            Rouge-SU4
            0.1360
            1
            24
            0.1238-0.1475
            10

            BE-HM
            0.0413
            0
            27
            0.0318-0.0508
            8

            Table 4: Oﬃcial DUC-2006 Automatic Metrics for our MCR submission (System 32).

            test data. The systems were used to generate query-focused, 250-word summaries using the DUC-2006
            test data, described in Section 4.3.

            The systems were evaluated using Rouge, conﬁgured to omit stopwords from the calculation.7
            Results are shown in Table 3. MCR using Trimmer compressions scored signiﬁcantly higher than
            MCR using HMM Hedge compressions and the baseline for Rouge-1, but there was not a signiﬁcant
            diﬀerence among the three systems for Rouge-2.

            Finally, the University of Maryland and BBN submitted a version of MCR to the oﬃcial DUC-
            2006 evaluation. This version used Trimmer as the source of sentence compressions. Results show
            that use of sentence compression hurt the system on human evaluation of grammaticality. This is not
            surprising, since Trimmer aims to produce compressions that are grammatical in Headlinese, rather
            than standard English. Our MCR run scored signiﬁcantly lower than 23 systems on NIST’s human
            evaluation of grammaticality. However, the system did not score signiﬁcantly lower than any other
            system on NIST’s human evaluation of content responsiveness. A second NIST evaluation of content
            responsiveness asked evaluators to take readability into consideration. In this evaluation, MCR scored
            signiﬁcantly lower than only two systems. The evaluators recognized that Trimmer compressions are
            not grammatical in standard English; yet, the content coverage was not signiﬁcantly diﬀerent from the
            best automatic systems and only two systems were found to be signiﬁcantly more readable.

            NIST computed three “oﬃcial” automatic evaluation metrics for DUC-2006: Rouge-2, Rouge-
            SU4 and BE-HM. Table 4 shows the oﬃcial scores of the submitted MCR system for these three
            metrics, along with numbers of systems that scored signiﬁcantly higher, signiﬁcantly lower, or were not
            signiﬁcantly diﬀerent from our MCR run. Also shown is the range of scores for the systems that were
            not signiﬁcantly diﬀerent from MCR. These results show that the performance of our MCR run was
            comparable to most other systems submitted to DUC-2006.

            7This is a change in the Rouge conﬁguration from the oﬃcial DUC-2006 evaluation. We note that the removal of
            non-essential stopwords (typical of Headlinese) is an important component of Trimmer-based sentence
            compression. For
            internal system comparisons, we conﬁgure Rouge in a way that will allow us to detect system diﬀerences
            relevant to our
            research focus. For reporting of oﬃcial Rouge results on submitted systems we use the community’s accepted
            Rouge
            conﬁgurations.

            21


            The evaluation in Table 3 suggests that Trimmer sentence compression is preferable to HMM
            Hedge sentence compression for generation of English summaries of collections of document in English.
            However, HMM Hedge may prove to have value with noisier data, as we discuss in the next section.
            Nevertheless, sentence compression appears to be a valuable component of our framework for multi-
            document summarization, thus validating the ideas behind Multi-Candidate Reduction.

            6 Applications to Diﬀerent Types of Texts

            We have applied the MCR framework to summarizing diﬀerent types of texts. In this section we brieﬂy
            touch on genre-speciﬁc issues that are the subject of ongoing work. Trimmer, Topiary, and HMM Hedge
            were designed for summarization of written news. In this genre, the lead sentence is almost always
            the ﬁrst non-trivial sentence of the document. More sophisticated methods for ﬁnding lead sentences
            did not outperform the baseline of simply selecting the ﬁrst sentence for AP wire “hard” news stories.
            However, some types of articles, such as sports stories, opinion pieces, and movie reviews often do
            not have informative lead sentences and will require additional work in ﬁnding the best sentence for
            compression.

            MCR has also been applied to summarizing transcripts of broadcast news—another input form
            where lead sentences are often not informative. The conventions of broadcast news introduce categories
            of story-initial light content sentences, such as “I’m Dan Rather” or “We have an update on the story
            we’ve been following”. These present challenges for the ﬁltering stage of our MCR framework.

            Such texts are additionally complicated by a range of problems not encountered in written news:
            noise introduced by automatic speech recognizers or other faulty transcription, issues associated with
            sentence boundary detection and story boundary detection. If word error rate is high, parser failures
            In this context, HMM Hedge becomes more
            can prevent Trimmer from producing useful output.
            attractive, since our language models are more resilient to noisy input.

            We have performed an initial evaluation of Trimmer, Topiary, and a baseline consisting of the
            ﬁrst 75 characters of a document, on the task of creating 75-character headlines for broadcast news
            transcriptions (Zajic, 2007). The corpus for this task consisted of 560 broadcast news stories from
            ABC, CNN, NBC, Public Radio International, and Voice of America. We used Rouge-1 recall to
            evaluate the summaries and found that both systems scored higher than the baseline and that Topiary
            scored higher than Trimmer. However there were no signiﬁcant diﬀerences among the systems.

            Another application of our framework is the summarization of email threads—collections of emails
            that share a common topic or were written as responses to each other. This task can essentially be
            treated as a multi-document summarization problem, albeit email thread structure introduces some
            constraints with respect to the ordering of summary sentences. Noisy data is inherent in this problem
            and pre-processing to remove quoted text, attachments, and headers is crucial. We have found that
            metadata, such as the name of the sender of each included extract help make email summaries easier
            to read.

            We performed an initial evaluation of HMM Hedge and Trimmer as the source of sentence com-
            pressions for an email thread summarization system based on the MCR framework (Zajic, 2007). The
            corpus for this task consisted of 10 manually constructed email threads from the Enron Corpus (Klimt
            and Yang, 2004). We used Rouge-1 and Rouge-2 recall with jackkniﬁng to compare the automatic
            systems and the human summarizers. We did not observe a signiﬁcant diﬀerence between the two sys-
            tems, but we found that the task of summarizing email threads was extremely diﬃcult for the humans
            (one summarizer scored signiﬁcantly worse than the automatic systems). This application of MCR to
            email thread summarization is an initial eﬀort. The diﬃculty of the task for the humans suggests that
            the community needs to develop a clearer understanding of what makes a good email thread summary
            and to explore practical uses for them.

            22


            Finally, Trimmer and HMM Hedge have been applied to Hindi-English cross-language summariza-
            In this case, Trimmer was applied to the output of machine translation. We adapted HMM
            tion.
            Hedge to cross-lingual summarization by using the mechanism developed for morphological variation
            to represent translation probabilities from Hindi story words to English headline words. For more
            details, see Dorr et al. (2003a).
        </corps>
        <conclusion>8 Conclusion

            This work presents Multi-Candidate Reduction, a general architecture for multi-document summa-
            rization. The framework integrates successful single-document compression techniques that we have
            previously developed. MCR is motivated by the insight that multiple candidate compressions of source
            sentences should be made available to subsequent processing modules, which may have access to more
            information for summary construction. This is implemented in a dynamic feature-based sentence se-
            lector that iteratively builds a summary from compressed variants. Evaluations show that sentence
            compression plays an important role in multi-document summarization and that our MCR framework
            is both ﬂexible and extensible.

            Acknowledgments

            This work has been supported, in part, under the GALE program of the Defense Advanced Research
            Projects Agency, Contract No. HR0011-06-2-0001, the TIDES program of the Defense Advanced
            Research Projects Agency, BBNT Contract No. 9500006806, and the University of Maryland Joint
            Institute for Knowledge Discovery. Any opinions, ﬁndings, conclusions or recommendations expressed
            in this paper are those of the authors and do not necessarily reﬂect the views of DARPA. The ﬁrst
            author would like to thank Naomi for proofreading, support, and encouragement. The second author
            would like to thank Steve, Carissa, and Ryan for their energy enablement. The third author would like
            to thank Esther and Kiri for their kind support.
        </conclusion>
        <discussion>Aucune discussion trouvée.</discussion>
        <biblio>L. Bahl, F. Jelinek, and R. Mercer. 1983. A maximum likelihood approach to continuous speech
            recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 5(2):179–190.

            M. Banko, V. Mittal, and M. Witbrock. 2000. Headline generation based on statistical translation.
            In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL
            2000), pages 318–325, Hong Kong.

            R. Barzilay, N. Elhadad, and K. McKeown. 2002. Inferring strategies for sentence ordering in multi-

            document news summarization. Journal of Artiﬁcial Intelligence Research, 17:35–55.

            L. Baum. 1972. An inequality and associated maximization technique in statistical estimation of

            probabilistic functions of a Markov process. Inequalities, 3:1–8.

            S. Bergler, R. Witte, M. Khalife, Z. Li, and F. Rudzicz. 2003. Using knowledge-poor coreference
            resolution for text summarization. In Proceedings of the HLT-NAACL 2003 Text Summarization
            Workshop and Document Understanding Conference (DUC 2003), pages 85–92, Edmonton, Alberta.

            D. Bikel, R. Schwartz, and R. Weischedel. 1999. An algorithm that learns what’s in a name. Machine

            Learning, 34(1/3):211–231.

            S. Blair-Goldensohn, D. Evans, V. Hatzivassiloglou, K. McKeown, A. Nenkova, R. Passonneau,
            2004. Columbia University
            In Proceedings of the 2004 Document Understanding Conference (DUC 2004) at

            B. Schiﬀman, A. Schlaikjer, A. Siddharthan, and S. Siegelman.
            at DUC 2004.
            HLT/NAACL 2004, pages 23–30, Boston, Massachusetts.

            24


            P. Brown, J. Cocke, S. Pietra, V. Pietra, F. Jelinek, J. Laﬀerty, R. Mercer, and P. Roossin. 1990. A

            statistical approach to machine translation. Computational Linguistics, 16(2):79–85.

            J. Carbonell and J. Goldstein. 1998. The use of MMR, diversity-based reranking for reordering
            documents and producing summaries. In Proceedings of the 21st Annual International ACM SIGIR
            Conference on Research and Development in Information Retrieval (SIGIR 1998), pages 335–336,
            Melbourne, Australia.

            Eugene Charniak. 2000. A Maximum-Entropy-Inspired Parser. In Proceedings of the First Meeting
            of the North American Chapter of the Association for Computational Linguistics (NAACL 2000),
            pages 132–139, Seattle, Washington.

            J. Clarke and M. Lapata. 2006. Models for sentence compression: A comparison across domains,
            training requirements and evaluation measures. In Proceedings of the 21st International Confer-
            ence on Computational Linguistics and 44th Annual Meeting of the Association for Computational
            Linguistics (COLING/ACL 2006), pages 377–384, Sydney, Australia.

            J. Conroy, J. Schlesinger, and J. Goldstein. 2005. CLASSY query-based multi-document summariza-
            tion. In Proceedings of the 2005 Document Understanding Conference (DUC-2005) at NLT/EMNLP
            2005, Vancouver, Canada.

            J. Conroy, J. Schlesinger, D. O’Leary, and J. Goldstein. 2006. Back to basics: CLASSY 2006. In
            Proceedings of the 2006 Document Understanding Conference (DUC 2006) at HLT/NAACL 2006,
            New York, New York.

            D. Cutting, J. Pedersen, and P. Sibun. 1992. A practical part-of-speech tagger. In Proceedings of the

            Third Conference on Applied Natural Language Processing, Trento, Italy.

            Hoa Dang and Donna Harman. 2006. Proceedings of the 2006 Document Understanding Conference

            (DUC 2006) at HLT/NAACL 2006.

            B. Dorr and T. Gaasterland. this special issue, 2007. Exploiting aspectual features and connecting
            words for summarization-inspired temporal-relation extraction. Information Processing and Man-
            agement.

            B. Dorr, D. Zajic, and R. Schwartz. 2003a. Cross-language headline generation for Hindi. ACM

            Transactions on Asian Language Information Processing (TALIP), 2(3):270–289.

            B. Dorr, D. Zajic, and R. Schwartz. 2003b. Hedge Trimmer: A parse-and-trim approach to headline
            generation. In Proceedings of the HLT-NAACL 2003 Text Summarization Workshop and Document
            Understanding Conference (DUC 2003), pages 1–8, Edmonton, Alberta.

            T. Dunning. 1994. Statistical identiﬁcation of language. Technical Report MCCS 94-273, New Mexico

            State University.

            T. Euler. 2002. Tailoring text using topic words: Selection and compression. In Proceedings of 13th
            International Workshop on Database and Expert Systems Applications (DEXA 2002), pages 215–
            222, Aix-en-Provence, France.

            J. Goldstein, V. Mittal, J. Carbonell, and M. Kantrowitz. 2000. Multi-document summarization by
            sentence extraction. In Proceedings of ANLP/NAACL 2000 Workshop on Automatic Summariza-
            tion, pages 40–48.

            D. Harman and M. Liberman. 1993. TIPSTER Complete. Linguistic Data Consortium (LDC),

            Philadelphia.

            25


            H. Jing and K. McKeown. 2000. Cut and paste based text summarization. In Proceedings of the First
            Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL
            2000), pages 178–185, Seattle, Washington.

            B. Klimt and Y. Yang. 2004. Introducing the Enron Corpus. In Proceedings of the First Conference

            on Email and Anti-Spam (CEAS), Mountain View, California.

            K. Knight and D. Marcu. 2000. Statistics-based summarization—step one: Sentence compression. In
            Proceedings of the Seventeenth National Conference on Artiﬁcial Intelligence (AAAI-2000), Austin,
            Texas.

            K. Knight and D. Marcu. 2002. Summarization beyond sentence extraction: A probabilistic approach

            to sentence compression. Artiﬁcial Intelligence, 139(1):91–107.

            M. Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proceedings
            of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL 2004), pages
            545–552, Barcelona, Spain.

            David Dolan Lewis. 1999. An evaluation of phrasal and clustered representations on a text categoriza-
            tion task. In Proceedings of the 15th Annual International ACM SIGIR Conference on Research
            and Development in Information Retrieval (SIGIR 1992), pages 37–50, Copenhagen, Denmark.

            C.-Y. Lin and E. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statis-
            tics. In Proceedings of the 2003 Human Language Technology Conference and the North American
            Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2003),
            pages 71–78, Edmonton, Alberta.

            I. M˚ardh. 1980. Headlinese: On the Grammar of English Front Page Headlines. Malmo.

            E. Mays, F. Damerau, and R. Mercer. 1990. Context-based spelling correction. In Proceedings of IBM

            Natural Language ITL, pages 517–522, Paris, France.

            S. Miller, L. Ramshaw, H. Fox, and R. Weischedel. 2000. A novel use of statistical parsing to extract
            information from text. In Proceedings of the First Meeting of the North American Chapter of the
            Association for Computational Linguistics (NAACL 2000), pages 226–233, Seattle, Washington.

            S. Muresan, E. Tzoukermann, and J. Klavans. 2001. Combining linguistic and machine learning
            techniques for email. In Proceedings of the ACL/EACL 2001 Workshop on Computational Natural
            Language Learning (ConLL), pages 290–297, Toulouse, France.

            N. Okazaki, Y. Matsuo, and M. Ishizuka. 2004. Improving chronological sentence ordering by prece-
            dence relation. In Proceedings of the 20th International Conference on Computational Linguistics
            (COLING 2004), pages 750–756, Geneva, Switzerland.

            M. Porter. 1980. An algorithm for suﬃx stripping. Program, 14(3):130–137.

            D. Radev, T. Allison, S. Blair-Goldensohn, J. Blitzer, A. C¸ elebi, S. Dimitrov, E. Drabek, A. Hakim,
            W. Lam, D. Liu, J. Otterbacher, H. Qi, H. Saggion, S. Teufel, M. Topper, A. Winkel, and Z. Zhang.
            2004. MEAD—a platform for multidocument multilingual text summarization. In Proceedings of
            the 4th International Conference on Language Resources and Evaluation (LREC 2004), Lisbon,
            Portugal.

            26


            R. Schwartz, T. Imai, F. Jubala, L. Nguyen, and J. Makhoul. 1997. A maximum likelihood model
            for topic classiﬁcation of broadcast news. In Proceedings of the Fifth European Speech Communica-
            tion Association Conference on Speech Communication and Technology (Eurospeech-97), Rhodes,
            Greece.

            S. Sista, R. Schwartz, T. Leek, and J. Makhoul. 2002. An algorithm for unsupervised topic discovery
            from broadcast news stories. In Proceedings of the 2002 Human Language Technology Conference
            (HLT), pages 99–103, San Diego, California.

            J. Turner and E. Charniak. 2005. Supervised and unsupervised learning for sentence compression.
            In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL
            2005), pages 290–297, Ann Arbor, Michigan.

            L. Vanderwende, H. Suzuki, and C. Brockett. 2006. Microsoft Research at DUC2006: Task-focused
            summarization with sentence simpliﬁcation and lexical expansion. In Proceedings of the 2006 Doc-
            ument Understanding Conference (DUC 2006) at HLT/NAACL 2006, New York, New York.

            A. Viterbi. 1967. Error bounds for convolution codes and an asymptotically optimal decoding algo-

            rithm. IEEE Transactions on Information Theory, 13:260–269.

            R. Wang, N. Stokes, W. Doran, E. Newman, J. Carthy, and J. Dunnion. 2005. Comparing Topiary-
            In Lecture Notes in Computer Science: Advances in
            style approaches to headline generation.
            Information Retrieval: 27th European Conference on IR Research (ECIR 2005), volume 3408,
            Santiago de Compostela, Spain. Springer Berlin / Heidelberg.

            D. Zajic, B. Dorr, and R. Schwartz. 2004. BBN/UMD at DUC-2004: Topiary.

            In Proceedings of
            the 2004 Document Understanding Conference (DUC 2004) at NLT/NAACL 2004, pages 112–119,
            Boston, Massachusetts.

            D. Zajic, B. Dorr, J. Lin, and R. Schwartz. 2005a. UMD/BBN at MSE2005. In Proceedings of the
            MSE2005 Track of the ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for
            MT and/or Summarization, Ann Arbor, Michigan.

            D. Zajic, B. Dorr, R. Schwartz, C. Monz, and J. Lin. 2005b. A sentence-trimming approach to
            multi-document summarization. In Proceedings of the 2005 Document Understanding Conference
            (DUC-2005) at NLT/EMNLP 2005, pages 151–158, Vancouver, Canada.

            D. Zajic. 2007. Multiple Alternative Sentence Compressions (MASC) as a Tool for Automatic Sum-

            marization Tasks. Ph.D. thesis, University of Maryland, College Park.

            L. Zhou and E. Hovy. 2003. Headline summarization at ISI.

            In Proceedings of the HLT-NAACL
            2003 Text Summarization Workshop and Document Understanding Conference (DUC 2003), pages
            174–178, Edmonton, Alberta.

            27
        </biblio>
    </article>
    <article>
        <preamble>compression_phrases_Prog-Linear-jair.txt</preamble>
        <titre>Global Inference for Sentence Compression An Integer Linear Programming Approach</titre>
        <auteur>James Clarke</auteur>
        <abstract>Sentence compression holds promise for many applications ranging from summarization to subtitle
            generation. Our work views sentence compression as an optimization problem and uses integer linear
            programming (ILP) to infer globally optimal compressions in the presence of linguistically motivated
            constraints. We show how previous formulations of sentence compression can be recast as ILPs and extend
            these models with novel global constraints. Experimental results on written and spoken texts demonstrate
            improvements over state-of-the-art models.
        </abstract>
        <introduction>The computational treatment of sentence compression has recently attracted much attention in the
            literature. The task can be viewed as producing a summary of a single sentence that retains the most
            important information and remains grammatical (Jing, 2000). A sentence compression mechanism would greatly
            beneﬁt a wide range of applications. For example, in summarization, it could improve the conciseness of the
            generated summaries (Jing, 2000; Lin, 2003; Zajic, Door, Lin, & Schwartz, 2007). Other examples include
            compressing text to be displayed on small screens such as mobile phones or PDAs (Corston-Oliver, 2001),
            subtitle generation from spoken transcripts (Vandeghinste & Pan, 2004), and producing audio scanning devices
            for the blind (Grefenstette, 1998). Sentence compression is commonly expressed as a word deletion problem:
            given an in- put source sentence of words x = x1, x2, . . . , xn, the aim is to produce a target compression
            by removing any subset of these words (Knight & Marcu, 2002). The compression prob- lem has been extensively
            studied across diﬀerent modeling paradigms, both supervised and unsupervised. Supervised models are
            typically trained on a parallel corpus of source sen- tences and target compressions and come in many
            ﬂavors. Generative models aim to model the probability of a target compression given the source sentence
            either directly (Galley & McKeown, 2007) or indirectly using the noisy-channel model (Knight & Marcu, 2002;
            Turner & Charniak, 2005), whereas discriminative formulations attempt to minimize error rate on a training
            set. These include decision-tree learning (Knight & Marcu, 2002), maxi- mum entropy (Riezler, King, Crouch,
            & Zaenen, 2003), support vector machines (Nguyen, Shimazu, Horiguchi, Ho, & Fukushi, 2004), and large-margin
            learning (McDonald, 2006). c(cid:13)2008 AI Access Foundation. All rights reserved. Clarke & Lapata
            Unsupervised methods dispense with the parallel corpus and generate compressions either using rules (Turner
            & Charniak, 2005) or a language model (Hori & Furui, 2004). Despite diﬀerences in formulation, all these
            approaches model the compression process using local information. For instance, in order to decide which
            words to drop, they exploit information about adjacent words or constituents. Local models can do a good job
            at producing grammatical compressions, however they are somewhat limited in scope since they cannot
            incorporate global constraints on the compression output. Such constraints consider the sentence as a whole
            instead of isolated linguistic units (words or constituents). To give a concrete example we may want to
            ensure that each target compression has a verb, provided that the source had one in the ﬁrst place. Or that
            verbal arguments are present in the compression. Or that pronouns are retained. Such constraints are fairly
            intuitive and can be used to instill not only linguistic but also task speciﬁc information into the model.
            For instance, an application which compresses text to be displayed on small screens would presumably have a
            higher compression rate than a system generating subtitles from spoken text. A global constraint could force
            the former system to generate compressions with a ﬁxed rate or a ﬁxed number of words. Existing approaches
            do not model global properties of the compression problem for a good reason. Finding the best compression
            for a source sentence given the space of all possible compressions1 (this search process is often referred
            to as decoding or inference) can become intractable for too many constraints and overly long sentences.
            Typically, the decoding problem is solved eﬃciently using dynamic programming often in conjunction with
            heuristics that reduce the search space (e.g., Turner & Charniak, 2005). Dynamic programming guarantees we
            will ﬁnd the global optimum provided the principle of optimal- ity holds. This principle states that given
            the current state, the optimal decision for each of the remaining stages does not depend on previously
            reached stages or previously made decisions (Winston & Venkataramanan, 2003). However, we know this to be
            false in the case of sentence compression. For example, if we have included modiﬁers to the left of a noun
            in a compression then we should probably include the noun too or if we include a verb we should also include
            its arguments. With a dynamic programming approach we cannot easily guarantee such constraints hold. In this
            paper we propose a novel framework for sentence compression that incorporates constraints on the compression
            output and allows us to ﬁnd an optimal solution. Our formulation uses integer linear programming (ILP), a
            general-purpose exact framework for NP-hard problems. Speciﬁcally, we show how previously proposed models
            can be recast as integer linear programs. We extend these models with constraints which we express as linear
            inequalities. Decoding in this framework amounts to ﬁnding the best solution given a linear (scoring)
            function and a set of linear constraints that can be either global or local. Although ILP has been
            previously used for sequence labeling tasks (Roth & Yih, 2004; Punyakanok, Roth, Yih, & Zimak, 2004), its
            application to natural language generation is less widespread. We present three compression models within
            the ILP framework, each representative of an unsupervised (Knight & Marcu, 2002), semi-supervised (Hori &
            Furui, 2004), and fully supervised modeling approach (McDonald, 2006). We propose a small number of
            constraints ensuring that the compressions are structurally and semantically 1. There are 2 n possible
            compressions where n is the number of words in a sentence. 400 Global Inference for Sentence Compression
            valid and experimentally evaluate their impact on the compression task. In all cases, we show that the added
            constraints yield performance improvements. The remainder of this paper is organized as follows. Section 2
            provides an overview of related work. In Section 3 we present the ILP framework and the compression models
            we employ in our experiments. Our constraints are introduced in Section 3.5. Section 4.3 discusses our
            experimental set-up and Section 5 presents our results. Discussion of future work concludes the paper.
        </introduction>
        <corps>2. Related Work

            In this paper we develop several ILP-based compression models. Before presenting these
            models, we brieﬂy summarize previous work addressing sentence compression with an em-
            phasis on data-driven approaches. Next, we describe how ILP techniques have been used
            in the past to solve other inference problems in natural language processing (NLP).

            2.1 Sentence Compression

            Jing (2000) was perhaps the ﬁrst to tackle the sentence compression problem. Her approach
            uses multiple knowledge sources to determine which phrases in a sentence to remove. Central
            to her system is a grammar checking module that speciﬁes which sentential constituents
            are grammatically obligatory and should therefore be present in the compression. This
            is achieved using simple rules and a large-scale lexicon. Other knowledge sources include
            WordNet and corpus evidence gathered from a parallel corpus of source-target sentence
            pairs. A phrase is removed only if it is not grammatically obligatory, not the focus of the
            local context and has a reasonable deletion probability (estimated from a parallel corpus).
            In contrast to Jing (2000), the bulk of the research on sentence compression relies ex-
            clusively on corpus data for modeling the compression process without recourse to exten-
            sive knowledge sources (e.g., WordNet). A large number of approaches are based on the
            noisy-channel model (Knight & Marcu, 2002). These approaches consist of a language
            model P (y) (whose role is to guarantee that compression output is grammatical), a channel
            model P (x|y) (capturing the probability that the source sentence x is an expansion of the
            target compression y), and a decoder (which searches for the compression y that maximizes
            P (y)P (x|y)). The channel model is acquired from a parsed version of a parallel corpus; it
            is essentially a stochastic synchronous context-free grammar (Aho & Ullman, 1969) whose
            rule probabilities are estimated using maximum likelihood. Modiﬁcations of this model are
            presented by Turner and Charniak (2005) and Galley and McKeown (2007) with improved
            RESULTS.

            In discriminative models (Knight & Marcu, 2002; Riezler et al., 2003; McDonald, 2006;
            Nguyen et al., 2004) sentences are represented by a rich feature space (also induced from
            parse trees) and the goal is to learn which words or word spans should be deleted in a given
            context. For instance, in Knight and Marcu’s (2002) decision-tree model, compression is
            performed deterministically through a tree rewriting process inspired by the shift-reduce
            parsing paradigm. Nguyen et al. (2004) render this model probabilistic through the use
            of support vector machines. McDonald (2006) formalizes sentence compression in a large-
            margin learning framework without making reference to shift-reduce parsing. In his model
            compression is a classiﬁcation task: pairs of words from the source sentence are classiﬁed

            401


            Clarke & Lapata

            as being adjacent or not in the target compression. A large number of features are deﬁned
            over words, parts-of-speech, phrase structure trees and dependencies. These features are
            gathered over adjacent words in the compression and the words in-between which were
            dropped (see Section 3.4.3 for a more detailed account).

            While most compression models have been developed with written text in mind, Hori
            and Furui (2004) propose a model for automatically transcribed spoken text. Their model
            generates compressions through word deletion without using parallel data or syntactic in-
            formation in any way. Assuming a ﬁxed compression rate, it searches for the compression
            with the highest score using a dynamic programming algorithm. The scoring function con-
            sists of a language model responsible for producing grammatical output, a signiﬁcance score
            indicating whether a word is topical or not, and a score representing the speech recognizer’s
            conﬁdence in transcribing a given word correctly.

            2.2 Integer Linear Programming in NLP

            ILPs are constrained optimization problems where both the objective function and the
            constraints are linear equations with integer variables (see Section 3.1 for more details). ILP
            techniques have been recently applied to several NLP tasks, including relation extraction
            (Roth & Yih, 2004), semantic role labeling (Punyakanok et al., 2004), the generation of
            route directions (Marciniak & Strube, 2005), temporal link analysis (Bramsen, Deshpande,
            Lee, & Barzilay, 2006), set partitioning (Barzilay & Lapata, 2006), syntactic parsing (Riedel
            & Clarke, 2006), and coreference resolution (Denis & Baldridge, 2007).

            Most of these approaches combine a local classiﬁer with an inference procedure based
            on ILP. The classiﬁer proposes possible answers which are assessed in the presence of global
            ILP is used to make a ﬁnal decision that is consistent with the constraints
            constraints.
            and likely according to the classiﬁer. For example, the semantic role labeling task involves
            identifying the verb-argument structure for a given sentence. Punyakanok et al. (2004) ﬁrst
            use SNOW, a multi-class classiﬁer2 (Roth, 1998), to identify and label candidate arguments.
            They observe that the labels assigned to arguments in a sentence often contradict each other.
            To resolve these conﬂicts they propose global constraints (e.g., each argument should be
            instantiated once for a given verb, every verb should have at least one argument) and use
            ILP to reclassify the output of SNOW.

            Dras (1999) develops a document paraphrasing model using ILP. The key premise of
            his work is that in some cases one may want to rewrite a document so as to conform to
            some global constraints such as length, readability, or style. The proposed model has three
            ingredients: a set of sentence-level paraphrases for rewriting the text, a set of global con-
            straints, and an objective function which quantiﬁes the eﬀect incurred by the paraphrases.
            Under this formulation, ILP can be used to select which paraphrases to apply so that the
            global constraints are satisﬁed. Paraphrase generation falls outside the scope of the ILP
            model – sentence rewrite operations are mainly syntactic and provided by a module based
            on synchronous tree adjoining grammar (S-TAG, Shieber & Schabes, 1990). Unfortunately,
            only a proof-of-concept is presented; implementation and evaluation of this module are left
            to future work.

            2. SNOW’s learning algorithm is a variation of the Winnow update rule.

            402


            Global Inference for Sentence Compression

            Our work models sentence compression as an optimization problem. We show how pre-
            viously proposed models can be reformulated in the context of integer linear programming
            which allows us to easily incorporate constraints during the decoding process. Our con-
            straints are linguistically and semantically motivated and are designed to bring less local
            syntactic knowledge into the model and help preserve the meaning of the source sentence.
            Previous work has identiﬁed several important features for the compression task (Knight
            & Marcu, 2002; McDonald, 2006); however, the use of global constraints is novel to our
            knowledge. Although sentence compression has not been explicitly formulated in terms of
            optimization, previous approaches rely on some optimization procedure for generating the
            best compression. The decoding process in the noisy-channel model searches for the best
            compression given the source and channel models. However, the compression found is usu-
            ally sub-optimal as heuristics are used to reduce the search space or is only locally optimal
            due to the search method employed. For example, in the work of Turner and Charniak
            (2005) the decoder ﬁrst searches for the best combination of rules to apply. As it traverses
            the list of compression rules, it removes sentences outside the 100 best compressions (ac-
            cording to the channel model). This list is eventually truncated to 25 compressions. In
            other models (Hori & Furui, 2004; McDonald, 2006) the compression score is maximized
            using dynamic programming which however can yield suboptimal results (see the discussion
            in Section 1).

            Contrary to most other NLP work using ILP (a notable exception is Roth & Yih, 2005),
            we do not view compression generation as a two stage process where learning and inference
            are carried out sequentially (i.e., ﬁrst a local classiﬁer hypothesizes a list of possible an-
            swers and then the best answer is selected using global constraints). Our models integrate
            learning with inference in a uniﬁed framework where decoding takes place in the presence
            of all available constraints, both local and global. Moreover, we investigate the inﬂuence
            of our constraint set across models and learning paradigms. Previous work typically for-
            mulates constraints for a single model (e.g., the SNOW classiﬁer) and learning paradigm
            (e.g., supervised). We therefore assess how the constraint-based framework advocated in
            this article inﬂuences the performance of expressive models (which require large amounts of
            parallel data) and non-expressive ones (which use very little parallel data or none at all). In
            other words, we are able to pose and answer the following question: what kinds of models
            beneﬁt most from constraint-based inference?

            Our work is close in spirit but rather diﬀerent in content to Dras (1999). We concentrate
            on compression, a speciﬁc paraphrase type, and apply our models on the sentence-level. Our
            constraints thus do not aﬀect the document as a whole but individual sentences. Further-
            more, compression generation is an integral part of our ILP models, whereas Dras assumes
            that paraphrases are generated by a separate process.

            3. Framework

            In this section we present the details of the proposed framework for sentence compression.
            As mentioned earlier, our work models sentence compression directly as an optimization
            problem. There are 2n possible compressions for each source sentence and while many
            of these will be unreasonable, it is unlikely that only one compression will be satisfac-
            tory (Knight & Marcu, 2002). Ideally, we require a function that captures the operations

            403


            Clarke & Lapata

            (or rules) that can be performed on a sentence to create a compression while at the same
            time factoring how desirable each operation makes the resulting compression. We can then
            perform a search over all possible compressions and select the best one, as determined by
            how desirable it is. A wide range of models can be expressed under this framework. The
            prerequisites for implementing these are fairly low, we only require that the decoding pro-
            cess be expressed as a linear function with a set of linear constraints. In practice, many
            models rely on a Markov assumption for factorization which is usually solved with a dy-
            namic programming-based decoding process. Such algorithms can be formulated as integer
            linear programs with little eﬀort.

            We ﬁrst give a brief introduction into integer linear programming, an extension of linear
            programming for readers unfamiliar with mathematical programming. Our compression
            models are next described in Section 3.4 and constraints in Section 3.5.

            3.1 Linear Programming

            Linear programming (LP) problems are optimization problems with constraints. They
            consist of three parts:

            • Decision variables. These are variables under our control which we wish to assign

            optimal values to.

            • A linear function (the objective function). This is the function we wish to minimize or
            maximize. This function is inﬂuences by the values assigned to the decision variables.

            • Constraints. Most problems will only allow the decision variables to take certain

            values. These restrictions are the constraints.

            These terms are best demonstrated with a simple example taken from Winston and
            Venkataramanan (2003). Imagine a manufacturer of tables and chairs which we shall call
            the Telfa Corporation. To produce a table, 1 hour of labor and 9 square board feet of wood
            is required. Chairs require 1 hour of labor and 5 square board feet of wood. Telfa have
            6 hours of labor and 45 square board feet of wood available. The proﬁt made from each
            table is 8 GBP and 5 GBP for chairs. We wish to determine the number of tables and
            chairs that should be manufactured to maximize Telfa’s proﬁt.

            First, we must determine the decision variables. In our case we deﬁne:

            x1 = number of tables manufactured
            x2 = number of chairs manufactured

            Our objective function is the value we wish to maximize, namely the proﬁt.

            Proﬁt = 8x1 + 5x2

            There are two constraints in this problem: we must not exceed 6 hours of labor and no
            more than 45 square board feet of wood must be used. Also, we cannot create a negative
            amount of chairs or tables:

            404


            Global Inference for Sentence Compression

            Labor constraint
            Wood constraint
            Variable constraints

            x1 + x2 ≤ 6
            9x1 + 5x2 ≤ 45
            x1 ≥ 0
            x2 ≥ 0

            Once the decision variables, objective function and constraints have been determined we
            can express the LP model:

            subject to (s.t.)

            max z = 8x1 + 5x2

            (Objective function)

            x1 + x2 ≤ 6 (Labor constraint)
            9x1 + 5x2 ≤ 45 (Wood constraint)

            x1 ≥ 0
            x2 ≥ 0

            Two of the most basic concepts involved in solving LP problems are the feasibility region
            and optimal solution. The optimal solution is one in which all constraints are satisﬁed
            and the objective function is minimized or maximized. A speciﬁcation of the value for
            each decision variable is referred to as a point. The feasibility region for a LP is a region
            consisting of the set of all points that satisfy all the LP’s constraints. The optimal solution
            lies within this feasibility region, it is the point with the minimum or maximum objective
            function value.

            A set of points satisfying a single linear inequality is a half-space. The feasibility region
            is deﬁned by a the intersection of m half-spaces (for m linear inequalities) and forms a
            polyhedron. Our Telfa example forms a polyhedral set (a polyhedral convex set) from
            the intersection of our four constraints. Figure 1a shows the feasible region for the Telfa
            example. To ﬁnd the optimal solution we graph a line (or hyperplane) on which all points
            have the same objective function value. In maximization problems it is called the isoproﬁt
            line and in minimization problems the isocost line. One isoproﬁt line is represented by the
            dashed black line in Figure 1a. Once we have one isoproﬁt line we can ﬁnd all other isoproﬁt
            lines by moving parallel to the original isoproﬁt line.

            The extreme points of the polyhedral set are deﬁned as the intersections of the lines
            that form the boundaries of the polyhedral set (points A B C and D in Figure 1a). It can
            be shown that any LP that has an optimal solution, has an extreme point that is globally
            optimal. This reduces the search space of the optimization problem to ﬁnding the extreme
            point with the highest or lowest value. The simplex algorithm (Dantzig, 1963) solves LPs
            by exploring the extreme points of a polyhedral set. Speciﬁcally, it moves from one extreme
            point to an adjacent extreme point (extreme points that lie on the same line segment) until
            an optimal extreme point is found. Although the simplex algorithm has an exponential
            worst-case complexity, in practice the algorithm is very eﬃcient.
            The optimal solution for the Telfa example is z = 165

            4 . Thus, to
            achieve a maximum proﬁt of 41.25 GBP they must build 3.75 tables and 2.25 chairs. This
            is obviously impossible as we would not expect people to buy fractions of tables and chairs.
            Here, we want to be able to constrain the problem such that the decision variables can only
            take integer values. This can be done with Integer Linear Programming.

            4 , x1 = 15

            4 , x2 = 9

            405


            Clarke & Lapata

            9x1 + 5x2 = 45

            = LP’s feasible region

            B

            Optimal LP solution

            C

            b.

            10

            9

            8

            7

            6

            5

            4

            3

            2

            x2

            a.

            10

            x2

            9

            8

            7

            6

            5

            4

            3

            2

            1

            A

            0

            0

            1

            2

            3

            4

            x1

            x1 + x2 = 6

            D
            5

            6

            7

            1

            1

            0

            0

            9x1+ 5x2 = 45

            = IP feasible point
            = IP relaxation’s feasible region

            Optimal LP solution

            x1 + x2 = 6

            1

            2

            3

            x1

            4

            5

            6

            7

            Figure 1: Feasible region for the Telfa example using linear (graph (a)) and integer linear

            (graph (b)) programming

            3.2 Integer Linear Programming

            Integer linear programming (ILP) problems are LP problems in which some or all of the
            variables are required to be non-negative integers. They are formulated in a similar manner
            to LP problems with the added constraint that all decision variables must take non-negative
            integer values.

            To formulate the Telfa problem as an ILP model we merely add the constraints that x1

            and x2 must be integer. This gives:

            max z = 8x1 + 5x2

            (Objective function)

            subject to (s.t.)

            x1 + x2 ≤
            9x1 + 5x2 ≤

            6 (Labor constraint)
            45 (Wood constraint)

            x1 ≥ 0; x1 integer
            x2 ≥ 0; x2 integer

            For LP models, it can be proved that the optimal solution lies on an extreme point of
            the feasible region. In the case of integer linear programs, we only wish to consider points
            that are integer values. This is illustrated in Figure 1b for the Telfa problem. In contrast to
            linear programming, which can be solved eﬃciently in the worst case, integer programming
            problems are in many practical situations NP-hard (Cormen, Leiserson, & Rivest, 1992).

            406


            Global Inference for Sentence Compression

            Fortunately, ILPs are a well studied optimization problem and a number of techniques have
            been developed to ﬁnd the optimal solution. Two such techniques are the cutting planes
            method (Gomory, 1960) and the branch-and-bound method (Land & Doig, 1960). We
            brieﬂy discuss these methods here. For a more detailed treatment we refer the interested
            reader to Winston and Venkataramanan (2003) or Nemhauser and Wolsey (1988).

            The cutting planes method adds extra constraints to slice parts of the feasible region
            until it contains only integer extreme points. However, this process can be diﬃcult or
            impossible (Nemhauser & Wolsey, 1988). The branch-and-bound method enumerates all
            points in the ILP’s feasible region but prunes those sections in the region which are known
            to be sub-optimal. It does this by relaxing the integer constraints and solving the resulting
            LP problem (known as the LP relaxation). If the solution of the LP relaxation is integral,
            then it is the optimal solution. Otherwise, the resulting solution provides an upper bound
            on the solution for the ILP. The algorithm proceeds by creating two new sub-problems based
            on the non-integer solution for one variable at a time. These are solved and the process
            repeats until the optimal integer solution is found.

            Using the branch-and-bound method, we ﬁnd that the optimal solution to the Telfa
            problem is z = 40, x1 = 5, x2 = 0; thus, to achieve a maximum proﬁt of 40 GBP, Telfa
            must manufacture 5 tables and 0 chairs. This is a relatively simple problem, which could be
            solved merely by inspection. Most ILP problems will involve many variables and constraints
            resulting in a feasible region with a large number of integer points. The branch-and-bound
            procedure can eﬃciently solve such ILPs in a matter of seconds and forms part of many
            commercial ILP solvers. In our experiments we use lp solve 3, a free optimization package
            which relies on the simplex algorithm and brand-and-bound methods for solving ILPs.

            Note that under special circumstances other solving methods may be applicable. For
            example, implicit enumeration can be used to solve ILPs where all the variables are binary
            (also known as pure 0−1 problems).
            Implicit enumeration is similar to the branch-and-
            bound method, it systematically evaluates all possible solutions, without however explicitly
            solving a (potentially) large number of LPs derived from the relaxation. This removes
            much of the computational complexity involved in determining if a sub-problem is infea-
            sible. Furthermore, for a class of ILP problems known as minimum cost network ﬂow
            problems (MCNFP), the LP relaxation always yields an integral solution. These problems
            can therefore be treated as LP problems.

            In general, a model will yield an optimal solution in which all variables are integers if
            the constraint matrix has a property known as total unimodularity. A matrix A is totally
            unimodular if every square sub-matrix of A has its determinant equal to 0, +1 or −1.
            It is the case that the more the constraint matrix looks totally unimodular, the easier
            the problem will be to solve by branch-and-bound methods.
            In practice it is good to
            formulate ILPs where as many variables as possible have coeﬃcients of 0, +1 or −1 in the
            constraints (Winston & Venkataramanan, 2003).

            3.3 Constraints and Logical Conditions

            Although integer variables in ILP problems may take arbitrary values, these are frequently
            are restricted to 0 and 1. Binary variables (0−1 variables) are particularly useful for rep-

            3. The software is available from http://lpsolve.sourceforge.net/.

            407


            Clarke & Lapata

            Condition
            Implication
            Iﬀ
            Or
            Xor
            And
            Not

            Statement
            if a then b
            a if and only if b
            a or b or c
            a xor b xor c
            a and b
            not a

            Constraint
            b − a ≥ 0
            a − b = 0
            a + b + c ≥ 1
            a + b + c = 1
            a = 1; b = 1
            1 − a = 1

            Table 1: How to represent logical conditions using binary variables and constraints in ILP.

            resenting a variety of logical conditions within the ILP framework through the use of con-
            straints. Table 1 lists several logical conditions and their equivalent constraints.

            We can also express transitivity, i.e., “c if and only if a and b”. Although it is of-
            ten thought that transitivity can only be expressed as a polynomial expression of binary
            variables (i.e., ab = c), it is possible to replace the latter by the following linear inequali-
            ties (Williams, 1999):

            (1 − c) + a ≥ 1
            (1 − c) + b ≥ 1
            c + (1 − a) + (1 − b) ≥ 1

            This can be easily extended to model indicator variables representing whether a set of binary
            variables can take certain values.

            3.4 Compression Models

            In this section we describe three compression models which we reformulate as integer linear
            programs. Our ﬁrst model is a simple language model which has been used as a baseline in
            previous research (Knight & Marcu, 2002). Our second model is based on the work of Hori
            and Furui (2004); it combines a language model with a corpus-based signiﬁcance scoring
            function (we omit here the conﬁdence score derived from the speech recognizer since our
            models are applied to text only). This model requires a small amount of parallel data to
            learn weights for the language model and the signiﬁcance score.

            Our third model is fully supervised, it uses a discriminative large-margin framework
            (McDonald, 2006), and is trained trained on a larger parallel corpus. We chose this model
            instead of the more popular noisy-channel or decision-tree models, for two reasons, a practi-
            cal one and a theoretical one. First, McDonald’s (2006) model delivers performance superior
            to the decision-tree model (which in turn performs comparably to the noisy-channel). Sec-
            ond, the noisy channel is not an entirely appropriate model for sentence compression. It
            uses a language model trained on uncompressed sentences even though it represents the
            probability of compressed sentences. As a result, the model will consider compressed sen-
            tences less likely than uncompressed ones (a further discussion is provided by Turner &
            Charniak, 2005).

            408


            Global Inference for Sentence Compression

            3.4.1 Language Model

            A language model is perhaps the simplest model that springs to mind. It does not require
            a parallel corpus (although a relatively large monolingual corpus is necessary for training),
            and will naturally prefer short sentences to longer ones. Furthermore, a language model can
            be used to drop words that are either infrequent or unseen in the training corpus. Knight
            and Marcu (2002) use a bigram language model as a baseline against their noisy-channel
            and decision-tree models.

            Let x = x1, x2, . . . , xn denote a source sentence for which we wish to generate a target
            compression. We introduce a decision variable for each word in the source and constrain it
            to be binary; a value of 0 represents a word being dropped, whereas a value of 1 includes
            the word in the target compression. Let:

            δi =

            (

            1 if xi is in the compression
            0 otherwise

            ∀i ∈ [1 . . . n]

            If we were using a unigram language model, our objective function would maximize the
            overall sum of the decision variables (i.e., words) multiplied by their unigram probabilities
            (all probabilities throughout this paper are log-transformed):

            n

            max

            δi · P (xi)

            i=1
            X

            (1)

            Thus, if a word is selected, its corresponding δi is given a value of 1, and its probability
            P (xi) according to the language model will be counted in our total score.

            A unigram language model will probably generate many ungrammatical compressions.
            We therefore use a more context-aware model in our objective function, namely a trigram
            model. Dynamic programming would be typically used to decode a language model by
            traversing the sentence in a left-to-right manner. Such an algorithm is eﬃcient and provides
            all the context required for a conventional language model. However, it can be diﬃcult
            or impossible to incorporate global constraints into such a model as decisions on word
            inclusion cannot extend beyond a three word window. By formulating the decoding process
            for a trigram language model as an integer linear program we are able to take into account
            constraints that aﬀect the compressed sentence more globally. This process is a much more
            involved task than in the unigram case where there is no context, instead we must now
            make decisions based on word sequences rather than isolated words. We ﬁrst create some
            additional decision variables:

            αi =

            (

            1 if xi starts the compression
            0 otherwise

            ∀i ∈ [1 . . . n]

            βij = 
            

            
            γijk = 
            

            

            1 if sequence xi, xj ends

            the compression

            0 otherwise

            ∀i ∈ [0 . . . n − 1]
            ∀j ∈ [i + 1 . . . n]

            1 if sequence xi, xj, xk

            ∀i ∈ [0 . . . n − 2]

            is in the compression ∀j ∈ [i + 1 . . . n − 1]

            0 otherwise

            ∀k ∈ [j + 1 . . . n]

            409


            Clarke & Lapata

            Our objective function is given in Equation (2). This is the sum of all possible trigrams
            that can occur in all compressions of the source sentence where x0 represents the ‘start’
            token and xi is the ith word in sentence x. Equation (3) constrains the decision variables
            to be binary.

            n

            max z =

            αi · P (xi|start)

            i=1
            X

            n−2

            n−1

            n

            +

            +

            γijk · P (xk|xi, xj)

            i=1
            X
            n−1

            j=i+1
            X
            n

            Xk=j+1

            βij · P (end|xi, xj)

            subject to:

            i=0
            X

            j=i+1
            X

            δi, αi, βij, γijk = 0 or 1

            (2)

            (3)

            The objective function in (2) allows any combination of trigrams to be selected. This
            means that invalid trigram sequences (e.g., two or more trigrams containing the ‘end’ token)
            could appear in the target compression. We avoid this situation by introducing sequential
            constraints (on the decision variables δi, γijk, αi, and βij) that restrict the set of allowable
            trigram combinations.

            Constraint 1 Exactly one word can begin a sentence.

            αi = 1

            n

            i=1
            X

            (4)

            Constraint 2 If a word is included in the sentence it must either start the sentence or be
            preceded by two other words or one other word and the ‘start’ token x0.

            k−2

            k−1

            δk − αk −

            γijk = 0

            j=1
            X

            i=0
            X
            ∀k : k ∈ [1 . . . n]

            (5)

            Constraint 3 If a word is included in the sentence it must either be preceded by one
            word and followed by another or it must be preceded by one word and end the sentence.

            j−1

            n

            j−1

            δj −

            Xi=0

            Xk=j+1

            γijk −

            βij = 0

            Xi=0
            ∀j : j ∈ [1 . . . n]

            (6)

            Constraint 4 If a word is in the sentence it must be followed by two words or followed
            by one word and then the end of the sentence or it must be preceded by one word and end
            the sentence.

            n−1

            n

            n

            i−1

            δi −

            γijk −

            βij −

            βhi = 0

            Xj=i+1

            Xk=j+1

            Xj=i+1

            Xh=0

            ∀i : i ∈ [1 . . . n]

            (7)

            410


            Global Inference for Sentence Compression

            Constraint 5 Exactly one word pair can end the sentence.

            n−1

            n

            i=0
            X

            j=i+1
            X

            βij = 1

            (8)

            The sequential constraints described above ensure that the second order factorization (for
            trigrams) holds and are diﬀerent from our compression-speciﬁc constraints which are pre-
            sented in Section 3.5.

            Unless normalized by sentence length, a language model will naturally prefer one-word
            output. This normalization is however non-linear and cannot be incorporated into our ILP
            Instead, we impose a constraint on the length of the compressed sentence.
            formulation.
            Equation (9) below forces the compression to contain at least b tokens.

            δi ≥ b

            n

            i=1
            X

            (9)

            Alternatively, we could force the compression to be exactly b tokens (by substituting the
            inequality with an equality in (9)) or to be less than b tokens (by replacing ≥ with ≤).4
            The constraint in (9) is language model-speciﬁc and is not used elsewhere.

            3.4.2 Significance Model

            The language model just described has no notion of which content words to include in the
            compression and thus prefers words it has seen before. But words or constituents will be of
            diﬀerent relative importance in diﬀerent documents or even sentences.

            Inspired by Hori and Furui (2004), we add to our objective function (see Equation (2))
            In Hori and Furui’s
            a signiﬁcance score designed to highlight important content words.
            original formulation each word is weighted by a score similar to un-normalized tf ∗ idf . The
            signiﬁcance score is not applied indiscriminately to all words in a sentence but solely to
            topic-related words, namely nouns and verbs. Our score diﬀers in one respect. It combines
            document-level with sentence-level signiﬁcance. So in addition to tf ∗ idf , each word is
            weighted by its level of embedding in the syntactic tree.

            Intuitively, in a sentence with multiply nested clauses, more deeply embedded clauses
            tend to carry more semantic content. This is illustrated in Figure 2 which depicts the
            clause embedding for the sentence “Mr Field has said he will resign if he is not reselected,
            a move which could divide the party nationally”. Here, the most important information is
            conveyed by clauses S3 (he will resign) and S4 (if he is not reselected) which are embedded.
            Accordingly, we should give more weight to words found in these clauses than in the main
            clause (S1 in Figure 2). A simple way to enforce this is to give clauses weight proportional
            to the level of embedding. Our modiﬁed signiﬁcance score becomes:

            I(xi) =

            l
            N

            · fi log

            Fa
            Fi

            (10)

            where xi is a topic word, fi and Fi are the frequency of xi in the document and corpus
            respectively, Fa is the sum of all topic words in the corpus, l is the number of clause

            4. Compression rate can be also limited to a range by including two inequality constraints.

            411


            Clarke & Lapata

            S1

            S2
            Mr Field has said

            S3
            he will resign

            S4
            if he is not reselected

            , a move

            SBAR
            which could divide the party nationally

            Figure 2: The clause embedding of the sentence “Mr Field has said he will resign if he is
            not reselected, a move which could divide the party nationally”; nested boxes
            correspond to nested clauses.

            constituents above xi, and N is the deepest level of clause embedding. Fa and Fi are
            estimated from a large document collection, fi is document-speciﬁc, whereas l
            N is sentence-
            speciﬁc. So, in Figure 2 the term l
            N is 1.0 (4/4) for clause S4, 0.75 (3/4) for clause S3, and
            so on. Individual words inherit their weight from their clauses.

            The modiﬁed objective function with the signiﬁcance score is given below:

            n

            n

            max z =

            δi · λI(xi) +

            αi · P (xi|start)

            Xi=1

            n−2

            n−1

            n

            Xi=1

            +

            +

            γijk · P (xk|xi, xj)

            i=1
            X
            n−1

            j=i+1
            X
            n

            Xk=j+1

            βij · P (end|xi, xj)

            (11)

            i=0
            X

            j=i+1
            X

            We also add a weighting factor (λ) to the objective, in order to counterbalance the impor-
            tance of the language model and the signiﬁcance score. The weight is tuned on a small
            parallel corpus. The sequential constraints from Equations (4)–(8) are again used to ensure
            that the trigrams are combined in a valid way.

            3.4.3 Discriminative Model

            As a fully supervised model, we used the discriminative model presented by McDonald
            (2006). This model uses a large-margin learning framework coupled with a feature set
            deﬁned on compression bigrams and syntactic structure.

            Let x = x1, . . . , xn denote a source sentence with a target compression y = y1, . . . , ym
            where each yj occurs in x. The function L(yi) ∈ {1 . . . n} maps word yi in the target com-

            412


            Global Inference for Sentence Compression

            pression to the index of the word in the source sentence, x. We also include the constraint
            that L(yi)< L(yi+1) which forces each word in x to occur at most once in the compression
            y. Let the score of a compression y for a sentence x be:

            s(x, y)

            (12)

            This score is factored using a ﬁrst-order Markov assumption on the words in the target
            compression to give:

            |y|

            s(x, y) =

            s(x, L(yj−1), L(yj))

            (13)

            The score function is deﬁned to be the dot product between a high dimensional feature
            representation and a corresponding weight vector:

            j=2
            X

            |y|

            s(x, y) =

            w · f (x, L(yj−1), L(yj))

            (14)

            Xj=2

            Decoding in this model amounts to ﬁnding the combination of bigrams that maximizes
            the scoring function in (14). McDonald (2006) uses a dynamic programming approach
            where the maximum score is found in a left-to-right manner. The algorithm is an extension
            of Viterbi for the case in which scores factor over dynamic sub-strings (Sarawagi & Cohen,
            2004; McDonald, Crammer, & Pereira, 2005a). This allows back-pointers to be used to
            reconstruct the highest scoring compression as well as the k-best compressions.

            Again this is similar to the trigram language model decoding process (see Section 3.4.1),
            except that here a bigram model is used. Consequently, the ILP formulation is slightly
            simpler than that of the trigram language model. Let:

            δi =

            (

            1 if xi is in the compression
            0 otherwise

            (1 ≤ i ≤ n)

            We then introduce some more decision variables:

            αi =

            (

            1 if xi starts the compression
            0 otherwise

            ∀i ∈ [1 . . . n]

            βi =

            (

            1 if word xi ends the compression
            0 otherwise

            ∀i ∈ [1 . . . n]

            γij =

            (

            1 if sequence xi, xj is in the compression ∀i ∈ [1 . . . n − 1]
            ∀j ∈ [i + 1 . . . n]
            0 otherwise

            The discriminative model can be now expressed as:

            n

            max z =

            αi · s(x, 0, i)

            Xi=1

            n−1

            n

            +

            +

            γij · s(x, i, j)

            i=1
            X
            n

            j=i+1
            X

            βi · s(x, i, n + 1)

            i=1
            X

            413

            (15)


            Clarke & Lapata

            Constraint 1 Exactly one word can begin a sentence.

            αi = 1

            n

            i=1
            X

            (16)

            Constraint 2 If a word is included in the sentence it must either start the compression
            or follow another word.

            j

            δj − αj −

            γij = 0

            Xi=1
            ∀j : j ∈ [1 . . . n]

            (17)

            Constraint 3 If a word is included in the sentence it must be either followed by another
            word or end the sentence.

            n

            δi −

            γij − βi = 0

            j=i+1
            X

            ∀i : i ∈ [1 . . . n]

            Constraint 4 Exactly one word can end a sentence.

            βi = 1

            n

            i=1
            X

            (18)

            (19)

            Again, the sequential constraints in Equations (16)–(19) are necessary to ensure that the
            resulting combination of bigrams are valid.

            The current formulation provides a single optimal compression given the model. How-
            ever, McDonald’s (2006) dynamic programming algorithm is capable of returning the k-best
            compressions; this is useful for their learning algorithm described later. In order to produce
            k-best compressions, we must rerun the ILP with extra constraints which forbid previous
            solutions. In other words, we ﬁrst formulate the ILP as above, solve it, add its solution to
            the k-best list, and then create a set of constraints that forbid the conﬁguration of δi decision
            variables which form the current solution. The procedure is repeated until k compressions
            are found.

            The computation of the compression score crucially relies on the dot product between
            a high dimensional feature representation and a corresponding weight vector (see Equa-
            tion (14)). McDonald (2006) employs a rich feature set deﬁned over adjacent words and
            individual parts-of-speech, dropped words and phrases from the source sentence, and de-
            pendency structures (also of the source sentence). These features are designed to mimic the
            information presented in the previous noisy-channel and decision-tree models of Knight and
            Marcu (2002). Features over adjacent words are used as a proxy to the language model of
            the noisy channel. Unlike other models, which treat the parses as gold standard, McDonald
            uses the dependency information as another form of evidence. Faced with parses that are
            noisy the learning algorithm can reduce the weighting given to those features if they prove

            414


            Global Inference for Sentence Compression

            poor discriminators on the training data. Thus, the model should be much more robust
            and portable across diﬀerent domains and training corpora.

            The weight vector, w is learned using the Margin Infused Relaxed Algorithm (MIRA,
            Crammer & Singer, 2003) a discriminative large-margin online learning technique (McDon-
            ald, Crammer, & Pereira, 2005b). This algorithm learns by compressing each sentence and
            comparing the result with the gold standard. The weights are updated so that the score of
            the correct compression (the gold standard) is greater than the score of all other compres-
            sions by a margin proportional to their loss. The loss function is the number of words falsely
            retained or dropped in the incorrect compression relative to the gold standard. A source
            sentence will have exponentially many compressions and thus exponentially many margin
            constraints. To render learning computationally tractable, McDonald et al. (2005b) create
            constraints only on the k compressions that currently have the highest score, bestk(x; w).

            3.5 Constraints

            We are now ready to describe our compression-speciﬁc constraints. The models presented
            in the previous sections contain only sequential constraints and are thus equivalent to their
            original formulation. Our constraints are linguistically and semantically motivated in a
            similar fashion to the grammar checking component of Jing (2000). However, they do
            not rely on any additional knowledge sources (such as a grammar lexicon or WordNet)
            beyond the parse and grammatical relations of the source sentence. We obtain these from
            RASP (Briscoe & Carroll, 2002), a domain-independent, robust parsing system for English.
            However, any other parser with broadly similar output (e.g., Lin, 2001) could also serve our
            purposes. Our constraints revolve around modiﬁcation, argument structure, and discourse
            related factors.

            Modiﬁer Constraints Modiﬁer constraints ensure that relationships between head words
            and their modiﬁers remain grammatical in the compression:

            δi − δj ≥ 0
            ∀i, j : xj ∈ xi’s ncmods
            δi − δj ≥ 0
            ∀i, j : xj ∈ xi’s detmods

            (20)

            (21)

            Equation (20) guarantees that if we include a non-clausal modiﬁer5 (ncmod) in the compres-
            sion (such as an adjective or a noun) then the head of the modiﬁer must also be included;
            this is repeated for determiners (detmod) in (21). In Table 2 we illustrate how these con-
            straints disallow the deletion of certain words (starred sentences denote compressions that
            would not be possible given our constraints). For example, if the modiﬁer word Pasok from
            sentence (1a) is in the compression, then its head Party will also included (see (1b)).

            We also want to ensure that the meaning of the source sentence is preserved in the
            compression, particularly in the face of negation. Equation (22) implements this by forcing
            not in the compression when the head is included (see sentence (2b) in Table 2). A similar
            constraint is added for possessive modiﬁers (e.g., his, our), including genitives (e.g., John’s

            5. Clausal modiﬁers (cmod) are adjuncts modifying entire clauses. In the example “he ate the cake because

            he was hungry”, the because-clause is a modiﬁer of the sentence “he ate the cake”.

            415


            Clarke & Lapata

            1a.

            He became a power player in Greek Politics in 1974, when he founded the
            socialist Pasok Party.

            1b.

            *He became a power player in Greek Politics in 1974, when he founded the

            Pasok.

            2a. We took these troubled youth who don’t have fathers, and brought them into

            a room to Dads who don’t have their children.

            2b.

            *We took these troubled youth who do have fathers, and brought them into a

            room to Dads who do have their children.

            2c.

            *We took these troubled youth who don’t have fathers, and brought them into

            a room to Dads who don’t have children.
            The chain stretched from Uganda to Grenada and Nicaragua, since the 1970s.

            *Stretched from Uganda to Grenada and Nicaragua, since the 1970s.
            *The chain from Uganda to Grenada and Nicaragua, since the 1970s.
            *The chain stretched Uganda to Grenada and Nicaragua, since the 1970s.
            *The chain stretched from to Grenada and Nicaragua, since the 1970s.
            *The chain stretched from Uganda to Grenada Nicaragua, since the 1970s.

            3a.
            3b.
            3c.
            3d.
            3e.
            3f.

            Table 2: Examples of compressions disallowed by our set of constraints.

            gift), as shown in Equation (23). An example of the possessive constraint is given in
            sentence (2c) in Table 2.

            δi − δj = 0
            ∀i, j : xj ∈ xi’s ncmods ∧ xj = not
            δi − δj = 0
            ∀i, j : xj ∈ xi’s possessive mods

            (22)

            (23)

            Argument Structure Constraints We also deﬁne a few intuitive constraints that take
            the overall sentence structure into account. The ﬁrst constraint (Equation (24)) ensures
            that if a verb is present in the compression then so are its arguments, and if any of the
            arguments are included in the compression then the verb must also be included. We thus
            force the program to make the same decision on the verb, its subject, and object (see
            sentence (3b) in Table 2).

            δi − δj = 0
            ∀i, j : xj ∈ subject/object of verb xi

            (24)

            Our second constraint forces the compression to contain at least one verb provided the
            source sentence contains one as well:

            δi ≥ 1

            Xi:xi∈verbs

            (25)

            The constraint entails that it is not possible to drop the main verb stretched from sen-
            tence (3a) (see also sentence (3c) in Table 2).

            416


            Global Inference for Sentence Compression

            Other sentential constraints include Equations (26) and (27) which apply to prepo-
            sitional phrases and subordinate clauses. These constraints force the introducing term
            (i.e., the preposition, or subordinator) to be included in the compression if any word from
            within the syntactic constituent is also included. By subordinator we mean wh-words
            (e.g., who, which, how, where), the word that, and subordinating conjunctions (e.g., after,
            although, because). The reverse is also true, i.e., if the introducing term is included, at
            least one other word from the syntactic constituent should also be included.

            δi − δj ≥ 0
            ∀i, j : xj ∈ PP/SUB
            ∧xi starts PP/SUB
            δi − δj ≥ 0

            Xi:xi∈PP/SUB
            ∀j : xj starts PP/SUB

            (26)

            (27)

            As an example consider sentence (3d) from Table 2. Here, we cannot drop the preposition
            from if Uganda is in the compression. Conversely, we must include from if Uganda is in the
            compression (see sentence (3e)).

            We also wish to handle coordination.

            If two head words are conjoined in the source
            sentence, then if they are included in the compression the coordinating conjunction must
            also be included:

            (1 − δi) + δj ≥ 1
            (1 − δi) + δk ≥ 1
            δi + (1 − δj) + (1 − δk) ≥ 1
            ∀i, j, k : xj ∧ xk conjoined by xi

            (28)

            (29)

            (30)

            Consider sentence (3f) from Table 2.
            compression, then we must include the conjunction and.

            If both Uganda and Nicaragua are present in the

            Finally, Equation (31) disallows anything within brackets in the source sentence from
            being included in the compression. This is a somewhat superﬁcial attempt at excluding
            parenthetical and potentially unimportant material from the compression.

            δi = 0
            ∀i : xi ∈ bracketed words (inc parentheses)

            (31)

            Discourse Constraints Our discourse constraint concerns personal pronouns. Specif-
            ically, Equation (32) forces personal pronouns to be included in the compression. The
            constraint is admittedly more important for generating coherent documents (as opposed to
            individual sentences). It nevertheless has some impact on sentence-level compressions, in
            particular when verbal arguments are missed by the parser. When these are pronominal,
            constraint (32) will result in more grammatical output since some of the argument structure
            of the source sentence will be preserved in the compression.

            δi = 1
            ∀i : xi ∈ personal pronouns

            417

            (32)


            Clarke & Lapata

            We should note that some of the constraints described above would be captured by
            models that learn synchronous deletion rules from a corpus. For example, the noisy-channel
            model of Knight and Marcu (2002) learns not to drop the head when the latter is modiﬁed
            by an adjective or a noun, since the transformations DT NN → DT or AJD NN → ADJ are
            almost never seen in the data. Similarly, the coordination constraint (Equations (28)–(30))
            would be enforced using Turner and Charniak’s (2005) special rules — they enhance their
            parallel grammar with rules modeling more structurally complicated deletions than those
            attested in their corpus. In designing our constraints we aimed at capturing appropriate
            deletions for many possible models, including those that do not rely on a training corpus
            or do not have an explicit notion of a parallel grammar (e.g., McDonald, 2006). The
            modiﬁcation constraints would presumably be redundant for the noisy-channel model, which
            could otherwise beneﬁt from more specialized constraints, e.g., targeting sparse rules or
            noisy parse trees, however we leave this to future work.

            Another feature of the modeling framework presented here is that deletions (or non-
            deletions) are treated as unconditional decisions. For example, we require not to drop the
            noun in adjective-noun sequences if the adjective is not deleted as well. We also require to
            always include a verb in the compression if the source sentence has one. These hardwired de-
            cisions could in some cases prevent valid compressions from being considered. For instance,
            it is not possible to compress the sentence “this is not appropriate behavior” to “this is
            not appropriate” or“Bob loves Mary and John loves Susan” to “Bob loves Mary and John
            Susan”. Admittedly we lose some expressive power, yet we ensure that the compressions
            will be broadly grammatically, even for unsupervised or semi-supervised models. Further-
            more, in practice we ﬁnd that our models consistently outperform non-constraint-based
            alternatives, without extensive constraint engineering.

            3.6 Solving the ILP

            As we mentioned earlier (Section 3.1), solving ILPs is NP-hard.
            In cases where the co-
            eﬃcient matrix is unimodular, it can be shown that the optimal solution to the linear
            program is integral. Although the coeﬃcient matrix in our problems is not unimodular, we
            obtained integral solutions for all sentences we experimented with (approximately 3,000,
            see Section 4.1 for details). We conjecture that this is due to the fact that all of our vari-
            ables have 0, +1 or −1 coeﬃcients in the constraints and therefore our constraint matrix
            shares many properties of a unimodular matrix. We generate and solve an ILP for every
            sentence we wish to compress. Solve times are less than a second per sentence (including
            input-output overheads) for all models presented here.

            4. Experimental Set-up

            Our evaluation experiments were motivated by three questions: (1) Do the constraint-
            based compression models deliver performance gains over non-constraint-based ones? We
            expect better compressions for the model variants which incorporate compression-speciﬁc
            constraints. (2) Are there diﬀerences among constraint-based models? Here, we would like
            to investigate how much modeling power is gained by the addition of the constraints. For
            example, it may be the case that a state-of-the-art model like McDonald’s (2006) does not
            beneﬁt much from the addition of constraints. And that their eﬀect is much bigger for less

            418


            Global Inference for Sentence Compression

            sophisticated models. (3) How do the models reported in this paper port across domains?
            In particular, we are interested in assessing whether the models and proposed constraints
            are general and robust enough to produce good compressions for both written and spoken
            texts.

            We next describe the data sets on which our models were trained and tested (Section 4.1),
            explain how model parameters were estimated (Section 4.2) and present our evaluation setup
            (Section 4.3). We discuss our results in Section 5.

            4.1 Corpora

            Our intent was to assess the performance of the models just described on written and spoken
            text. The appeal of written text is understandable since most summarization work today
            focuses on this domain. Speech data not only provides a natural test-bed for compression
            applications (e.g., subtitle generation) but also poses additional challenges. Spoken utter-
            ances can be ungrammatical, incomplete, and often contain artefacts such as false starts,
            interjections, hesitations, and disﬂuencies. Rather than focusing on spontaneous speech
            which is abundant in these artefacts, we conduct our study on the less ambitious domain
            of broadcast news transcripts. This lies in-between the extremes of written text and spon-
            taneous speech as it has been scripted beforehand and is usually read oﬀ on autocue.

            Previous work on sentence compression has almost exclusively used the Ziﬀ-Davis corpus
            for training and testing purposes. This corpus originates from a collection of news articles
            on computer products. It was created automatically by matching sentences that occur in
            an article with sentences that occur in an abstract (Knight & Marcu, 2002). The abstract
            sentences had to contain a subset of the source sentence’s words and the word order had
            In earlier work (Clarke & Lapata, 2006) we have argued that the
            to remain the same.
            Ziﬀ-Davis corpus is not ideal for studying compression for several reasons. First, we showed
            that human-authored compressions diﬀer substantially from the Ziﬀ-Davis which tends to
            be more aggressively compressed. Second, humans are more likely to drop individual words
            than lengthy constituents. Third, the test portion of the Ziﬀ-Davis contains solely 32 sen-
            tences. This is an extremely small data set to reveal any statistically signiﬁcant diﬀerences
            among systems. In fact, previous studies relied almost exclusively on human judgments for
            assessing the well-formedness of the compressed output, and signiﬁcance tests are reported
            for by-subjects analyses only.

            We thus focused in the present study on manually created corpora. Speciﬁcally, we
            asked annotators to perform sentence compression by removing tokens on a sentence-by-
            sentence basis. Annotators were free to remove any words they deemed superﬂuous provided
            their deletions: (a) preserved the most important information in the source sentence, and
            (b) ensured the compressed sentence remained grammatical. If they wished, they could leave
            a sentence uncompressed by marking it as inappropriate for compression. They were not
            allowed to delete whole sentences even if they believed they contained no information content
            with respect to the story as this would blur the task with abstracting. Following these
            guidelines, our annotators produced compressions of 82 newspaper articles (1,433 sentences)
            from the British National Corpus (BNC) and the American News Text corpus (henceforth
            written corpus) and 50 stories (1,370 sentences) from the HUB-4 1996 English Broadcast
            News corpus (henceforth spoken corpus). The written corpus contains articles from The LA

            419


            Clarke & Lapata

            Times, Washington Post, Independent, The Guardian and Daily Telegraph. The spoken
            corpus contains broadcast news from a variety of networks (CNN, ABC, CSPAN and NPR)
            which have been manually transcribed and segmented at the story and sentence level. Both
            corpora have been split into training, development and testing sets6 randomly on article
            boundaries (with each set containing full stories) and are publicly available from http:
            //homepages.inf.ed.ac.uk/s0460084/data/.

            4.2 Parameter Estimation

            In this work we present three compression models ranging from unsupervised to semi-
            supervised, and fully supervised. The unsupervised model simply relies on a trigram lan-
            guage model for driving compression (see Section 3.4.1). This was estimated from 25 mil-
            lion tokens of the North American corpus using the CMU-Cambridge Language Modeling
            Toolkit (Clarkson & Rosenfeld, 1997) with a vocabulary size of 50,000 tokens and Good-
            Turing discounting. To discourage one-word output we force the ILP to generate compres-
            sions whose length is no less than 40% of the source sentence (see the constraint in (9)).
            The semi-supervised model is the weighted combination of a word-based signiﬁcance score
            with a language model (see Section 3.4.2). The signiﬁcance score was calculated using
            25 million tokens from the American News Text corpus. We optimized its weight (see
            Equation (11)) on a small subset of the training data (three documents in each case) us-
            ing Powell’s method (Press, Teukolsky, Vetterling, & Flannery, 1992) and a loss function
            based on the F-score of the grammatical relations found in the gold standard compression
            and the system’s best compression (see Section 4.3 for details). The optimal weight was
            approximately 1.8 for the written corpus and 2.2 for the spoken corpus.

            McDonald’s (2006) supervised model was trained on the written and spoken training
            sets. Our implementation used the same feature sets as McDonald, the only diﬀerence
            being that our phrase structure and dependency features were extracted from the output of
            Roark’s (2001) parser. McDonald uses Charniak’s (2000) parser which performs comparably.
            The model was learnt using k-best compressions. On the development data, we found that
            k = 10 provided the best performance.

            4.3 Evaluation

            Previous studies have relied almost exclusively on human judgments for assessing the well-
            formedness of automatically derived compressions. These are typically rated by naive sub-
            jects on two dimensions, grammaticality and importance (Knight & Marcu, 2002). Although
            automatic evaluation measures have been proposed (Riezler et al., 2003; Bangalore, Ram-
            bow, & Whittaker, 2000) their use is less widespread, we suspect due to the small size of
            the test portion of the Ziﬀ-Davis corpus which is commonly used in compression work.

            We evaluate the output of our models in two ways. First, we present results using
            an automatic evaluation measure put forward by Riezler et al. (2003). They compare
            the grammatical relations found in the system compressions against those found in a gold
            standard. This allows us to measure the semantic aspects of summarization quality in terms
            of grammatical-functional information and can be quantiﬁed using F-score. Furthermore,

            6. The splits are 908/63/462 sentences for the written corpus and 882/78/410 sentences for the spoken

            corpus.

            420


            Global Inference for Sentence Compression

            in Clarke and Lapata (2006) we show that relations-based F-score correlates reliably with
            human judgments on compression output. Since our test corpora are larger than Ziﬀ-
            Davis (by more than a factor of ten), diﬀerences among systems can be highlighted using
            signiﬁcance testing.

            Our implementation of the F-score measure used the grammatical relations annotations
            provided by RASP (Briscoe & Carroll, 2002). This parser is particularly appropriate for the
            compression task since it provides parses for both full sentences and sentence fragments and
            is generally robust enough to analyze semi-grammatical sentences. We calculated F-score
            over all the relations provided by RASP (e.g., subject, direct/indirect object, modiﬁer; 15
            in total).

            In line with previous work we also evaluate our models by eliciting human judgments.
            Following the work of Knight and Marcu (2002), we conducted two separate experiments.
            In the ﬁrst experiment participants were presented with a source sentence and its target
            compression and asked to rate how well the compression preserved the most important
            information from the source sentence. In the second experiment, they were asked to rate
            the grammaticality of the compressed outputs. In both cases they used a ﬁve point rating
            scale where a high number indicates better performance. We randomly selected 21 sentences
            from the test portion of each corpus. These sentences were compressed automatically by
            the three models presented in this paper with and without constraints. We also included
            gold standard compressions. Our materials thus consisted of 294 (21 × 2 × 7) source-
            target sentences. A Latin square design ensured that subjects did not see two diﬀerent
            compressions of the same sentence. We collected ratings from 42 unpaid volunteers, all self
            reported native English speakers. Both studies were conducted over the Internet using a
            custom build web interface. Examples of our experimental items are given in Table 3.

            5. Results

            Let us ﬁrst discuss our results when compression output is evaluated in terms of F-score.
            Tables 4 and 5 illustrate the performance of our models on the written and spoken corpora,
            respectively. We also present the compression rate7 for each system.
            In all cases the
            constraint-based models (+Constr) yield better F-scores than the non-constrained ones.
            The diﬀerence is starker for the semi-supervised model (Sig). The constraints bring an
            improvement of 17.2% on the written corpus and 18.3% on the spoken corpus. We further
            examined whether performance diﬀerences among models are statistically signiﬁcant, using
            the Wilcoxon test. On the written corpus all constraint models signiﬁcantly outperform the
            models without constraints. The same tendency is observed on the spoken corpus except for
            the model of McDonald (2006) which performs comparably with and without constraints.
            We also wanted to establish which is the best constraint model. On both corpora we
            ﬁnd that the language model performs worst, whereas the signiﬁcance model and McDonald
            perform comparably (i.e., the F-score diﬀerences are not statistically signiﬁcant). To get
            a feeling for the diﬃculty of the task, we calculated how much our annotators agreed in
            their compression output. The inter-annotator agreement (F-score) on the written corpus
            was 65.8% and on the spoken corpus 73.4%. The agreement is higher on spoken texts since
            they consists of many short utterances (e.g., Okay, That’s it for now, Good night) that can

            7. The term refers to the percentage of words retained from the source sentence in the compression.

            421


            Clarke & Lapata

            Source

            The aim is to give councils some control over the future growth of second
            homes.
            The aim is to give councils control over the growth of homes.
            The aim is to the future.

            Gold
            LM
            LM+Constr The aim is to give councils control.
            Sig
            The aim is to give councils control over the future growth of homes.
            Sig+Constr The aim is to give councils control over the future growth of homes.
            McD
            McD+Constr The aim is to give councils some control over the growth of homes.
            Source

            The aim is to give councils.

            The Clinton administration recently unveiled a new means to encourage
            brownﬁelds redevelopment in the form of a tax incentive proposal.
            The Clinton administration unveiled a new means to encourage brown-
            ﬁelds redevelopment in a tax incentive proposal.
            The Clinton administration in the form of tax.

            LM
            LM+Constr The Clinton administration unveiled a means to encourage redevelop-

            Gold

            Sig

            ment in the form.
            The Clinton administration unveiled a encourage brownﬁelds redevelop-
            ment form tax proposal.

            Sig+Constr The Clinton administration unveiled a means to encourage brownﬁelds

            McD

            redevelopment in the form of tax proposal.
            The Clinton unveiled a means to encourage brownﬁelds redevelopment
            in a tax incentive proposal.

            McD+Constr The Clinton administration unveiled a means to encourage brownﬁelds

            redevelopment in the form of a incentive proposal.

            Table 3: Example compressions produced by our systems (Source: source sentence, Gold:
            gold-standard compression, LM: language model compression, LM+Constr: lan-
            guage model compression with constraints, Sig: signiﬁcance model, Sig+Constr:
            signiﬁcance model with constraints, McD: McDonald’s (2006) compression model,
            McD+Constr: McDonald’s (2006) compression model with constraints).

            be compressed only very little or not all. Note that there is a marked diﬀerence between the
            automatic and human compressions. Our best performing systems are inferior to human
            output by more than 20 F-score percentage points.

            Diﬀerences between the automatic systems and the human output are also observed
            with respect to the compression rate. As can be seen the language model compresses most
            aggressively, whereas the signiﬁcance model and McDonald tend to be more conservative
            and closer to the gold standard. Interestingly, the constraints do not necessarily increase
            the compression rate. The latter increases for the signiﬁcance model but decreases for
            the language model and remains relatively constant for McDonald. It is straightforward to
            impose the same compression rate for all constraint-based models (e.g., by forcing the model
            n
            i=1 δi = b). However, we refrained from doing this since we wanted our
            to retain b tokens

            P

            422


            Global Inference for Sentence Compression

            Models
            LM
            Sig
            McD
            LM+Constr
            Sig+Constr
            McD+Constr
            Gold

            CompR F-score
            18.4
            23.3
            36.0
            28.2∗
            40.5∗†
            40.8∗†
            —

            46.2
            60.6
            60.1
            41.2
            72.0
            63.7
            70.3

            Table 4: Results on the written corpus; compression rate (CompR) and grammatical re-
            lation F-score (F-score); ∗: +Constr model is signiﬁcantly diﬀerent from model
            without constraints; †: signiﬁcantly diﬀerent from LM+Constr.

            Models
            LM
            Sig
            McD
            LM+Constr
            Sig+Constr
            McD+Constr
            Gold

            CompR F-score
            25.4
            30.4
            47.6
            34.8∗
            48.7∗†
            50.1†
            —

            52.0
            60.9
            68.6
            49.5
            78.4
            68.5
            76.1

            Table 5: Results on the spoken corpus; compression rate (CompR) and grammatical rela-
            tion F-score (F-score); ∗: +Constr model is signiﬁcantly diﬀerent from without
            constraints; †: signiﬁcantly diﬀerent from LM+Constr.

            models to regulate the compression rate for each sentence individually according to its
            speciﬁc information content and structure.

            We next consider the results of our human study which assesses in more detail the quality
            of the generated compressions on two dimensions, namely grammaticality and information
            content. F-score conﬂates these two dimensions and therefore in theory could unduly reward
            a system that produces perfectly grammatical output without any information loss. Tables 6
            and 7 show the mean ratings8 for each system (and the gold standard) on the written and
            spoken corpora, respectively. We ﬁrst performed an Analysis of Variance (Anova) to
            examine the eﬀect of diﬀerent system compressions. The Anova revealed a reliable eﬀect
            on both grammaticality and importance for each corpus (the eﬀect was signiﬁcant by both
            subjects and items (p
            <
            0.01)).

            We next examine the impact of the constraints (+Constr in the tables). In most cases
            we observe an increase in ratings for both grammaticality and importance when a model
            is supplemented constraints. Post-hoc Tukey tests reveal that the grammaticality and
            importance ratings of the language model and signiﬁcance model signiﬁcantly improve with

            8. All statistical tests reported subsequently were done using the mean ratings.

            423


            Clarke & Lapata

            Models

            LM
            Sig
            McD

            LM+Constr
            Sig+Constr
            McD+Constr
            Gold

            Grammar
            2.25†$
            2.26†$
            3.05†
            3.47∗†
            3.76∗
            3.50†
            4.25

            Importance
            1.82†$
            2.99†$
            2.84†
            2.37∗†$
            3.53∗
            3.17†
            3.98

            Table 6: Results on the written text corpus; average grammaticality score (Grammar) and
            average importance score (Importance) for human judgments; ∗: +Constr model
            is signiﬁcantly diﬀerent from model without constraints; †: signiﬁcantly diﬀerent
            from gold standard; $: signiﬁcantly diﬀerent from McD+Constr.

            Models

            LM
            Sig
            McD

            LM+Constr
            Sig+Constr
            McD+Constr
            Gold

            Grammar
            2.20†$
            2.29†$
            3.33†
            3.18∗†
            3.80∗†
            3.60†
            4.45

            Importance
            1.56†
            2.64†
            3.32†
            2.49∗†$
            3.69∗†
            3.31†
            4.25

            Table 7: Results on the spoken text corpus; average grammaticality score (Grammar) and
            average importance score (Importance) for human judgments; ∗: +Constr model
            is signiﬁcantly diﬀerent from model without constraints; †: signiﬁcantly diﬀerent
            from gold standard; $: signiﬁcantly diﬀerent from McD+Constr.

            the constraints (α
            <
            0.01). In contrast, McDonald’s system sees a numerical improvement
            with the additional constraints, but this diﬀerence is not statistically signiﬁcant. These
            tendencies are observed on the spoken and written corpus.

            Upon closer inspection, we can see that the constraints inﬂuence considerably the
            grammaticality of the unsupervised and semi-supervised systems. Tukey tests reveal that
            LM+Constr and Sig+Constr are as grammatical as McD+Constr. In terms of importance,
            Sig+Constr and McD+Constr are signiﬁcantly better than LM+Constr (α
            <
            0.01). This
            is not surprising given that LM+Constr is a very simple model without a mechanism for
            highlighting important words in a sentence.
            Interestingly, Sig+Constr performs as well
            as McD+Constr in retaining the most important words, despite the fact that it requires
            minimal supervision. Although constraint-based models overall perform better than mod-
            els without constraints, they receive lower ratings (for grammaticality and importance) in
            comparison to the gold standard. And the diﬀerences are signiﬁcant in most cases.

            424


            Global Inference for Sentence Compression

            In summary, we observe that the constraints boost performance. This is more pro-
            nounced for compression models that are either unsupervised or use small amounts of
            parallel data. For example, a simple model like Sig yields performance comparable to
            McDonald (2006) when constraints are taken into account. This is an encouraging result
            suggesting that ILP can be used to create good compression models with relatively little
            eﬀort (i.e., without extensive feature engineering or elaborate knowledge sources). Per-
            formance gains are also obtained for competitive models like McDonald’s that are fully
            supervised. But these gains are smaller, presumably because the initial model contains a
            rich feature representation consisting of syntactic information and generally does a good job
            at producing grammatical output. Finally, our improvements are consistent across corpora
            and evaluation paradigms.
        </corps>
        <conclusion>6. Conclusions

            In this paper we have presented a novel method for automatic sentence compression. A key
            aspect of our approach is the use of integer linear programming for inferring globally optimal
            compressions in the presence of linguistically motivated constraints. We have shown how
            previous formulations of sentence compression can be recast as ILPs and extended these
            models with local and global constraints ensuring that the compressed output is structurally
            and semantic well-formed. Contrary to previous work that has employed ILP solely for
            decoding, our models integrate learning with inference in a uniﬁed framework.

            Our experiments have demonstrated the advantages of the approach. Constraint-based
            models consistently bring performance gains over models without constraints. These im-
            provements are more impressive for models that require little or no supervision. A case
            in point here is the signiﬁcance model discussed above. The no-constraints incarnation of
            this model performs poorly and considerably worse than McDonald’s (2006) state-of-the-art
            model. The addition of constraints improves the output of this model so that its perfor-
            mance is indistinguishable from McDonald. Note that the signiﬁcance model requires a
            small amount of training data (50 parallel sentences), whereas McDonald is trained on hun-
            dreds of sentences. It also presupposes little feature engineering, whereas McDonald utilizes
            thousands of features. Some eﬀort is associated with framing the constraints, however these
            are created once and are applied across models and corpora. We have also observed small
            performance gains for McDonald’s system when the latter is supplemented with constraints.
            Larger improvements are possible with more sophisticated constraints, however our intent
            was to devise a set of general constraints that are not tuned to the mistakes of any speciﬁc
            system in particular.

            Future improvements are many and varied. An obvious extension concerns our con-
            straint set. Currently our constraints are mostly syntactic and consider each sentence in
            isolation. By incorporating discourse constraints we could highlight words that are impor-
            tant at the document-level. Presumably words topical in a document should be retained in
            the compression. Other constraints could manipulate the compression rate. For example,
            we could encourage a higher compression rate for longer sentences. Another interesting
            direction includes the development of better objective functions for the compression task.
            The objective functions presented so far rely on ﬁrst or second-order Markov assumptions.
            Alternative objectives could take into account the structural similarity between the source

            425


            Clarke & Lapata

            sentence and its target compression; or whether they share the same content which could
            be operationalized in terms of entropy.

            Beyond the task and systems presented in this paper, we believe the approach holds
            promise for other generation applications using decoding algorithms for searching the space
            of possible outcomes. Examples include sentence-level paraphrasing, headline generation,
            and summarization.

            Acknowledgments

            We are grateful to our annotators Vasilis Karaiskos, Beata Kouchnir, and Sarah Luger.
            Thanks to Jean Carletta, Frank Keller, Steve Renals, and Sebastian Riedel for helpful
            comments and suggestions and to the anonymous referees whose feedback helped to sub-
            stantially improve the present paper. Lapata acknowledges the support of EPSRC (grant
            GR/T04540/01). A preliminary version of this work was published in the proceedings of
            ACL 2006.
        </conclusion>
        <discussion>Aucune discussion trouvée.</discussion>
        <biblio>Aho, A. V., & Ullman, J. D. (1969). Syntax directed translations and the pushdown assem-

            bler. Journal of Computer and System Sciences, 3, 37–56.

            Bangalore, S., Rambow, O., & Whittaker, S. (2000). Evaluation metrics for generation.
            In Proceedings of the ﬁrst International Conference on Natural Language Generation,
            pp. 1–8, Mitzpe Ramon, Israel.

            Barzilay, R., & Lapata, M. (2006). Aggregation via set partitioning for natural language
            In Proceedings of the Human Language Technology Conference of the
            generation.
            North American Chapter of the Association for Computational Linguistics, pp. 359–
            366, New York, NY, USA.

            Bramsen, P., Deshpande, P., Lee, Y. K., & Barzilay, R. (2006). Inducing temporal graphs.
            In Proceedings of the 2006 Conference on Empirical Methods in Natural Language
            Processing, pp. 189–198, Sydney, Australia.

            Briscoe, E. J., & Carroll, J. (2002). Robust accurate statistical annotation of general text. In
            Proceedings of the Third International Conference on Language Resources and Eval-
            uation, pp. 1499–1504, Las Palmas, Gran Canaria.

            Charniak, E. (2000). A maximum-entropy-inspired parser. In Proceedings of the 1st North
            American Annual Meeting of the Association for Computational Linguistics, pp. 132–
            139, Seattle, WA, USA.

            Clarke, J., & Lapata, M. (2006). Models for sentence compression: A comparison across
            domains, training requirements and evaluation measures. In Proceedings of the 21st
            International Conference on Computational Linguistics and 44th Annual Meeting of
            the Association for Computational Linguistics, pp. 377–384, Sydney, Australia.

            Clarkson, P., & Rosenfeld, R. (1997). Statistical language modeling using the CMU–
            Cambridge toolkit. In Proceedings of Eurospeech’97, pp. 2707–2710, Rhodes, Greece.

            426


            Global Inference for Sentence Compression

            Cormen, T. H., Leiserson, C. E., & Rivest, R. L. (1992). Intoduction to Algorithms. The

            MIT Press.

            Corston-Oliver, S. (2001). Text Compaction for Display on Very Small Screens. In Proceed-
            ings of the Workshop on Automatic Summarization at the 2nd Meeting of the North
            American Chapter of the Association for Computational Linguistics, pp. 89–98, Pitts-
            burgh, PA, USA.

            Crammer, K., & Singer, Y. (2003). Ultraconservative online algorithms for multiclass prob-

            lems. Journal of Machine Learning Research, 3, 951–991.

            Dantzig, G. B. (1963). Linear Programming and Extensions. Princeton University Press,

            Princeton, NJ, USA.

            Denis, P., & Baldridge, J. (2007). Joint determination of anaphoricity and coreference
            resolution using integer programming. In Human Language Technologies 2007: The
            Conference of the North American Chapter of the Association for Computational Lin-
            guistics; Proceedings of the Main Conference, pp. 236–243, Rochester, NY.

            Dras, M. (1999). Tree Adjoining Grammar and the Reluctant Paraphrasing of Text. Ph.D.

            thesis, Macquarie University.

            Galley, M., & McKeown, K. (2007). Lexicalized markov grammars for sentence compression.
            In In Proceedings of the North American Chapter of the Association for Computational
            Linguistics, pp. 180–187, Rochester, NY, USA.

            Gomory, R. E. (1960). Solving linear programming problems in integers.

            In Bellman,
            R., & Hall, M. (Eds.), Combinatorial analysis, Proceedings of Symposia in Applied
            Mathematics, Vol. 10, Providence, RI, USA.

            Grefenstette, G. (1998). Producing Intelligent Telegraphic Text Reduction to Provide an
            Audio Scanning Service for the Blind. In Hovy, E., & Radev, D. R. (Eds.), Proceedings
            of the AAAI Symposium on Intelligent Text Summarization, pp. 111–117, Stanford,
            CA, USA.

            Hori, C., & Furui, S. (2004). Speech summarization: an approach through word extraction
            and a method for evaluation. IEICE Transactions on Information and Systems, E87-
            D (1), 15–25.

            Jing, H. (2000). Sentence reduction for automatic text summarization. In Proceedings of
            the 6th Applied Natural Language Processing Conference, pp. 310–315, Seattle,WA,
            USA.

            Knight, K., & Marcu, D. (2002). Summarization beyond sentence extraction: a probabilistic

            approach to sentence compression. Artiﬁcial Intelligence, 139 (1), 91–107.

            Land, A. H., & Doig, A. G. (1960). An automatic method for solving discrete programming

            problems. Econometrica, 28, 497–520.

            Lin, C.-Y. (2003). Improving summarization performance by sentence compression — a pilot
            study. In Proceedings of the 6th International Workshop on Information Retrieval with
            Asian Languages, pp. 1–8, Sapporo, Japan.

            Lin, D. (2001). LaTaT: Language and text analysis tools. In Proceedings of the ﬁrst Human

            Language Technology Conference, pp. 222–227, San Francisco, CA, USA.

            427


            Clarke & Lapata

            Marciniak, T., & Strube, M. (2005). Beyond the pipeline: Discrete optimization in NLP. In
            Proceedings of the Ninth Conference on Computational Natural Language Learning,
            pp. 136–143, Ann Arbor, MI, USA.

            McDonald, R. (2006). Discriminative sentence compression with soft syntactic constraints.
            In Proceedings of the 11th Conference of the European Chapter of the Association for
            Computational Linguistics, Trento, Italy.

            McDonald, R., Crammer, K., & Pereira, F. (2005a). Flexible text segmentation with struc-
            tured multilabel classiﬁcation. In Proceedings of Human Language Technology Con-
            ference and Conference on Empirical Methods in Natural Language Processing, pp.
            987–994, Vancouver, BC, Canada.

            McDonald, R., Crammer, K., & Pereira, F. (2005b). Online large-margin training of de-
            In 43rd Annual Meeting of the Association for Computational

            pendency parsers.
            Linguistics, pp. 91–98, Ann Arbor, MI, USA.

            Nemhauser, G. L., & Wolsey, L. A. (1988). Integer and Combinatorial Optimization. Wiley-
            Interscience series in discrete mathematicals and opitmization. Wiley, New York, NY,
            USA.

            Nguyen, M. L., Shimazu, A., Horiguchi, S., Ho, T. B., & Fukushi, M. (2004). Probabilistic
            sentence reduction using support vector machines. In Proceedings of the 20th inter-
            national conference on Computational Linguistics, pp. 743–749, Geneva, Switzerland.

            Press, W. H., Teukolsky, S. A., Vetterling, W. T., & Flannery, B. P. (1992). Numerical
            Recipes in C: The Art of Scientiﬁc Computing. Cambridge University Press, New
            York, NY, USA.

            Punyakanok, V., Roth, D., Yih, W., & Zimak, D. (2004). Semantic role labeling via inte-
            ger linear programming inference. In Proceedings of the International Conference on
            Computational Linguistics, pp. 1346–1352, Geneva, Switzerland.

            Riedel, S., & Clarke, J. (2006). Incremental integer linear programming for non-projective
            dependency parsing. In Proceedings of the 2006 Conference on Empirical Methods in
            Natural Language Processing, pp. 129–137, Sydney, Australia.

            Riezler, S., King, T. H., Crouch, R., & Zaenen, A. (2003). Statistical sentence condensation
            using ambiguity packing and stochastic disambiguation methods for lexical-functional
            In Human Language Technology Conference and the 3rd Meeting of the
            grammar.
            North American Chapter of the Association for Computational Linguistics, pp. 118–
            125, Edmonton, Canada.

            Roark, B. (2001). Probabilistic top-down parsing and language modeling. Computational

            Linguistics, 27 (2), 249–276.

            Roth, D. (1998). Learning to resolve natural language ambiguities: A uniﬁed approach. In
            In Proceedings of the 15th of the American Association for Artiﬁcial Intelligence, pp.
            806–813, Madison, WI, USA.

            Roth, D., & Yih, W. (2004). A linear programming formulation for global inference in
            natural language tasks. In Proceedings of the Annual Conference on Computational
            Natural Language Learning, pp. 1–8, Boston, MA, USA.

            428


            Global Inference for Sentence Compression

            Roth, D., & Yih, W. (2005). Integer linear programming inference for conditional random
            ﬁelds. In Proceedings of the International Conference on Machine Learning, pp. 737–
            744, Bonn.

            Sarawagi, S., & Cohen, W. W. (2004). Semi-markov conditional random ﬁelds for informa-
            tion extraction. In Advances in Neural Information Processing Systems, Vancouver,
            BC, Canada.

            Shieber, S., & Schabes, Y. (1990). Synchronous tree-adjoining grammars.

            In Proceed-
            ings of the 13th International Conference on Computational Linguistics, pp. 253–258,
            Helsinki, Finland.

            Turner, J., & Charniak, E. (2005). Supervised and unsupervised learning for sentence
            compression. In Proceedings of the 43rd Annual Meeting of the Association for Com-
            putational Linguistics, pp. 290–297, Ann Arbor, MI, USA.

            Vandeghinste, V., & Pan, Y. (2004). Sentence compression for automated subtitling: A
            hybrid approach. In Marie-Francine Moens, S. S. (Ed.), Text Summarization Branches
            Out: Proceedings of the ACL-04 Workshop, pp. 89–95, Barcelona, Spain.

            Williams, H. P. (1999). Model Building in Mathematical Programming (4th edition). Wiley.

            Winston, W. L., & Venkataramanan, M. (2003). Introduction to Mathematical Program-

            ming: Applications and Algorithms (4th edition). Duxbury.

            Zajic, D., Door, B. J., Lin, J., & Schwartz, R. (2007). Multi-candidate reduction: Sentence
            Information Processing

            compression as a tool for document summarization tasks.
            Management Special Issue on Summarization, 43 (6), 1549–1570.

            429
        </biblio>
    </article>
    <article>
        <preamble>probabilistic_sentence_reduction.txt</preamble>
        <titre>Probabilistic Sentence Reduction Using Support Vector Machines</titre>
        <auteur>Minh Le Nguyen, Akira Shimazu, Susumu Horiguchi</auteur>
        <abstract>This paper investigates a novel application of sup- port vector machines (SVMs) for sentence
            reduction. We also propose a new probabilistic sentence reduc- tion method based on support vector machine
            learn- ing. Experimental results show that the proposed methods outperform earlier methods in term of sen-
            tence reduction performance.
        </abstract>
        <introduction>The most popular methods of sentence reduc- tion for text summarization are corpus based methods.
            Jing (Jing 00) developed a method to remove extraneous phrases from sentences by using multiple sources of
            knowledge to de- cide which phrases could be removed. However, while this method exploits a simple model for
            sentence reduction by using statistics computed from a corpus, a better model can be obtained by using a
            learning approach. Knight and Marcu (Knight and Marcu 02) proposed a corpus based sentence reduction method
            using machine learning techniques. They discussed a noisy-channel based approach and a decision tree based
            approach to sentence reduction. Their algorithms provide the best way to scale up the full problem of
            sentence re- duction using available data. However, these al- gorithms require that the word order of a
            given sentence and its reduced sentence are the same. Nguyen and Horiguchi (Nguyen and Horiguchi 03)
            presented a new sentence reduction tech- nique based on a decision tree model without that constraint. They
            also indicated that se- mantic information is useful for sentence reduc- tion tasks. The major drawback of
            previous works on sentence reduction is that those methods are likely to output local optimal results, which
            may have lower accuracy. This problem is caused by the inherent sentence reduction model; that is, only a
            single reduced sentence can be obtained. As pointed out by Lin (Lin 03), the best sen- tence reduction
            output for a single sentence is not approximately best for text summarization. This means that “local
            optimal” refers to the best reduced output for a single sentence, while the best reduced output for the
            whole text is “global optimal”. Thus, it would be very valu- able if the sentence reduction task could
            gener- ate multiple reduced outputs and select the best one using the whole text document. However, such a
            sentence reduction method has not yet been proposed. Support Vector Machines (Vapnik 95), on the other hand,
            are strong learning methods in com- parison with decision tree learning and other learning methods (Sholkopf
            97). The goal of this paper is to illustrate the potential of SVMs for enhancing the accuracy of sentence
            reduc- tion in comparison with previous work. Accord- ingly, we describe a novel deterministic method for
            sentence reduction using SVMs and a two- stage method using pairwise coupling (Hastie 98). To solve the
            problem of generating mul- tiple best outputs, we propose a probabilistic sentence reduction model, in which
            a variant of probabilistic SVMs using a two-stage method with pairwise coupling is discussed. The rest of
            this paper will be organized as follows: Section 2 introduces the Support Vec- tor Machines learning.
            Section 3 describes the previous work on sentence reduction and our deterministic sentence reduction using
            SVMs. We also discuss remaining problems of deter- ministic sentence reduction. Section 4 presents a
            probabilistic sentence reduction method using support vector machines to solve this problem. Section 5
            discusses implementation and our ex- perimental results; Section 6 gives our conclu- sions and describes
            some problems that remain to be solved in the future.
        </introduction>
        <corps>2 Support Vector Machine
            Support vector machine (SVM)(Vapnik 95) is a
            technique of machine learning based on statisti-
            cal learning theory. Suppose that we are given
            l training examples (xi, yi), (1 ≤ i ≤ l), where
            xi is a feature vector in n dimensional feature
            space, yi is the class label {-1, +1 } of xi. SVM
            ﬁnds a hyperplane w.x + b = 0 which correctly
            separates the training examples and has a max-
            imum margin which is the distance between two
            hyperplanes w.x + b ≥ 1 and w.x + b ≤ −1. The
            optimal hyperplane with maximum margin can
            be obtained by solving the following quadratic
            programming.

            l(cid:80)

            1
            2 (cid:107)w(cid:107) + C0
            yi(w.xi + b) ≥ 1 − ξi

            ξi

            i

            min

            s.t.
            ξi ≥ 0

            (1)

            where C0 is the constant and ξi is a slack vari-
            able for the non-separable case.
            In SVM, the
            optimal hyperplane is formulated as follows:

            f (x) = sign

            (cid:195)

            l(cid:88)

            1

            (cid:33)

            αiyiK(xi, x) + b

            (2)

            where αi

            is the Lagrange multiple, and
            K(x(cid:48), x(cid:48)(cid:48)) is a kernel function, the SVM calcu-
            lates similarity between two arguments x(cid:48) and
            x(cid:48)(cid:48). For instance, the Polynomial kernel func-
            tion is formulated as follow:

            K(x(cid:48), x(cid:48)(cid:48)) = (x(cid:48).x(cid:48)(cid:48))p

            (3)

            SVMs estimate the label of an unknown ex-
            ample x whether the sign of f (x) is positive or
            not.

            3 Deterministic Sentence Reduction

            Using SVMs

            3.1 Problem Description
            In the corpus-based decision tree approach, a
            given input sentence is parsed into a syntax tree
            and the syntax tree is then transformed into a
            small tree to obtain a reduced sentence.

            Let t and s be syntax trees of the original sen-
            tence and a reduced sentence, respectively. The
            process of transforming syntax tree t to small
            tree s is called “rewriting process” (Knight and
            Marcu 02), (Nguyen and Horiguchi 03). To
            transform the syntax tree t to the syntax tree
            s, some terms and ﬁve rewriting actions are de-
            ﬁned.

            An Input list consists of a sequence of words
            subsumed by the tree t where each word in the
            Input list is labelled with the name of all syntac-
            tic constituents in t. Let CSTACK be a stack

            that consists of sub trees in order to rewrite a
            small tree. Let RSTACK be a stack that con-
            sists of sub trees which are removed from the
            Input list in the rewriting process.

            • SHIFT action transfers the ﬁrst word from the
            Input list into CSTACK. It is written mathe-
            matically and given the label SHIFT.

            • REDUCE(lk, X) action pops the lk syntactic
            trees located at the top of CSTACK and com-
            bines them in a new tree, where lk is an integer
            and X is a grammar symbol.

            • DROP X action moves subsequences of words
            that correspond to syntactic constituents from
            the Input list to RSTACK.

            • ASSIGN TYPE X action changes the label of
            trees at the top of the CSTACK. These POS
            tags might be diﬀerent from the POS tags in
            the original sentence.

            • RESTORE X action takes the X element in
            RSTACK and moves it into the Input list,
            where X is a subtree.

            For convenience, let conﬁguration be a status
            of Input list, CSTACK and RSTACK. Let cur-
            rent context be the important information in a
            conﬁguration. The important information are
            deﬁned as a vector of features using heuristic
            methods as in (Knight and Marcu 02), (Nguyen
            and Horiguchi 03).

            The main idea behind deterministic sentence
            reduction is that it uses a rule in the current
            context of the initial conﬁguration to select a
            distinct action in order to rewrite an input sen-
            tence into a reduced sentence. After that, the
            current context is changed to a new context and
            the rewriting process is repeated for selecting
            an action that corresponds to the new context.
            The rewriting process is ﬁnished when it meets
            a termination condition. Here, one rule corre-
            sponds to the function that maps the current
            context to a rewriting action. These rules are
            learned automatically from the corpus of long
            sentences and their reduced sentences (Knight
            and Marcu 02), (Nguyen and Horiguchi 03).

            3.2 Example
            Figure 1 shows an example of applying a se-
            quence of actions to rewrite the input sentence
            (a, b, c, d, e), when each character is a word. It
            illustrates the structure of the Input list, two
            stacks, and the term of a rewriting process based
            on the actions mentioned above. For example,
            in the ﬁrst row, DROP H deletes the sub-tree
            with its root node H in the Input list and stores


            it in the RSTACK. The reduced tree s can be
            obtained after applying a sequence of actions
            as follows: DROP H; SHIFT; ASSIGN TYPE K;
            DROP B; SHIFT; ASSIGN TYPE H; REDUCE 2
            F; RESTORE H; SHIFT; ASSIGN TYPE D; RE-
            DUCE 2G. In this example, the reduced sentence
            is (b, e, a).

            Figure 2: Example of Conﬁguration

            that start with the ﬁrst unit in the Input list.
            For example,
            in Figure 2 the syntactic con-
            stituents are labels of the current element in the
            Input list from “VP” to the verb “convince”.
            Semantic features
            The following features are used in our model as
            semantic information.

            • Semantic information about current words
            within the Input list; these semantic types
            are obtained by using the named entities such
            as Location, Person, Organization and Time
            within the input sentence. To deﬁne these
            name entities, we use the method described in
            (Borthwick 99).

            • Semantic information about whether or not the

            word in the Input list is a head word.

            • Word relations, such as whether or not a word
            has a relationship with other words in the sub-
            categorization table. These relations and the
            sub-categorization table are obtained using the
            Commlex database (Macleod 95).

            Using the semantic information, we are able to
            avoid deleting important segments within the
            given input sentence. For instance, the main
            verb, the subject and the object are essential
            and for the noun phrase, the head noun is essen-
            tial, but an adjective modiﬁer of the head noun
            is not. For example, let us consider that the
            verb “convince” was extracted from the Com-
            lex database as follows.

            convince
            NP-PP: PVAL (“of”)
            NP-TO-INF-OC

            This entry indicates that the verb “convince”

            Figure 1: An Example of the Rewriting Process

            3.3 Learning Reduction Rules Using

            SVMs

            As mentioned above, the action for each conﬁg-
            uration can be decided by using a learning rule,
            which maps a context to an action. To obtain
            such rules, the conﬁguration is represented by
            a vector of features with a high dimension. Af-
            ter that, we estimate the training examples by
            using several support vector machines to deal
            with the multiple classiﬁcation problem in sen-
            tence reduction.

            3.3.1 Features
            One important task in applying SVMs to text
            summarization is to deﬁne features. Here, we
            describe features used in our sentence reduction
            models.

            The features are extracted based on the cur-
            rent context. As it can be seen in Figure 2, a
            context includes the status of the Input list and
            the status of CSTACK and RSTACK. We de-
            ﬁne a set of features for a current context as
            described bellow.
            Operation feature
            The set of features as described in (Nguyen and
            Horiguchi 03) are used in our sentence reduction
            models.
            Original tree features
            These features denote the syntactic constituents


            can be followed by a noun phrase and a preposi-
            tional phrase starting with the preposition “of”.
            It can be also followed by a noun phrase and a
            to-inﬁnite phrase. This information shows that
            we cannot delete an “of” prepositional phrase
            or a to-inﬁnitive that is the part of the verb
            phrase.

            3.3.2 Two-stage SVM Learning using

            Pairwise Coupling

            Using these features we can extract training
            data for SVMs. Here, a sample in our training
            data consists of pairs of a feature vector and
            an action. The algorithm to extract training
            data from the training corpus is modiﬁed using
            the algorithm described in our pervious work
            (Nguyen and Horiguchi 03).

            Since the original support vector machine
            (SVM) is a binary classiﬁcation method, while
            the sentence reduction problem is formulated as
            multiple classiﬁcation, we have to ﬁnd a method
            to adapt support vector machines to this prob-
            lem. For multi-class SVMs, one can use strate-
            gies such as one-vs all, pairwise comparison or
            DAG graph (Hsu 02). In this paper, we use the
            pairwise strategy, which constructs a rule for
            discriminating pairs of classes and then selects
            the class with the most winning among two class
            decisions.

            To boost the training time and the sentence
            reduction performance, we propose a two-stage
            SVM described below.

            Suppose that the examples in training data
            are divided into ﬁve groups m1, m2, ..., m5 ac-
            cording to their actions. Let Svmc be multi-
            class SVMs and let Svmc-i be multi-class SVMs
            for a group mi. We use one Svmc classiﬁer to
            identify the group to which a given context e
            should be belong. Assume that e belongs to
            the group mi. The classiﬁer Svmc-i is then used
            to recognize a speciﬁc action for the context e.
            The ﬁve classiﬁers Svmc-1, Svmc-2,..., Svmc-5
            are trained by using those examples which have
            actions belonging to SHIFT, REDUCE, DROP,
            ASSIGN TYPE and RESTORE.

            Table 1 shows the distribution of examples in

            ﬁve data groups.

            3.4 Disadvantage of Deterministic

            Sentence Reductions

            The idea of the deterministic algorithm is to
            use the rule for each current context to select
            the next action, and so on. The process termi-
            nates when a stop condition is met. If the early
            steps of this algorithm fail to select the best ac-

            Table 1: Distribution of example data on ﬁve
            data groups
            Name
            SHIFT-GROUP
            REDUCE-GROUP
            DROP-GROUP
            ASSIGN-GROUP
            RESTORE-GROUP
            TOTAL

            Number of examples
            13,363
            11,406
            4,216
            13,363
            2,004
            44,352

            tions, then the possibility of obtaining a wrong
            reduced output becomes high.

            One way to solve this problem is to select mul-
            tiple actions that correspond to the context at
            each step in the rewriting process. However,
            the question that emerges here is how to deter-
            mine which criteria to use in selecting multiple
            actions for a context.
            If this problem can be
            solved, then multiple best reduced outputs can
            be obtained for each input sentence and the best
            one will be selected by using the whole text doc-
            ument.

            In the next section propose a model for se-
            lecting multiple actions for a context in sentence
            reduction as a probabilistic sentence reduction
            and present a variant of probabilistic sentence
            reduction.

            4 Probabilistic Sentence Reduction

            Using SVM

            a

            set

            of k

            4.1 The Probabilistic SVM Models
            Let A be
            actions A =
            {a1, a2...ai, ..., ak} and C be a set of n con-
            texts C = {c1, c2...ci, ..., cn} . A probabilistic
            model α for sentence reduction will select an
            action a ∈ A for the context c with probability
            pα(a|c). The pα(a|c) can be used to score ac-
            tion a among possible actions A depending the
            context c that is available at the time of deci-
            sion. There are several methods for estimating
            such scores; we have called these “probabilistic
            sentence reduction methods”. The conditional
            probability pα(a|c) is estimated using a variant
            of probabilistic support vector machine, which
            is described in the following sections.
            4.1.1 Probabilistic SVMs using
            Pairwise Coupling

            For convenience, we denote uij = p(a = ai|a =
            ai ∨aj, c). Given a context c and an action a, we
            assume that the estimated pairwise class prob-
            abilities rij of uij are available. Here rij can
            be estimated by some binary classiﬁers. For
            instance, we could estimate rij by using the


            SVM binary posterior probabilities as described
            in (Plat 2000). Then, the goal is to estimate
            {pi}k
            i=1 , where pi = p(a = ai|c), i = 1, 2, ..., k.
            For this propose, a simple estimate of these
            probabilities can be derived using the following
            voting method:

            pi = 2

            (cid:88)

            j:j(cid:54)=i

            I{rij >rji}/k(k − 1)

            where I is an indicator function and k(k − 1) is
            the number of pairwise classes. However, this
            model is too simple; we can obtain a better one
            with the following method.

            Assume that uij are pairwise probabilities of
            the model subject to the condition that uij =
            pi/(pi+pj). In (Hastie 98), the authors proposed
            to minimize the Kullback-Leibler (KL) distance
            between the rij and uij
            (cid:88)

            l(p) =

            nijrijlog

            (4)

            rij
            uij

            i(cid:54)=j

            where rij and uij are the probabilities of a pair-
            wise ai and aj in the estimated model and in
            our model, respectively, and nij is the number
            of training data in which their classes are ai or
            aj. To ﬁnd the minimizer of equation (6), they
            ﬁrst calculate

            ∂l(p)
            ∂pi

            =

            (cid:88)

            i(cid:54)=j

            nij(−

            rij
            pi

            +

            1
            pi + pj

            ).

            Thus, letting ∆l(p) = 0, they proposed to ﬁnd
            a point satisfying

            (cid:88)

            nijuij =

            (cid:88)

            nijrij,

            j:j(cid:54)=i

            j:j(cid:54)=i

            k(cid:80)

            i=1

            pi = 1,

            where i = 1, 2, ...k and pi > 0.
            Such a point can be obtained by using an algo-
            rithm described elsewhere in (Hastie 98). We
            applied it to obtain a probabilistic SVM model
            for sentence reduction using a simple method as
            follows. Assume that our class labels belong to
            l groups: M = {m1, m2...mi, ..., ml} , where l
            is a number of groups and mi is a group e.g.,
            SHIFT, REDUCE ,..., ASSIGN TYPE. Then
            the probability p(a|c) of an action a for a given
            context c can be estimated as follows.

            p(a|c) = p(mi|c) × p(a|c, mi)

            (5)

            where mi is a group and a ∈ mi. Here, p(mi|c)
            and p(a|c, mi) are estimated by the method in
            (Hastie 98).

            4.2 Probabilistic sentence reduction

            algorithm

            After obtaining a probabilistic model p, we then
            use this model to deﬁne function score, by which

            the search procedure ranks the derivation of in-
            complete and complete reduced sentences. Let
            d(s) = {a1, a2, ...ad} be the derivation of a small
            tree s, where each action ai belongs to a set of
            possible actions. The score of s is the product
            of the conditional probabilities of the individual
            actions in its derivation.

            (cid:89)

            Score(s) =

            p(ai|ci)

            (6)

            ai∈d(s)

            where ci is the context in which ai was decided.
            The search heuristic tries to ﬁnd the best re-
            duced tree s∗ as follows:

            s∗ = argmax
            (cid:124) (cid:123)(cid:122) (cid:125)
            s∈tree(t)

            Score(s)

            (7)

            where tree(t) are all the complete reduced trees
            from the tree t of the given long sentence. As-
            sume that for each conﬁguration the actions
            {a1, a2, ...an} are sorted in decreasing order ac-
            cording to p(ai|ci), in which ci is the context
            of that conﬁguration. Algorithm 1 shows a
            probabilistic sentence reduction using the top
            K-BFS search algorithm. This algorithm uses
            a breadth-ﬁrst search which does not expand
            the entire frontier, but instead expands at most
            the top K scoring incomplete conﬁgurations in
            the frontier; it is terminated when it ﬁnds M
            completed reduced sentences (CL is a list of re-
            duced trees), or when all hypotheses have been
            exhausted. A conﬁguration is completed if and
            only if the Input list is empty and there is one
            tree in the CSTACK. Note that the function
            get-context(hi, j) obtains the current context of
            the jth conﬁguration in hi, where hi is a heap at
            step i. The function Insert(s,h) ensures that the
            heap h is sorted according to the score of each
            element in h. Essentially, in implementation we
            can use a dictionary of contexts and actions ob-
            served from the training data in order to reduce
            the number of actions to explore for a current
            context.
        </corps>
        <conclusion>6 Conclusions
            We have presented a new probabilistic sentence
            reduction approach that enables a long sentence
            to be rewritten into reduced sentences based on
            support vector models. Our methods achieves
            better performance when compared with earlier
            methods. The proposed reduction approach can
            generate multiple best outputs. Experimental
            results showed that the top 10 reduced sentences
            returned by the reduction process might yield
            accuracies higher than previous work. We be-
            lieve that a good ranking method might improve
            the sentence reduction performance further in a
            text.
        </conclusion>
        <discussion>5 Experiments and Discussion
            We used the same corpus as described in
            (Knight and Marcu 02), which includes 1,067
            pairs of sentences and their reductions. To
            evaluate sentence reduction algorithms, we ran-
            domly selected 32 pairs of sentences from our
            parallel corpus, which is refered to as the test
            corpus. The training corpus of 1,035 sentences
            extracted 44,352 examples, in which each train-
            ing example corresponds to an action. The
            SVM tool, LibSVM (Chang 01) is applied to
            train our model. The training examples were


            Algorithm 1 A probabilistic sentence reduction
            algorithm

            1: CL={Empty};

            i = 0; h0={ Initial conﬁguration}

            2: while |CL|
            < M do
              if hi is empty then
            3:
            4:
            5:
            6:
            7:
            8:

            end if
            u =min(|hi|, K)
            for j = 1 to u do

            break;

            c=get-context(hi, j)
            m(cid:80)

            9:

            10:
            11:
            12:

            Select m so that

            p(ai|c)< Q is maximal

                     for l=1 to m do

            i=1

            parameter=get-parameter(al);
            Obtain a new conﬁguration s by performing action al
            with parameter
            if Complete(s) then

            Insert(s, CL)

            Insert(s, hi+1)

            else

            13:
            14:
            15:
            16:
            17:
            18:
            19:
            20:
            21: end while

            end for
            i = i + 1

            end if
            end for

            divided into SHIFT, REDUCE, DROP, RE-
            STORE, and ASSIGN groups. To train our
            support vector model in each group, we used
            the pairwise method with the polynomial ker-
            nel function, in which the parameter p in (3)
            and the constant C0 in equation (1) are 2 and
            0.0001, respectively.

            The algorithms (Knight and Marcu 02) and
            (Nguyen and Horiguchi 03) served as the base-
            line1 and the baseline2 for comparison with the
            proposed algorithms. Deterministic sentence re-
            duction using SVM and probabilistic sentence
            reduction were named as SVM-D and SVMP, re-
            spectively. For convenience, the ten top reduced
            outputs using SVMP were called SVMP-10. We
            used the same evaluation method as described
            in (Knight and Marcu 02) to compare the pro-
            posed methods with previous methods. For this
            experiment, we presented each original sentence
            in the test corpus to three judges who are spe-
            cialists in English, together with three sentence
            reductions: the human generated reduction sen-
            tence, the outputs of the proposed algorithms,
            and the output of the baseline algorithms.

            The judges were told that all outputs were
            generated automatically. The order of the out-
            puts was scrambled randomly across test cases.
            The judges participated in two experiments. In
            the ﬁrst, they were asked to determine on a scale
            from 1 to 10 how well the systems did with re-
            spect to selecting the most important words in
            the original sentence. In the second, they were
            asked to determine the grammatical criteria of

            reduced sentences.

            Table 2 shows the results of English language
            sentence reduction using a support vector ma-
            chine compared with the baseline methods and
            with human reduction. Table 2 shows compres-
            sion rates, and mean and standard deviation re-
            sults across all judges, for each algorithm. The
            results show that the length of the reduced sen-
            tences using decision trees is shorter than using
            SVMs, and indicate that our new methods out-
            perform the baseline algorithms in grammatical
            and importance criteria. Table 2 shows that the

            Table 2: Experiment results with Test Corpus

            METHOD
            Baseline1
            Baseline2
            SVM-D
            SVMP-10
            Human

            Comp
            Gramma
            57.19% 8.60 ± 2.8
            57.15% 8.60 ± 2.1
            57.65% 8.76 ± 1.2
            57.51% 8.80 ± 1.3
            64.00% 9.05 ± 0.3

            Impo
            7.18 ± 1.92
            7.42 ± 1.90
            7.53 ± 1.53
            7.74 ± 1.39
            8.50 ± 0.80

            ﬁrst 10 reduced sentences produced by SVMP-
            10 (the SVM probabilistic model) obtained the
            highest performances. We also compared the
            computation time of sentence reduction using
            support vector machine with that in previous
            works. Table 3 shows that the computational
            times for SVM-D and SVMP-10 are slower than
            baseline, but it is acceptable for SVM-D.

            Table 3: Computational times of performing re-
            ductions on test-set. Average sentence length
            was 21 words.
            METHOD
            Baseline1
            SVM-D
            SVMP-10

            Computational times (sec)
            138.25
            212.46
            1030.25

            We also investigated how sensitive the pro-
            posed algorithms are with respect to the train-
            ing data by carrying out the same experi-
            ment on sentences of diﬀerent genres. We
            created the test corpus by selecting sentences
            from the web-site of the Benton Foundation
            (http://www.benton.org). The leading sen-
            tences in each news article were selected as the
            most relevant sentences to the summary of the
            news. We obtained 32 leading long sentences
            and 32 headlines for each item. The 32 sen-
            tences are used as a second test for our methods.
            We use a simple ranking criterion: the more the
            words in the reduced sentence overlap with the
            words in the headline, the more important the


            sentence is. A sentence satisfying this criterion
            is called a relevant candidate.

            For a given sentence, we used a simple
            method, namely SVMP-R to obtain a re-
            duced sentence by selecting a relevant candi-
            date among the ten top reduced outputs using
            SVMP-10.

            Table 4 depicts the experiment results for
            the baseline methods, SVM-D, SVMP-R, and
            SVMP-10. The results shows that, when ap-
            plied to sentence of a diﬀerent genre, the per-
            formance of SVMP-10 degrades smoothly, while
            the performance of the deterministic sentence
            reductions (the baselines and SVM determinis-
            tic) drops sharply. This indicates that the prob-
            abilistic sentence reduction using support vector
            machine is more stable.
            that

            the performance of
            SVMP-10 is also close to the human reduction
            outputs and is better than previous works. In
            addition, SVMP-R outperforms the determin-
            istic sentence reduction algorithms and the dif-
            ferences between SVMP-R’s results and SVMP-
            10 are small. This indicates that we can ob-
            tain reduced sentences which are relevant to the
            headline, while ensuring the grammatical and
            the importance criteria compared to the origi-
            nal sentences.

            Table 4 shows

            Table 4: Experiment results with Benton Cor-
            pus

            Comp
            Gramma
            METHOD
            54.14% 7.61 ± 2.10
            Baseline1
            53.13% 7.72 ± 1.60
            Baseline2
            SVM-D
            56.64% 7.86 ± 1.20
            SVMP-R 58.31% 8.25 ± 1.30
            57.62% 8.60 ± 1.32
            SVMP-10
            64.00% 9.01 ± 0.25
            Human

            Impo
            6.74 ± 1.92
            7.02 ± 1.90
            7.23 ± 1.53
            7.54 ± 1.39
            7.71 ± 1.41
            8.40 ± 0.60
        </discussion>
        <biblio>A. Borthwick, “A Maximum Entropy Approach
            to Named Entity Recognition”, Ph.D the-
            sis, Computer Science Department, New York
            University (1999).

            C.-C. Chang

            and C.-J.

            Lin,
            support

            “LIB-
            vec-
            at

            a

            library

            available

            for
            Software

            SVM:
            tor machines”,
            http://www.csie.ntu.edu.tw/ cjlin/libsvm.
            H. Jing, “Sentence reduction for automatic
            text summarization”, In Proceedings of the
            First Annual Meeting of the North Ameri-
            can Chapter of the Association for Compu-
            tational Linguistics NAACL-2000.

            T.T. Hastie and R. Tibshirani, “Classiﬁcation
            by pairwise coupling”, The Annals of Statis-
            tics, 26(1): pp. 451-471, 1998.

            C.-W. Hsu and C.-J. Lin, “A comparison of
            methods for multi-class support vector ma-
            chines”, IEEE Transactions on Neural Net-
            works, 13, pp. 415-425, 2002.

            K. Knight and D. Marcu, “Summarization be-
            yond sentence extraction: A Probabilistic ap-
            proach to sentence compression”, Artiﬁcial
            Intelligence 139: pp. 91-107, 2002.

            C.Y. Lin, “Improving Summarization Perfor-
            mance by Sentence Compression — A Pi-
            lot Study”, Proceedings of the Sixth Inter-
            national Workshop on Information Retrieval
            with Asian Languages, pp.1-8, 2003.

            C. Macleod and R. Grishman, “COMMLEX
            syntax Reference Manual”; Proteus Project,
            New York University (1995).

            M.L. Nguyen and S. Horiguchi, “A new sentence
            reduction based on Decision tree model”,
            Proceedings of 17th Paciﬁc Asia Conference
            on Language, Information and Computation,
            pp. 290-297, 2003

            V. Vapnik, “The Natural of Statistical Learning
            Theory”, New York: Springer-Verlag, 1995.
            J. Platt,“ Probabilistic outputs for support vec-
            tor machines and comparison to regularized
            likelihood methods,” in Advances in Large
            Margin Classiﬁers, Cambridege, MA: MIT
            Press, 2000.

            B. Scholkopf et al, “Comparing Support Vec-
            tor Machines with Gausian Kernels to Radius
            Basis Function Classifers”, IEEE Trans. Sig-
            nal Procesing, 45, pp. 2758-2765, 1997.
        </biblio>
    </article>
    <article>
        <preamble>Torres.txt</preamble>
        <titre>Summary Evaluation</titre>
        <auteur>with and without References Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and
            Patricia Vel´azquez-Morales
        </auteur>
        <abstract>evaluation a new content-based method for systems without the human models which is used to produce
            system rankings. The research is carried out using a new content-based evaluation framework called FRESA to
            compute a variety of divergences among probability distributions. We apply our comparison framework to
            various well-established content-based evaluation measures in text summarization such as COVERAGE, their
            RESPONSIVENESS, PYRAMIDS associations including text generic multi-document summarization in English and
            French, summarization in English and focus-based multi-document generic single-document summarization in
            French and Spanish. summarization tasks and ROUGE in various studying Index Terms—Text summarization
            evaluation, content-based evaluation measures, divergences.
        </abstract>
        <introduction>issue complex and controversial T EXT summarization evaluation has always been a in computational
            linguistics. In the last decade, signiﬁcant advances have been made in this ﬁeld as well as various
            evaluation measures have been designed. Two evaluation campaigns have been led by the U.S. agence DARPA. The
            ﬁrst one, SUMMAC, ran from 1996 to 1998 under the auspices of the Tipster program [1], and the second one,
            entitled DUC (Document Understanding Conference) [2], was the main evaluation forum from 2000 until 2007.
            Nowadays, the Text Analysis Conference (TAC) [3] provides a forum for assessment of different information
            access technologies including text summarization. Evaluation in text summarization can be extrinsic or
            intrinsic [4]. In an extrinsic evaluation, the summaries are assessed in the context of an speciﬁc task
            carried out by a human or a machine. In an intrinsic evaluation, the summaries are evaluated in reference to
            some ideal model. SUMMAC was mainly extrinsic while DUC and TAC followed an intrinsic evaluation paradigm.
            In an intrinsic evaluation, an Manuscript received June 8, 2010. Manuscript accepted for publication July
            25, 2010. Juan-Manuel and Torres-Moreno ´Ecole France (juan-manuel.torres@univ-avignon.fr). Polytechnique is
            with LIA/Universit´e de Montr´eal, d’Avignon, Canada Eric with SanJuan (eric.sanjuan@univ-avignon.fr). is
            LIA/Universit´e d’Avignon, France Horacio Saggion is with DTIC/Universitat Pompeu Fabra, Spain
            (horacio.saggion@upf.edu). Iria da Cunha is with IULA/Universitat Pompeu Fabra, Spain; LIA/Universit´e
            d’Avignon, France and Instituto de Ingenier´ıa/UNAM, Mexico (iria.dacunha@upf.edu). Patricia
            Vel´azquez-Morales (patricia velazquez@yahoo.com). is with VM Labs, France automatically generated summary
            (peer) has to be compared with one or more reference summaries (models). DUC used an interface called SEE to
            allow human judges to compare a peer with a model. Thus, judges give a COVERAGE score to each peer produced
            by a system and the ﬁnal system COVERAGE score is the average of the COVERAGE’s scores asigned. These
            system’s COVERAGE scores can then be used to rank summarization systems. In the case of query-focused
            summarization (e.g. when the summary should answer a question or series of questions) a RESPONSIVENESS score
            is also assigned to each summary, which indicates how responsive the summary is to the question(s). Because
            manual comparison of peer summaries with model summaries is an arduous and costly process, a body of
            research has been produced in the last decade on automatic content-based evaluation procedures. Early
            studies used text similarity measures such as cosine similarity (with or without weighting schema) to
            compare peer and model summaries [5]. Various vocabulary overlap measures such as n-grams overlap or longest
            common subsequence between peer and model have also been proposed [6], [7]. The BLEU machine translation
            evaluation measure [8] has also been tested in summarization [9]. The DUC conferences adopted the ROUGE
            package for content-based evaluation [10]. ROUGE implements a series of recall measures based on n-gram
            co-occurrence between a peer summary and a set of model summaries. These measures are used to produce
            systems’ rank. It has been shown that system rankings, produced by some ROUGE measures (e.g., ROUGE-2, which
            uses 2-grams), have a correlation with rankings produced using COVERAGE. In recent years the PYRAMIDS
            evaluation method [11] has been introduced. It is based on the distribution of “content” of a set of model
            summaries. Summary Content Units (SCUs) are ﬁrst identiﬁed in the model summaries, then each SCU receives a
            weight which is the number of models containing or expressing the same unit. Peer SCUs are identiﬁed in the
            peer, matched against model SCUs, and weighted accordingly. The PYRAMIDS score given to a peer is the ratio
            of the sum of the weights of its units and the sum of the weights of the best possible ideal summary with
            the same number of SCUs as the peer. The PYRAMIDS scores can be also used for ranking summarization systems.
            [11] showed that PYRAMIDS scores produced reliable system rankings when multiple (4 or more) models were
            used and that PYRAMIDS rankings correlate with rankings produced by ROUGE-2 and ROUGE-SU2 (i.e. ROUGE with
            skip 2-grams). However, this method requires the creation 13Polibits (42) 2010 of models and the
            identiﬁcation, matching, and weighting of SCUs in both: models and peers. non-random systems, no clear
            conclusion was reached on the value of each of the studied measures. [12] evaluated the effectiveness of the
            Jensen-Shannon (J S) [13] theoretic measure in predicting systems ranks in two summarization tasks:
            query-focused and update summarization. They have shown that ranks produced and those produced by J S
            measure by PYRAMIDS correlate. However, investigate the effect the measure in summarization tasks such as
            generic of multi-document 2), biographical summarization (DUC 2004 Task 5), opinion summarization (TAC 2008
            OS), and summarization in languages other than English. (DUC 2004 Task summarization they did not In this
            paper we present a series of experiments aimed at a better understanding of the value of the J S divergence
            for ranking summarization systems. We have carried out experimentation with the proposed measure and we have
            veriﬁed that in certain tasks (such as those studied by there is a strong correlation among PYRAMIDS, [12])
            RESPONSIVENESS and the J S divergence, but as we will show in this paper, there are datasets in which the
            correlation is not so strong. We also present experiments in Spanish and French showing positive correlation
            between the J S and ROUGE which is the de facto evaluation measure used in evaluation of non-English
            summarization. To the best of our knowledge this is the more extensive set of experiments interpreting the
            value of evaluation without human models. The rest of the paper is organized in the following way: First in
            Section II we introduce related work in the area of content-based evaluation identifying the departing point
            for our inquiry; then in Section III we explain the methodology adopted in our work and the tools and
            resources used for experimentation. In Section IV we present the experiments carried out together with the
            results. Section V discusses the results and Section VI concludes the paper and identiﬁes future work.
        </introduction>
        <corps>II. RELATED WORK

            One of the ﬁrst works to use content-based measures in
            text summarization evaluation is due to [5], who presented an
            evaluation framework to compare rankings of summarization
            systems produced by recall and cosine-based measures. They
            showed that
            there was weak correlation among rankings
            produced by recall, but that content-based measures produce
            rankings which were strongly correlated. This put forward
            the idea of using directly the full document for comparison
            purposes in text summarization evaluation. [6] presented a
            set of evaluation measures based on the notion of vocabulary
            overlap including n-gram overlap, cosine similarity, and
            longest common subsequence, and they applied them to
            summarization in English and Chinese.
            multi-document
            However,
            the
            measures in different summarization tasks. [7] also compared
            various evaluation measures based on vocabulary overlap.
            Although these measures were able to separate random from

            they did not evaluate the performance of

            Nowadays,

            summarization

            a widespread

            evaluation
            framework is ROUGE [14], which offers a set of statistics
            that compare peer
            It counts
            co-occurrences of n-grams in peer and models to derive a
            score. There are several statistics depending on the used
            n-grams and the text processing applied to the input texts
            (e.g., lemmatization, stop-word removal).

            summaries with models.

            [15] proposed a method of evaluation based on the
            use of “distances” or divergences between two probability
            distributions (the distribution of units in the automatic
            summary and the distribution of units
            in the model
            summary). They studied two different Information Theoretic
            measures of divergence: the Kullback-Leibler (KL) [16] and
            Jensen-Shannon (J S) [13] divergences. KL computes the
            divergence between probability distributions P and Q in the
            following way:

            DKL(P ||Q) =

            1
            2

            (cid:88)

            w

            Pw log2

            Pw
            Qw

            (1)

            While J S divergence is deﬁned as follows:

            w

            1
            2

            (cid:88)

            Pw log2

            + Qw log2

            DJ S (P ||Q) =

            2Pw
            Pw + Qw

            2Qw
            Pw + Qw
            (2)
            These measures can be applied to the distribution of units in
            system summaries P and reference summaries Q. The value
            obtained may be used as a score for the system summary. The
            method has been tested by [15] over the DUC 2002 corpus for
            single and multi-document summarization tasks showing good
            correlation among divergence measures and both coverage and
            ROUGE rankings.

            [12] went even further and, as in [5], they proposed to
            compare directly the distribution of words in full documents
            with the distribution of words in automatic summaries to
            derive a content-based evaluation measure. They found a
            high correlation between rankings produced using models
            and rankings produced without models. This last work is the
            departing point for our inquiry into the value of measures that
            do not rely on human models.

            III. METHODOLOGY

            The followed methodology in this paper mirrors the one
            adopted in past work (e.g. [5], [7], [12]). Given a particular
            summarization task T , p data points to be summarized
            with input material {Ii}p−1
            i=0 (e.g. document(s), question(s),
            topic(s)), s peer summaries {SUMi,k}s−1
            k=0 for input i, and
            m model summaries {MODELi,j}m−1
            for input i, we will
            j=0
            compare rankings of the s peer summaries produced by various
            evaluation measures. Some measures that we use compare
            summaries with n of the m models:

            MEASUREM (SUMi,k, {MODELi,j}n−1
            j=0 )

            (3)

            Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia
            Velázquez-Morales14Polibits (42) 2010
            while other measures compare peers with all or some of the
            input material:

            MEASUREM (SUMi,k, I (cid:48)
            i)

            (4)

            where I (cid:48)
            is some subset of input Ii. The values produced
            i
            by the measures for each summary SUMi,k are averaged
            for each system k = 0, . . . , s − 1 and these averages are
            used to produce a ranking. Rankings are then compared
            using Spearman Rank correlation [17] which is used to
            measure the degree of association between two variables
            whose values are used to rank objects. We have chosen
            to use this correlation to compare directly results to those
            presented in [12]. Computation of correlations is done using
            the Statistics-RankCorrelation-0.12 package1, which computes
            the rank correlation between two vectors. We also veriﬁed
            the good conformity of the results with the correlation test
            of Kendall τ calculated with the statistical software R. The
            two nonparametric tests of Spearman and Kendall do not
            really stand out as the treatment of ex-æquo. The good
            correspondence between the two tests shows that they do not
            introduce bias in our analysis. Subsequently will mention only
            the ρ of Sperman more widely used in this ﬁeld.

            A. Tools

            based

            evaluation measures

            We carry out experimentation using a new summarization
            evaluation framework: FRESA –FRamework for Evaluating
            Summaries Automatically–, which includes document-based
            summary
            probabilities
            distribution2. As in the ROUGE package, FRESA supports
            different n-grams and skip n-grams probability distributions.
            The FRESA environment can be used in the evaluation of
            summaries in English, French, Spanish and Catalan, and it
            integrates ﬁltering and lemmatization in the treatment of
            summaries and documents. It is developed in Perl and will
            be made publicly available. We also use the ROUGE package
            [10] to compute various ROUGE statistics in new datasets.

            on

            B. Summarization Tasks and Data Sets

            We have conducted our experimentation with the following

            summarization tasks and data sets:

            1) Generic multi-document-summarization

            in English
            (production of a short summary of a cluster of related
            documents) using data from DUC’043,
            task 2: 50
            clusters, 10 documents each – 294,636 words.

            2) Focused-based summarization in English (production of
            a short focused multi-document summary focused on the
            question “who is X?”, where X is a person’s name) using
            data from the DUC’04 task 5: 50 clusters, 10 documents
            each plus a target person name – 284,440 words.

            3) Update-summarization task that consists of creating a
            summary out of a cluster of documents and a topic. Two
            sub-tasks are considered here: A) an initial summary has
            to be produced based on an initial set of documents and
            topic; B) an update summary has to be produced from
            a different (but related) cluster assuming documents
            used in A) are known. The English TAC’08 Update
            Summarization dataset is used, which consists of 48
            topics with 20 documents each – 36,911 words.

            4) Opinion summarization where systems have to analyze
            a set of blog articles and summarize the opinions
            about a target
            in the articles. The TAC’08 Opinion
            Summarization in English4 data set (taken from the
            Blogs06 Text Collection) is used: 25 clusters and targets
            (i.e., target entity and questions) were used – 1,167,735
            words.

            5) Generic single-document summarization in Spanish
            using the Medicina Cl´ınica5 corpus, which is composed
            of 50 medical articles in Spanish, each one with its
            corresponding author abstract – 124,929 words.

            6) Generic single document summarization in French using
            the “Canadien French Sociological Articles” corpus
            from the journal Perspectives interdisciplinaires sur le
            travail et la sant´e (PISTES)6. It contains 50 sociological
            articles in French, each one with its corresponding
            author abstract – 381,039 words.

            7) Generic multi-document-summarization in French using
            data from the RPM27 corpus [18], 20 different themes
            consisting of 10 articles and 4 abstracts by reference
            thematic – 185,223 words.

            For experimentation in the TAC and the DUC datasets we use
            directly the peer summaries produced by systems participating
            in the evaluations. For experimentation in Spanish and French
            (single and multi-document summarization) we have created
            summaries at a similar ratio to those of reference using the
            following systems:

            – ENERTEX [19], a summarizer based on a theory of

            textual energy;

            – CORTEX [20], a single-document sentence extraction
            system for Spanish and French that combines various
            statistical measures of relevance (angle between sentence
            and topic, various Hamming weights for sentences, etc.)
            and applies an optimal decision algorithm for sentence
            selection;

            – SUMMTERM [21], a terminology-based summarizer that
            is used for summarization of medical articles and
            uses specialized terminology for scoring and ranking
            sentences;

            – REG [22], summarization system based on an greedy

            algorithm;

            1http://search.cpan.org/∼gene/Statistics-RankCorrelation-0.12/
            2FRESA is available at: http://lia.univavignon.fr/ﬁleadmin/axes/TALNE/

            Ressources.html

            3http://www-nlpir.nist.gov/projects/duc/guidelines/2004.html

            4http://www.nist.gov/tac/data/index.html
            5http://www.elsevier.es/revistas/ctl servlet? f=7032&revistaid=2
            6http://www.pistes.uqam.ca/
            7http://www-labs.sinequa.com/rpm2

            Summary Evaluation with and without References15Polibits (42) 2010
            – J S summarizer, a summarization system that scores
            and ranks sentences according to their Jensen-Shannon
            divergence to the source document;

            – a lead-based summarization system that selects the lead

            sentences of the document;

            – a random-based summarization system that

            selects

            sentences at random;

            – Open Text Summarizer [23], a multi-lingual summarizer

            based on the frequency and

            – commercial systems: Word, SSSummarizer8, Pertinence9

            and Copernic10.

            C. Evaluation Measures

            The following measures derived from human assessment of

            the content of the summaries are used in our experiments:

            – COVERAGE is understood as the degree to which one
            peer summary conveys the same information as a model
            summary [2]. COVERAGE was used in DUC evaluations.
            This measure is used as indicated in equation 3 using
            human references or models.

            – RESPONSIVENESS ranks summaries in a 5-point scale
            indicating how well
            the summary satisﬁed a given
            is used in focused-based
            information need [2].
            summarization tasks. This measure is used as indicated
            in equation 4 since a human judges the summary
            with respect
            to a given input “user need” (e.g., a
            question). RESPONSIVENESS was used in DUC and TAC
            evaluations.

            It

            – PYRAMIDS [11] is a content assessment measure which
            compares content units in a peer summary to weighted
            content units in a set of model summaries. This
            measure is used as indicated in equation 3 using human
        </corps>
        <conclusion>VI. CONCLUSIONS AND FUTURE WORK

            This paper has presented a series of experiments in
            content-based measures that do not rely on the use of model
            summaries for comparison purposes. We have carried out
            extensive experimentation with different summarization tasks
            drawing a clearer picture of tasks where the measures could
            be applied. This paper makes the following contributions:

            – We have shown that if we are only interested in ranking
            summarization systems according to the content of their
            automatic summaries, there are tasks were models could
            be subtituted by the full document in the computation of
            the J S measure obtaining reliable rankings. However,
            we have also found that
            the substitution of models
            by full-documents is not always advisable. We have

            Summary Evaluation with and without References17Polibits (42) 2010
            TABLE II
            SPEARMAN ρ OF CONTENT-BASED MEASURES WITH COVERAGE IN DUC’04 TASK 2

            Mesure
            ROUGE-2
            J S

            COVERAGE
            0.79
            0.68

            p-value
            p
            <
            0.0050
            p
            <
            0.0025

            TABLE III
            SPEARMAN ρ OF CONTENT-BASED MEASURES IN DUC’04 TASK 5

            Mesure
            ROUGE-2
            J S

            COVERAGE
            0.78
            0.40

            p-value
            p
            <
            0.001
            p
            <
            0.050

            RESPONSIVENESS
            0.44
            -0.18

            p-value
            p
            <
            0.05
            p
            <
            0.25

            TABLE IV
            SPEARMAN ρ OF CONTENT-BASED MEASURES IN TAC’08 OS TASK

            Mesure
            J S

            PYRAMIDS
            -0.13

            p-value
            p
            <
            0.25

            RESPONSIVENESS
            -0.14

            p-value
            p
            <
            0.25

            TABLE V
            SPEARMAN ρ OF CONTENT-BASED MEASURES WITH ROUGE IN THE Medicina Cl´ınica CORPUS (SPANISH)

            Mesure
            J S
            J S2
            J S4
            J SM

            ROUGE-1
            0.56
            0.88
            0.88
            0.82

            p-value
            p
            <
            0.100
            p
            <
            0.001
            p
            <
            0.001
            p
            <
            0.005

            ROUGE-2
            0.46
            0.80
            0.80
            0.71

            p-value
            p
            <
            0.100
            p
            <
            0.002
            p
            <
            0.002
            p
            <
            0.020

            ROUGE-SU4
            0.45
            0.81
            0.81
            0.71

            p-value
            p
            <
            0.200
            p
            <
            0.005
            p
            <
            0.005
            p
            <
            0.010

            found weak correlation among different rankings in
            complex summarization tasks such as the summarization
            of biographical information and the summarization of
            opinions.

            – We have also carried out

            large-scale experiments in
            Spanish and French which show positive medium to
            strong correlation among system’s ranks produced by
            ROUGE and divergence measures that do not use the
            model summaries.

            – We have also presented a new framework, FRESA, for
            the computation of measures based on J S divergence.
            Following the ROUGE approach, FRESA package use
            word uni-grams, 2-grams and skip n-grams computing
            divergences. This framework will be available to the
            community for research purposes.

            Although we have made a number of contributions, this paper
            leaves many open questions than need to be addressed. In
            order to verify correlation between ROUGE and J S, in the
            short term we intend to extend our investigation to other
            languages such as Portuguese and Chinesse for which we
            have access to data and summarization technology. We also
            plan to apply FRESA to the rest of the DUC and TAC
            summarization tasks, by using several smoothing techniques.
            As a novel idea, we contemplate the possibility of adapting
            the evaluation framework for the phrase compression task
            [29], which, to our knowledge, does not have an efﬁcient
            evaluation measure. The main idea is to calculate J S from
            an automatically-compressed sentence taking the complete
            sentence by reference. In the long term, we plan to incorporate

            a representation of
            the task/topic in the calculation of
            measures. To carry out these comparisons, however, we are
            dependent on the existence of references.

            FRESA will also be used in the new question-answer task
            campaign INEX’2010 (http://www.inex.otago.ac.nz/tracks/qa/
            qa.asp) for the evaluation of long answers. This task aims
            to answer a question by extraction and agglomeration of
            sentences in Wikipedia. This kind of
            task corresponds
            to those for which we have found a high correlation
            among the measures J S and evaluation methods with
            human intervention. Moreover, the J S calculation will be
            among the summaries produced and a representative set of
            relevant passages from Wikipedia. FRESA will be used to
            compare three types of systems, although different tasks: the
            multi-document summarizer guided by a query, the search
            systems targeted information (focused IR) and the question
            answering systems.
        </conclusion>
        <discussion>V. DISCUSSION

            The departing point for our inquiry into text summarization
            evaluation has been recent work on the use of content-based

            evaluation metrics that do not rely on human models but that
            compare summary content to input content directly [12]. We
            have some positive and some negative results regarding the
            direct use of the full document in content-based evaluation.

            and

            We have veriﬁed that
            in

            in both generic muti-document
            summarization
            topic-based multi-document
            summarization in English correlation among measures
            that use human models
            (PYRAMIDS, RESPONSIVENESS
            and ROUGE) and a measure that does not use models
            (J S divergence) is strong. We have found that correlation
            among the same measures is weak for summarization of
            biographical information and summarization of opinions in
            blogs. We believe that in these cases content-based measures
            should be considered, in addition to the input document, the
            summarization task (i.e. text-based representation, description)
            to better assess the content of the peers [25], the task being a
            determinant factor in the selection of content for the summary.
            Our multi-lingual experiments in generic single-document
            summarization conﬁrm a strong correlation among the
            J S divergence and ROUGE measures. It
            is worth noting
            that ROUGE is
            the chosen framework for
            presenting content-based evaluation results in non-English
            summarization.

            in general

            For the experiments in Spanish, we are conscious that we
            only have one model summary to compare with the peers.
            Nevertheless, these models are the corresponding abstracts
            written by the authors. As the experiments in [26] show, the
            professionals of a specialized domain (as, for example, the
            medical domain) adopt similar strategies to summarize their
            texts and they tend to choose roughly the same content chunks
            for their summaries. Previous studies have shown that author
            abstracts are able to reformulate content with ﬁdelity [27] and
            these abstracts are ideal candidates for comparison purposes.
            Because of this, the summary of the author of a medical article
            can be taken as reference for summaries evaluation. It is worth
            noting that there is still debate on the number of models to be
            used in summarization evaluation [28]. In the French corpus
            PISTES, we suspect the situation is similar to the Spanish
            case.
        </discussion>
        <biblio>[1] I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, and
            B. Sundheim, “Summac: a text summarization evaluation,” Natural
            Language Engineering, vol. 8, no. 1, pp. 43–68, 2002.

            [2] P. Over, H. Dang, and D. Harman, “DUC in context,” IPM, vol. 43,

            no. 6, pp. 1506–1520, 2007.

            [3] Proceedings of the Text Analysis Conference. Gaithesburg, Maryland,

            USA: NIST, November 17-19 2008.

            [4] K. Sp¨arck Jones and J. Galliers, Evaluating Natural Language
            Processing Systems, An Analysis and Review, ser. Lecture Notes in
            Computer Science. Springer, 1996, vol. 1083.

            [5] R. L. Donaway, K. W. Drummey, and L. A. Mather, “A comparison of
            rankings produced by summarization evaluation measures,” in NAACL
            Workshop on Automatic Summarization, 2000, pp. 69–78.

            [6] H. Saggion, D. Radev, S. Teufel, and W. Lam, “Meta-evaluation
            of Summaries in a Cross-lingual Environment using Content-based
            Metrics,” in COLING 2002, Taipei, Taiwan, August 2002, pp. 849–855.
            [7] D. R. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer, H. Qi, A. C¸ elebi,
            D. Liu, and E. Dr´abek, “Evaluation challenges in large-scale document
            summarization,” in ACL’03, 2003, pp. 375–382.

            [8] K. Papineni, S. Roukos, T. Ward, , and W. J. Zhu, “BLEU: a method
            for automatic evaluation of machine translation,” in ACL’02, 2002, pp.
            311–318.

            [9] K. Pastra and H. Saggion, “Colouring summaries BLEU,” in Evaluation
            Initiatives in Natural Language Processing. Budapest, Hungary: EACL,
            14 April 2003.

            [10] C.-Y. Lin, “ROUGE: A Package for Automatic Evaluation of
            Summaries,” in Text Summarization Branches Out: ACL-04 Workshop,
            M.-F. Moens and S. Szpakowicz, Eds., Barcelona, July 2004, pp. 74–81.
            [11] A. Nenkova and R. J. Passonneau, “Evaluating Content Selection in
            Summarization: The Pyramid Method,” in HLT-NAACL, 2004, pp.
            145–152.

            [12] A. Louis and A. Nenkova, “Automatically Evaluating Content Selection
            in Summarization without Human Models,” in Empirical Methods in
            Natural Language Processing, Singapore, August 2009, pp. 306–314.
            [Online]. Available: http://www.aclweb.org/anthology/D/D09/D09-1032
            [13] J. Lin, “Divergence Measures based on the Shannon Entropy,” IEEE
            Transactions on Information Theory, vol. 37, no. 145-151, 1991.
            [14] C.-Y. Lin and E. Hovy, “Automatic Evaluation of Summaries Using
            N-gram Co-occurrence Statistics,” in HLT-NAACL. Morristown, NJ,
            USA: Association for Computational Linguistics, 2003, pp. 71–78.
            [15] C.-Y. Lin, G. Cao, J. Gao, and J.-Y. Nie, “An information-theoretic
            approach to automatic evaluation of summaries,” in HLT-NAACL,
            Morristown, USA, 2006, pp. 463–470.

            [16] S. Kullback and R. Leibler, “On information and sufﬁciency,” Ann. of

            Math. Stat., vol. 22, no. 1, pp. 79–86, 1951.

            [17] S. Siegel and N. Castellan, Nonparametric Statistics for the Behavioral

            Sciences. McGraw-Hill, 1998.

            Summary Evaluation with and without References19Polibits (42) 2010
        </biblio>
    </article>
</articles>
