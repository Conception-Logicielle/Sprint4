<articles>
	<article>
		<preamble>ACL2004-HEADLINE.txt</preamble>
		<titre>Hybrid Headlines: Combining Topics and Sentence Compression</titre>
		<auteur>David Zajic, Bonnie Dorr, Stacy President Richard Schwartz Department of Computer Science</auteur>
		<abstract>This paper presents Topiary, a headlinegeneration system that creates very short, informative summaries for news stories by combining sentence compression and unsupervised topic discovery. We will show that the combination of linguistically motivated sentence compression with statistically selected topic terms performs better than either alone, according to some automatic summary evaluation measures. In addition we describe experimental results establishing an appropriate extrinsic task on which to measure the effect of summarization on human performance. We demonstrate the usefulness of headlines in comparison to full texts in the context of this extrinsic task.</abstract>
		<introduction>In this paper we present Topiary, a headlinegeneration system that creates very short, informative summaries for news stories by combining sentence compression and unsupervised topic discovery. Hedge Trimmer performs sentence compression by removing constituents from a parse tree of the lead sentence according to a set of linguistically-motivated heuristics until a length threshold is reached. Unsupervised Topic Discovery is a statistical method for deriving a set of topic models from a document corpus, assigning meaningful names to the topic models, and associating sets of topics with specific documents. The topics and sentence compressions are combined in a manner that preserves the advantages of each approach: the fluency and event-oriented information from the lead sentence with the broader coverage of the topic models. The next section presents previous work in the area of automatic summarization. Following this we describe Hedge Trimmer and Unsupervised Topic Discovery in more detail, and describe the algorithm for combining sentence compression with topics. Next we show that Topiary scores higher than either Hedge Trimmer or Unsupervised Topic Discovery alone according to certain automatic evaluation tools for summarization. Finally we propose event tracking as an extrinsic task using automatic summarization for measuring how human performance is affected by automatic summarization, and for correlating human peformance with automatic evaluation tools. We describe an experiment that supports event tracking as an appropriate task for this purpose, and show results that suggest that a well-written human headline is nearly as useful for event tracking as the full text. 2 Previous Work Hedge Trimmer is a sentence compression algorithm based on linguistically-motivated heuristics. Previous work on sentence compression (Knight and Marcu, 2000) uses a noisy-channel model to find the most probable short string that generated the observed full sentence. Other work (Euler, 2002) combines a word-list with syntactic in- formation to decide which words and phrases to cancel. Our approach differs from Knight’s in that we do not use a statistical model, so we do not require any prior training on a large corpus of story/headline pairs. Topiary shares with Euler the combination of topic lists and sentence compression. However Euler uses the topic lists to guide sentence selection and compression towards a query-specific summary, whereas Topiary uses topics to augment the concept coverage of a generic summary. Summaries can also consist of lists of words or short phrases indicating that the topic or concept they denote is important in the document. Extractive topic summaries consist of keywords or key phrases that occur in the document. (Bergler et al., 2003) achieves this by choosing noun phrases that represent the most important text entities, as represented by noun phrase coreference chains. (Zhou and Hovy, 2003) imposes fluency onto a topic list by finding phrase clusters early in the text that contain important topic words found throughout the text. In text categorization documents are assigned to pre-defined categories. This is equivalent to assigning topics to a document from a static topic list, so the words in the summary need not actually appear in the document. (Lewis, 1992) describes a probabilistic feature-based method for assigning Reuters topics to news stories. OnTopic (Schwartz et al., 1997) uses a HMM to assign topics from a topic-annotated corpus to a new document. 3 Algorithm Description Topiary produces headlines by combining the output of Hedge Trimmer, a sentence compression algorithm, with Unsupervised Topic Detection (UTD). In this section we will give brief descriptions of Hedge Trimmer, recent modifications to Hedge Trimmer, and UTD. We will then describe how Hedge Trimmer and UTD are combined. 3.1 Hedge Trimmer Hedge Trimmer (Dorr et al., 2003b) generates a headline for a news story by compressing the lead (or main) topic sentence according to a linguistically motivated algorithm. For news stories, the first sentence of the document is taken to be the lead sentence. The compression consists of parsing the sentence using the BBN SIFT parser (Miller et al., 1998) and removing low-content syntactic constituents. Some constituents, such as certain determiners (the, a) and time expressions are always removed, because they rarely occur in human-generated headlines and are low-content in comparison to other constituents. Other constituents are removed one-by-one until a length threshold has been reached. These include, among others, relative clauses, verb-phrase conjunction, preposed adjuncts and prepositional phrases that do not contain named entities. 1 The threshold can be specified either in number of words or number of characters. If the threshold is specified in number of characters, Hedge Trimmer will not include partial words. 3.2 Recent Hedge Trimmer Work Recently we have investigated a rendering of the summary as “Headlinese” (Mårdh, 1980) in which certain constituents are dropped with no loss of meaning. The result of this investigation has been used to enhance Hedge Trimmer, most notably the removal of certain instances of have and be. For example, the previous headline generator produced summaries such as Sentence (2), whereas the have/be removal produces (3). (1) Input: The senior Olympic official who leveled stunning allegations of corruption within the IOC said Sunday he had been “muzzled” by president Juan Antonio Samaranch and might be thrown out of the organization. (2) Without participle have/be removal: Senior Olympic official said he had been muzzled (3) With participle have/be removal: Senior Olympic official said he muzzled by president Juan Antonio Samaranch Have and be are removed if they are part of a past or present participle construction. In this example, the removal of had been allows a high-content constituent by president Juan Antonio Samaranch to fit into the headline. The removal of forms of to be allows Hedge Trimmer to produce headlines that concentrate 1 More details of the Hedge Trimmer algorithm can be found in (Dorr et al., 2003b) and (Dorr et al., 2003a). more information in the allowed space. The removal of forms of to be results in sentences that are not grammatical in general English, but are typical of Headlinese English. For example, sentences (5), (6) and all other examples in this paper were trimmed to fit in 75 characters. emergency shortening methods which are only to be used when the alternative is truncating the headline after the threshold, possibly cutting the middle of a word. These include removal of adverbs and adverbial phrases, adjectives and adjective phrases, and nouns that modify other nouns. (4) Input: Leading maxi yachts Brindabella, Sayonara and Marchioness were locked in a three-way duel down the New South Wales state coast Saturday as the Sydney to Hobart fleet faced deteriorating weather. 3.3 (5) Without to be removal: Sayonara and Marchioness were locked in three (6) With to be removal: Leading maxi yachts Brindabella Sayonara and Marchioness locked in three When have and be occur with a modal verb, the modal verb is also removed. Sentence (9) shows an example of this. It could be argued that by removing modals such as should and would the meaning is vitally changed. The intended use of the headline must be considered. If the headlines are to be used for determining query relevance, removal of modals may not hinder the user while making room for additional high-content words may help. (7) Input: Organizers of December’s Asian Games have dismissed press reports that a sports complex would not be completed on time, saying preparations are well in hand, a local newspaper said Friday. (8) Without Modal-Have/Be Removal: Organizers have dismissed press reports saying (9) With Modal-Have/Be Removal: Organizers dismissed press reports sports complex not completed saying In addition when it or there appears as a subject with a form of be or have, as in extraposition (It was clear that the thief was hungry) or existential clauses (There have been a spate of dog maulings), the subject and the verb are removed. Finally, for situations in which the length threshold is a hard constraint, we added some Unsupervised Topic Discovery Unsupervised Topic Discovery (UTD) is used when we do not have a corpus annotated with topics. It takes as input a large unannotated corpus in any language and automatically creates a set of topic models with meaningful names. The algorithm has several stages. First, it analyzes the corpus to find strings of words that occur frequently. (It does this using a Minimum Description Length criterion.) These are frequently phrases that are meaningful names of topics. Second, it finds the high-content words in each document (using a modified tf.idf measure). These are possible topic names for each document. It keeps only those names that occur in at least four different documents. These are taken to be an initial set of topic names. In the third stage UTD trains topic models corresponding to these topic names. The modified EM procedure of OnTopicTM is used to determine which words in the documents often signify these topic names. This produces topic models. Fourth, these topic models are used to find the most likely topics for each document. This often adds new topics to documents, even though the topic name did not appear in the document. We found, in various experiments, that the topics derived by this procedure were usually meaningful and that the topic assignment was about as good as when the topics were derived from a corpus that was annotated by people. We have also used this procedure on different languages and shown the same behavior. Sentence (10) is a topic list generated for a story about the investigation into the bombing of the U.S. Embassy in Nairobi on August 7, 1998. (10) BIN LADEN EMBASSY BOMBING POLICE OFFICIALS PRISON HOUSE FIRE KABILA 3.4 Combination of Hedge Trimmer and Topics: Topiary The Hedge Trimmer algorithm is constrained to take its headline from a single sentence. It is often the case that there is no single sentence that contains all the important information in a story. The information can be spread over two or three sentences, with pronouns or ellipsis used to link them. In addition, our algorithms do not always select the ideal sentence and trim it perfectly. Topics alone also have drawbacks. UTD rarely generates any topic names that are verbs. Thus topic lists are good at indicating the general subject are but rarely give any direct indication of what events took place. Topiary is a modification of the enhanced Hedge Trimmer algorithm to take a list of topics with relevance scores as additional input. The compression threshold is lowered so that there will be room for the highest scoring topic term that isn’t already in the headline. This amount of threshold lowering is dynamic, because the trimming of the sentence can remove a previously ineligible high-scoring topic term from the headline. After trimming is complete, additional topic terms that do not occur in the headline are added to use up any remaining space. This often results in one or more main topics about the story and a short sentence that says what happened concerning them. The combination is often more concise than a fully fluent sentence and compensates for the fact that the topic and the description of what happened to it do not appear in the same sentence in the original story. Sentences (11) and (12) are the output of Hedge Trimmer and Topiary for the same story for which the topics in Sentence (10) were generated. (11) FBI agents this week began questioning relatives of the victims (12) BIN LADEN EMBASSY BOMBING FBI agents this week began questioning relatives Topiary was submitted to the Document Understanding Conference Workshop. Figure 1 shows how Topiary peformed in comparison with other DUC2004 participants on task 1, using ROUGE. Task 1 was to produce a summary for a single news document no more than than 75 characters. The different ROUGE variants are sorted by overall performance of the systems. The key observations are that there was a wide range of performance among the submitted systems, and that Topiary scored first or second among the automatic systems on each ROUGE measure. 4</introduction>
		<corps></corps>
		<conclusion></conclusion>
		<discussion></discussion>
		<biblio></biblio>
	</article>
	<article>
		<preamble>Boudin-Torres-2006.txt</preamble>
		<titre>A Scalable MMR Approach to Sentence Scoring for Multi-Document Update Summarization</titre>
		<auteur>Florian Boudin  and Marc El-Bèze</auteur>
		<abstract>redundancy with previously read documents (history) has to be removed from the extract. A natural way to go about update summarization would be extracting temporal tags (dates, elapsed times, temporal expressions...) (Mani and Wilson, 2000) or to automatically construct the timeline from documents (Swan and Allan, 2000). These temporal marks could be used to focus extracts on the most recently written facts. However, most recently written facts are not necessarily new facts. Machine Reading (MR) was used by (Hickl et al., 2007) to construct knowledge representations from clusters of documents. Sentences containing “new” information (i.e. that could not be inferred by any previously considered document) are selected to generate summary. However, this highly efficient approach (best system in DUC 2007 update) requires large linguistic resources. (Witte et al., 2007) propose a rule-based system based on fuzzy coreference cluster graphs. Again, this approach requires to manually write the sentence ranking scheme. Several strategies remaining on post-processing redundancy removal techniques have been suggested. Extracts constructed from history were used by (Boudin and TorresMoreno, 2007) to minimize history’s redundancy. (Lin et al., 2007) have proposed a modified Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) re-ranker during sentence selection, constructing the summary by incrementally re-ranking sentences. In this paper, we propose a scalable sentence scoring method for update summarization derived from MMR. Motivated by the need for relevant novelty, candidate sentences are selected according to a combined criterion of query relevance and dissimilarity with previously read sentences. The rest of the paper is organized as follows. Section 2 We present S MMR, a scalable sentence scoring method for query-oriented update summarization. Sentences are scored thanks to a criterion combining query relevance and dissimilarity with already read documents (history). As the amount of data in history increases, non-redundancy is prioritized over query-relevance. We show that S MMR achieves promising results on the DUC 2007 update corpus. 1</abstract>
		<introduction>Extensive experiments on query-oriented multidocument summarization have been carried out over the past few years. Most of the strategies to produce summaries are based on an extraction method, which identifies salient textual segments, most often sentences, in documents. Sentences containing the most salient concepts are selected, ordered and assembled according to their relevance to produce summaries (also called extracts) (Mani and Maybury, 1999). Recently emerged from the Document Understanding Conference (DUC) 20071 , update summarization attempts to enhance summarization when more information about knowledge acquired by the user is available. It asks the following question: has the user already read documents on the topic? In the case of a positive answer, producing an extract focusing on only new facts is of interest. In this way, an important issue is introduced: c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 Document Understanding Conferences are conducted since 2000 by the National Institute of Standards and Technology (NIST), http://www-nlpir.nist.gov 23 Coling 2008: Companion volume – Posters and Demonstrations, pages 23–26 Manchester, August 2008</introduction>
		<corps></corps>
		<conclusion></conclusion>
		<discussion></discussion>
		<biblio></biblio>
	</article>
	<article>
		<preamble>compression.txt</preamble>
		<titre>Multi-Candidate Reduction: Sentence Compression as a Tool for Document Summarization Tasks∗</titre>
		<auteur>David Zajic , Bonnie J. Dorr , Jimmy Lin , Richard Schwartz</auteur>
		<abstract>This article examines the application of two single-document sentence compression techniques to the problem of multi-document summarization—a “parse-and-trim” approach and a statistical noisy-channel approach. We introduce the Multi-Candidate Reduction (MCR) framework for multi-document summarization, in which many compressed candidates are generated for each source sentence. These candidates are then selected for inclusion in the final summary based on a combination of static and dynamic features. Evaluations demonstrate that sentence compression is a valuable component of a larger multi-document summarization framework. Keywords: headline generation, summarization, parse-and-trim, Hidden Markov Model PACS: Artificial intelligence, 07.05.Mh; Computer science and technology, 89.20.Ff; Spoken languages, processing of, 43.71.Sy 1</abstract>
		<introduction></introduction>
		<corps></corps>
		<conclusion></conclusion>
		<discussion></discussion>
		<biblio></biblio>
	</article>
	<article>
		<preamble>compression_phrases_Prog-Linear-jair.txt</preamble>
		<titre>Global Inference for Sentence Compression An Integer Linear Programming Approach</titre>
		<auteur>James Clarke</auteur>
		<abstract>Sentence compression holds promise for many applications ranging from summarization to subtitle generation. Our work views sentence compression as an optimization problem and uses integer linear programming (ILP) to infer globally optimal compressions in the presence of linguistically motivated constraints. We show how previous formulations of sentence compression can be recast as ILPs and extend these models with novel global constraints. Experimental results on written and spoken texts demonstrate improvements over state-of-the-art models.</abstract>
		<introduction>The computational treatment of sentence compression has recently attracted much attention in the literature. The task can be viewed as producing a summary of a single sentence that retains the most important information and remains grammatical (Jing, 2000). A sentence compression mechanism would greatly benefit a wide range of applications. For example, in summarization, it could improve the conciseness of the generated summaries (Jing, 2000; Lin, 2003; Zajic, Door, Lin, & Schwartz, 2007). Other examples include compressing text to be displayed on small screens such as mobile phones or PDAs (Corston-Oliver, 2001), subtitle generation from spoken transcripts (Vandeghinste & Pan, 2004), and producing audio scanning devices for the blind (Grefenstette, 1998). Sentence compression is commonly expressed as a word deletion problem: given an input source sentence of words x = x1 , x2 , . . . , xn , the aim is to produce a target compression by removing any subset of these words (Knight & Marcu, 2002). The compression problem has been extensively studied across different modeling paradigms, both supervised and</introduction>
		<corps></corps>
		<conclusion></conclusion>
		<discussion></discussion>
		<biblio></biblio>
	</article>
	<article>
		<preamble>hybrid_approach.txt</preamble>
		<titre>Sentence Compression for Automated Subtitling: A Hybrid Approach</titre>
		<auteur>Vincent Vandeghinste and Yi Pan Centre for Computational Linguistics Katholieke Universiteit Leuven Maria Theresiastraat  BE-3000 Leuven</auteur>
		<abstract>In this paper a sentence compression tool is described. We describe how an input sentence gets analysed by using a.o. a tagger, a shallow parser and a subordinate clause detector, and how, based on this analysis, several compressed versions of this sentence are generated, each with an associated estimated probability. These probabilities were estimated from a parallel transcript/subtitle corpus. To avoid ungrammatical sentences, the tool also makes use of a number of rules. The evaluation was done on three different pronunciation speeds, averaging sentence reduction rates of 40% to 17%. The number of reasonable reductions ranges between 32.9% and 51%, depending on the average estimated pronunciation speed.</abstract>
		<introduction>A sentence compression tool has been built with the purpose of automating subtitle generation for the deaf and hard-of-hearing. Verbatim transcriptions cannot be presented as the subtitle presentation time is between 690 and 780 characters per minute, which is more or less 5.5 seconds for two lines (ITC, 1997), (Dewulf and Saerens, 2000), while the average speech rate contains a lot more than the equivalent of 780 characters per minute. The actual amount of compression needed depends on the speed of the speaker and on the amount of time available after the sentence. In documentaries, for instance, there are often large silent intervals between two sentences, the speech is often slower and the speaker is off-screen, so the available presentation time is longer. When the speaker is off-screen, the synchrony of the subtitles with the speech is of minor importance. When subtitling the news the speech rate is often very high so the amount of reduction needed to allow the synchronous presentation of subtitles and speech is much greater. The sentence compression rate is a parameter which can be set for each sentence. Note that the sentence compression tool de- scribed in this paper is not a subtitling tool. When subtitling, only when a sentence needs to be reduced, and the amount of reduction is known, the sentence is sent to the sentence compression tool. So the sentence compression tool is a module of an automated subtitling tool. The output of the sentence compression tool needs to be processed according to the subtitling guidelines like (Dewulf and Saerens, 2000), in order to be in the correct lay-out which makes it usable for actual subtitling. Manually post-editing the subtitles will still be required, as for some sentences no automatic compression is generated. In real subtitling it often occurs that the sentences are not compressed, but to keep the subtitles synchronized with the speech, some sentences are entirely removed. In section 2 we describe the processing of a sentence in the sentence compressor, from input to output. In section 3 we describe how the system was evaluated and the results of the evaluation. Section 4 contains the conclusions. 2 From Full Sentence to Compressed Sentence The sentence compression tool is inspired by (Jing, 2001). Although her goal is text summarization</introduction>
		<corps></corps>
		<conclusion></conclusion>
		<discussion></discussion>
		<biblio></biblio>
	</article>
	<article>
		<preamble>marcu_statistics_sentence_pass_one.txt</preamble>
		<titre>Statistics-Based Summarization — Step One: Sentence Compression</titre>
		<auteur>Kevin Knight and Daniel Marcu Information Sciences Institute and Department of Computer Science</auteur>
		<abstract>When humans produce summaries of documents, they do not simply extract sentences and concatenate them. Rather, they create new sentences that are grammatical, that cohere with one another, and that capture the most salient pieces of information in the original document. Given that large collections of text/abstract pairs are available online, it is now possible to envision algorithms that are trained to mimic this process. In this paper, we focus on sentence compression, a simpler version of this larger challenge. We aim to achieve two goals simultaneously: our compressions should be grammatical, and they should retain the most important pieces of information. These two goals can conﬂict. We devise both noisy-channel and decision-tree approaches to the problem, and we evaluate results against manual compressions and a simple baseline.</abstract>
		<introduction>Most of the research in automatic summarization has focused on extraction, i.e., on identifying the most important clauses/sentences/paragraphs in texts (see (Mani & Maybury 1999) for a representative collection of papers). However, determining the most important textual segments is only half of what a summarization system needs to do because, in most cases, the simple catenation of textual segments does not yield coherent outputs. Recently, a number of researchers have started to address the problem of generating coherent summaries: McKeown et al. (1999), Barzilay et al. (1999), and Jing and McKeown (1999) in the context of multidocument summarization; Mani et al. (1999) in the context of revising single document extracts; and Witbrock and Mittal (1999) in the context of headline generation. The approach proposed by Witbrock and Mittal (1999) is the only one that applies a probabilistic model trained directly on Headline, Document pairs. However, this model has yet to scale up to generating multiple-sentence abstracts as well as well-formed, grammatical sentences. All other approaches employ sets of manually written or semi-automatically derived c 2000, American Association for Artiﬁcial InCopyright  telligence (www.aaai.org). All rights reserved. rules for deleting information that is redundant, compressing long sentences into shorter ones, aggregating sentences, repairing reference links, etc. Our goal is also to generate coherent abstracts. However, in contrast with the above work, we intend to eventually use Abstract, Text tuples, which are widely available, in order to automatically learn how to rewrite Texts as coherent Abstracts. In the spirit of the work in the statistical MT community, which is focused on sentence-to-sentence translations, we also decided to focus ﬁrst on a simpler problem, that of sentence compression. We chose this problem for two reasons: • First, the problem is complex enough to require the development of sophisticated compression models: Determining what is important in a sentence and determining how to convey the important information grammatically, using only a few words, is just a scaled down version of the text summarization problem. Yet, the problem is simple enough, since we do not have to worry yet about discourse related issues, such as coherence, anaphors, etc. • Second, an adequate solution to this problem has an immediate impact on several applications. For example, due to time and space constraints, the generation of TV captions often requires only the most important parts of sentences to be shown on a screen (Linke-Ellis 1999; Robert-Ribes et al. 1999). A good sentence compression module would therefore have an impact on the task of automatic caption generation. A sentence compression module can also be used to provide audio scanning services for the blind (Grefenstette 1998). In general, since all systems aimed at producing coherent abstracts implement manually written sets of sentence compression rules (McKeown et al. 1999; Mani, Gates, & Bloedorn 1999; Barzilay, McKeown, & Elhadad 1999), it is likely that a good sentence compression module would impact the overall quality of these systems as well. This becomes particularly important for text genres that use long sentences. In this paper, we present two approaches to the sentence compression problem. Both take as input a sequence of words W = w1 , w2 , . . . , wn (one sentence). An algorithm may drop any subset of these words. The words that remain (order unchanged) form a compression. There are 2n compressions to choose from—some are reasonable, most are not. Our ﬁrst approach develops a probabilistic noisy-channel model for sentence compression. The second approach develops a decisionbased, deterministic model. A noisy-channel model for sentence compression This section describes a probabilistic approach to the compression problem. In particular, we adopt the noisy channel framework that has been relatively successful in a number of other NLP applications, including speech recognition (Jelinek 1997), machine translation (Brown et al. 1993), part-of-speech tagging (Church 1988), transliteration (Knight & Graehl 1998), and information retrieval (Berger & Laﬀerty 1999). In this framework, we look at a long string and imagine that (1) it was originally a short string, and then (2) someone added some additional, optional text to it. Compression is a matter of identifying the original short string. It is not critical whether or not the “original” string is real or hypothetical. For example, in statistical machine translation, we look at a French string and say, “This was originally English, but someone added ‘noise’ to it.” The French may or may not have been translated from English originally, but by removing the noise, we can hypothesize an English source—and thereby translate the string. In the case of compression, the noise consists of optional text material that pads out the core signal. For the larger case of text summarization, it may be useful to imagine a scenario in which a news editor composes a short document, hands it to a reporter, and tells the reporter to “ﬂesh it out” . . . which results in the article we read in the newspaper. As summarizers, we may not have access to the editor’s original version (which may or may not exist), but we can guess at it— which is where probabilities come in. As in any noisy channel application, we must solve three problems: • Source model. We must assign to every string s a probability P(s), which gives the chance that s is generated as an “original short string” in the above hypothetical process. For example, we may want P(s) to be very low if s is ungrammatical. • Channel model. We assign to every pair of strings s, t a probability P(t | s), which gives the chance that when the short string s is expanded, the result is the long string t. For example, if t is the same as s except for the extra word “not,” then we may want P(t | s) to be very low. The word “not” is not optional, additional material. • Decoder. When we observe a long string t, we search for the short string s that maximizes P(s | t). This is equivalent to searching for the s that maximizes P(s) · P (t | s). It is advantageous to break the problem down this way, as it decouples the somewhat independent goals of creating a short text that (1) looks grammatical, and (2) preserves important information. It is easier to build a channel model that focuses exclusively on the latter, without having to worry about the former. That is, we can specify that a certain substring may represent unimportant information, but we do not need to worry that deleting it will result in an ungrammatical structure. We leave that to the source model, which worries exclusively about well-formedness. In fact, we can make use of extensive prior work in source language modeling for speech recognition, machine translation, and natural language generation. The same goes for actual compression (“decoding” in noisy-channel jargon)—we can re-use generic software packages to solve problems in all these application domains.</introduction>
		<corps></corps>
		<conclusion></conclusion>
		<discussion></discussion>
		<biblio></biblio>
	</article>
	<article>
		<preamble>probabilistic_sentence_reduction.txt</preamble>
		<titre>Probabilistic Sentence Reduction Using Support Vector Machines</titre>
		<auteur>Minh Le Nguyen, Akira Shimazu, Susumu Horiguchi Bao Tu Ho and Masaru Fukushi Japan Advanced Institute of Science and Technology</auteur>
		<abstract>This paper investigates a novel application of support vector machines (SVMs) for sentence reduction. We also propose a new probabilistic sentence reduction method based on support vector machine learning. Experimental results show that the proposed methods outperform earlier methods in term of sentence reduction performance. 1</abstract>
		<introduction>The most popular methods of sentence reduction for text summarization are corpus based</introduction>
		<corps></corps>
		<conclusion></conclusion>
		<discussion></discussion>
		<biblio></biblio>
	</article>
	<article>
		<preamble>Stolcke_1996_Automatic_linguistic.txt</preamble>
		<titre>AUTOMATIC LINGUISTIC SEGMENTATION OF CONVERSATIONAL SPEECH</titre>
		<auteur>Andreas Stolcke Elizabeth Shriberg Speech Technology and Research Laboratory SRI International, Menlo Park, CA 940 5</auteur>
		<abstract>As speech recognition moves toward more unconstrained domains such as conversational speech, we encounter a need to be able to segment (or resegment) waveforms and recognizer output into linguistically meaningful units, such a sentences. Toward this end, we present a simple automatic segmenter of transcripts based on N-gram language modeling. We also study the relevance of several word-level features for segmentation performance. Using only word-level information, we achieve 85% recall and 70% precision on linguistic boundary detection.</abstract>
		<introduction>Today’s large-vocabulary speech recognizers typically prefer to process a few tens of seconds of speech at a time, to keep the time and memory demands of the decoder within bounds. For longer inputs, the waveform is usually presegmented into shorter pieces based on simple acoustic criteria, such as nonspeech intervals (e.g., pauses) and turn boundaries (when several speakers are involved). We refer to such segmentations as acoustic segmentations. Acoustic segmentations generally do not reflect the linguistic structure of utterances. They may fragment sentences or semantic units, or group together spans of unrelated units. We examine several reasons why such behavior is undesirable, and propose that linguistic segmentations be used instead. This requires algorithms for automatically finding linguistic units. In this paper we report on first results from our ongoing efforts toward such an automatic linguistic segmentation. In all further discussion, unless otherwise noted, the terms ‘segment,’ ‘segmentation,’ etc. will refer to linguistic segmentations. 2. THE IMPORTANCE OF LINGUISTIC SEGMENTATION Acoustic segmentations are inadequate in cases where the output of a speech recognizer is to serve as input for further processing based on syntactically or semantically coherent units. This includes most natural language (NL) parsers or NL understanding or translation systems. For such systems, the fragmented recognition output would have to be put back together and large spans of unrelated material would need to be resegmented into linguistic units. Automatic detection of linguistic segments could also improve the</introduction>
		<corps></corps>
		<conclusion></conclusion>
		<discussion></discussion>
		<biblio></biblio>
	</article>
	<article>
		<preamble>Torres.txt</preamble>
		<titre>Summary Evaluation with and without References</titre>
		<auteur>Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales</auteur>
		<abstract>We study a new content-based method for the evaluation of text summarization systems without human models which is used to produce system rankings. The research is carried out using a new content-based evaluation framework called F RESA to compute a variety of divergences among probability distributions. We apply our comparison framework to various well-established content-based evaluation measures in text summarization such as C OVERAGE, R ESPONSIVENESS, P YRAMIDS and ROUGE studying their associations in various text summarization tasks including generic multi-document summarization in English and French, focus-based multi-document summarization in English and generic single-document summarization in French and Spanish. Index Terms—Text summarization evaluation, content-based evaluation measures, divergences.</abstract>
		<introduction></introduction>
		<corps></corps>
		<conclusion></conclusion>
		<discussion></discussion>
		<biblio></biblio>
	</article>
	<article>
		<preamble>Word2Vec.txt</preamble>
		<titre>Efficient Estimation of Word Representations in Vector Space</titre>
		<auteur>Tomas Mikolov Google Inc., Mountain View, CA</auteur>
		<abstract>We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities. 1</abstract>
		<introduction>Many current NLP systems and techniques treat words as atomic units - there is no notion of similarity between words, as these are represented as indices in a vocabulary. This choice has several good reasons - simplicity, robustness and the observation that simple models trained on huge amounts of</introduction>
		<corps></corps>
		<conclusion></conclusion>
		<discussion></discussion>
		<biblio></biblio>
	</article>
</articles>
